{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Level 3\n",
        "Task 6: Music Genre Classification Description"
      ],
      "metadata": {
        "id": "FI4nTIHS5y8n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Description:\n",
        "\n",
        "\n",
        "*   Dataset (Recommended): GTZAN (Kaggle).\n",
        "*   Classify songs into genres based on extracted audio features.\n",
        "*   Preprocess features such as MFCCs or use spectrogram images.\n",
        "*   Train and evaluate a multi-class model using tabular or image data.\n",
        "*   If image-based, use a CNN model.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2gYwkRNb6AGn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tools & Libraries:\n",
        "\n",
        "\n",
        "*   Python\n",
        "*   Librosa (for features)\n",
        "*   Scikit-learn or Keras\n",
        "\n",
        "\n",
        "Covered Topics:\n",
        "\n",
        "\n",
        "*   Audio data\n",
        "*   CNNs\n",
        "*   Multi-class classification\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ba1UMKVz6gZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bonus:\n",
        "\n",
        "\n",
        "*   Try both tabular and image-based approaches and compare results.\n",
        "*   Use transfer learning on spectrograms.\n",
        "\n"
      ],
      "metadata": {
        "id": "FWFOF7oK6-et"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "piUGpDH44AD0"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.svm import LinearSVC\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "import math\n",
        "import librosa\n",
        "from librosa.util import find_files"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Visualization"
      ],
      "metadata": {
        "id": "if2Heoq5eYRe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "U5uNHPQ4otRK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09ed1640-2506-414c-a0e4-d0b385875409"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Collect WAV paths + labels (audio) =====\n",
        "\n",
        "AUDIO_ROOT = \"/content/drive/MyDrive/Elevvo Internship/Task 6/gtzan/Data/genres_original\"  # the folder with 10 genre subfolders\n",
        "\n",
        "wav_paths = find_files(AUDIO_ROOT, ext=[\"wav\"], recurse=True)  # list of \".../<genre>/<file>.wav\"\n",
        "labels = [p.replace(\"\\\\\", \"/\").split(\"/\")[-2] for p in wav_paths]  # parent folder name = genre\n",
        "\n",
        "print(f\"Found {len(wav_paths)} audio files across {len(set(labels))} genres.\")\n",
        "print(\"Example:\", wav_paths[0], \"->\", labels[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWfHKuSpa9cy",
        "outputId": "4a4d3eb1-8a77-4b99-e9c1-350754ade70c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1000 audio files across 10 genres.\n",
            "Example: /content/drive/MyDrive/Elevvo Internship/Task 6/gtzan/Data/genres_original/blues/blues.00000.wav -> blues\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Robust, faster MFCC extraction that skips bad files\n",
        "\n",
        "def extract_mfcc_mean_safe(path, n_mfcc=13, sr=22050, duration=10, hop_length=1024):\n",
        "    \"\"\"\n",
        "    Load audio and return 13-D MFCC mean vector.\n",
        "    Skips files that fail to decode.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        y, sr = librosa.load(\n",
        "            path, sr=sr, mono=True, duration=duration, res_type=\"kaiser_fast\"\n",
        "        )\n",
        "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc, hop_length=hop_length)\n",
        "        feat = mfcc.mean(axis=1)  # 13 values\n",
        "        return [float(v) for v in feat]\n",
        "    except Exception as e:\n",
        "        # Comment out the print if too noisy\n",
        "        print(f\"[skip] {path} -> {type(e).__name__}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Build features\n",
        "X_raw = [extract_mfcc_mean_safe(p) for p in wav_paths]  # wav_paths from your earlier code\n",
        "y_raw = labels  # parallel list of genres\n",
        "\n",
        "# Filter out failed loads\n",
        "X, y = [], []\n",
        "skipped = 0\n",
        "for xi, yi in zip(X_raw, y_raw):\n",
        "    if xi is None:\n",
        "        skipped += 1\n",
        "    else:\n",
        "        X.append(xi)\n",
        "        y.append(yi)\n",
        "print(f\"Built features: kept {len(X)}, skipped {skipped}\")\n",
        "\n",
        "# Encode labels & split\n",
        "le = LabelEncoder()\n",
        "y_enc = le.fit_transform(y)\n",
        "\n",
        "Xtr, Xte, ytr, yte = train_test_split(\n",
        "    X, y_enc, test_size=0.2, stratify=y_enc, random_state=42\n",
        ")\n",
        "\n",
        "# Scale & train a fast baseline (you can switch to RBF SVM later)\n",
        "scaler = StandardScaler()\n",
        "Xtr = scaler.fit_transform(Xtr)\n",
        "Xte = scaler.transform(Xte)\n",
        "\n",
        "clf = LinearSVC(random_state=42)  # very fast; try SVC(kernel=\"rbf\") after it works\n",
        "clf.fit(Xtr, ytr)\n",
        "yhat = clf.predict(Xte)\n",
        "\n",
        "print(\"Accuracy:\", f\"{accuracy_score(yte, yhat):.4f}\")\n",
        "print(classification_report(yte, yhat, target_names=list(le.classes_)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLAh69dxdCqv",
        "outputId": "749bc0d9-c59d-4eb5-a5cf-3d9fe97706bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-556468103.py:14: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  y, sr = librosa.load(\n",
            "/usr/local/lib/python3.12/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[skip] /content/drive/MyDrive/Elevvo Internship/Task 6/gtzan/Data/genres_original/jazz/jazz.00054.wav -> NoBackendError: \n",
            "Built features: kept 999, skipped 1\n",
            "Accuracy: 0.4450\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       blues       0.41      0.65      0.50        20\n",
            "   classical       0.70      0.80      0.74        20\n",
            "     country       0.37      0.35      0.36        20\n",
            "       disco       0.27      0.20      0.23        20\n",
            "      hiphop       0.16      0.15      0.15        20\n",
            "        jazz       0.36      0.25      0.29        20\n",
            "       metal       0.61      0.85      0.71        20\n",
            "         pop       0.57      0.80      0.67        20\n",
            "      reggae       0.39      0.35      0.37        20\n",
            "        rock       0.25      0.05      0.08        20\n",
            "\n",
            "    accuracy                           0.45       200\n",
            "   macro avg       0.41      0.44      0.41       200\n",
            "weighted avg       0.41      0.45      0.41       200\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Safe mel-spectrograms + CNN (skips unreadable files) =====\n",
        "\n",
        "TARGET_MELS = 128\n",
        "TARGET_FRAMES = 128\n",
        "\n",
        "def mel_db_tensor_safe(path, sr=22050, duration=10, n_mels=128, hop_length=512,\n",
        "                       target_mels=TARGET_MELS, target_frames=TARGET_FRAMES):\n",
        "    \"\"\"\n",
        "    Returns a [target_mels, target_frames, 1] tensor, or None if load fails.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        y, sr = librosa.load(path, sr=sr, mono=True, duration=duration, res_type=\"kaiser_fast\")\n",
        "        if y is None or len(y) == 0:\n",
        "            return None\n",
        "        S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels, hop_length=hop_length)\n",
        "        S_db = librosa.power_to_db(S, ref=lambda x: S.max() if S.max() != 0 else 1.0)\n",
        "\n",
        "        t = tf.convert_to_tensor(S_db, dtype=tf.float32)[:, :, None]  # [mels, T, 1]\n",
        "        t = tf.image.resize(t, size=(target_mels, tf.shape(t)[1]))\n",
        "        cur_T = tf.shape(t)[1]\n",
        "        t = tf.cond(cur_T < target_frames,\n",
        "                    lambda: tf.pad(t, [[0,0],[0, target_frames-cur_T],[0,0]]),\n",
        "                    lambda: t[:, :target_frames, :])\n",
        "        t = t[:, :target_frames, :]\n",
        "        t_min, t_max = tf.reduce_min(t), tf.reduce_max(t)\n",
        "        t = tf.cond(t_max > t_min, lambda: (t - t_min) / (t_max - t_min), lambda: tf.zeros_like(t))\n",
        "        return t\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "# Build tensors while skipping failures\n",
        "X_tensors, y_clean = [], []\n",
        "for p, lab in zip(wav_paths, labels):\n",
        "    t = mel_db_tensor_safe(p)\n",
        "    if t is not None:\n",
        "        X_tensors.append(t); y_clean.append(lab)\n",
        "print(f\"Kept {len(X_tensors)} files, skipped {len(wav_paths) - len(X_tensors)} unreadable files.\")\n",
        "\n",
        "X_all = tf.stack(X_tensors, axis=0)\n",
        "# simple label map without sklearn\n",
        "classes = sorted(set(y_clean))\n",
        "cls2id = {c:i for i,c in enumerate(classes)}\n",
        "y_all = tf.constant([cls2id[c] for c in y_clean], dtype=tf.int32)\n",
        "\n",
        "# random split (pure TF)\n",
        "N = tf.shape(X_all)[0]\n",
        "idx = tf.random.shuffle(tf.range(N), seed=42)\n",
        "val_size = tf.cast(tf.math.round(0.2 * tf.cast(N, tf.float32)), tf.int32)\n",
        "idx_val = idx[:val_size]; idx_tr = idx[val_size:]\n",
        "\n",
        "Xtr, ytr = tf.gather(X_all, idx_tr), tf.gather(y_all, idx_tr)\n",
        "Xva, yva = tf.gather(X_all, idx_val), tf.gather(y_all, idx_val)\n",
        "\n",
        "batch_size = 16\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((Xtr, ytr)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "val_ds   = tf.data.Dataset.from_tensor_slices((Xva, yva)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(TARGET_MELS, TARGET_FRAMES, 1)),\n",
        "    tf.keras.layers.Conv2D(32, 3, activation=\"relu\"), tf.keras.layers.MaxPooling2D(),\n",
        "    tf.keras.layers.Conv2D(64, 3, activation=\"relu\"), tf.keras.layers.MaxPooling2D(),\n",
        "    tf.keras.layers.Conv2D(128, 3, activation=\"relu\"),\n",
        "    tf.keras.layers.GlobalAveragePooling2D(),\n",
        "    tf.keras.layers.Dense(128, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(len(classes), activation=\"softmax\"),\n",
        "])\n",
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "hist = model.fit(train_ds, validation_data=val_ds, epochs=15)\n",
        "print(\"Validation accuracy:\", hist.history[\"val_accuracy\"][-1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oz6qu2vvfmX8",
        "outputId": "a751f5fb-c49f-46b6-f37f-66e7d6c19170"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2542452473.py:14: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  y, sr = librosa.load(path, sr=sr, mono=True, duration=duration, res_type=\"kaiser_fast\")\n",
            "/usr/local/lib/python3.12/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kept 999 files, skipped 1 unreadable files.\n",
            "Epoch 1/15\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 547ms/step - accuracy: 0.1015 - loss: 2.3047 - val_accuracy: 0.0750 - val_loss: 2.3094\n",
            "Epoch 2/15\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 533ms/step - accuracy: 0.1190 - loss: 2.2760 - val_accuracy: 0.1450 - val_loss: 2.2738\n",
            "Epoch 3/15\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 521ms/step - accuracy: 0.1908 - loss: 2.1247 - val_accuracy: 0.1950 - val_loss: 2.1098\n",
            "Epoch 4/15\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 489ms/step - accuracy: 0.2334 - loss: 2.0606 - val_accuracy: 0.2200 - val_loss: 2.0852\n",
            "Epoch 5/15\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 521ms/step - accuracy: 0.2563 - loss: 2.0319 - val_accuracy: 0.2500 - val_loss: 2.0697\n",
            "Epoch 6/15\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 505ms/step - accuracy: 0.2999 - loss: 1.9710 - val_accuracy: 0.2900 - val_loss: 1.9748\n",
            "Epoch 7/15\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 488ms/step - accuracy: 0.3188 - loss: 1.9401 - val_accuracy: 0.2900 - val_loss: 1.9584\n",
            "Epoch 8/15\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 483ms/step - accuracy: 0.3269 - loss: 1.8878 - val_accuracy: 0.3000 - val_loss: 1.8980\n",
            "Epoch 9/15\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 479ms/step - accuracy: 0.3287 - loss: 1.8421 - val_accuracy: 0.3150 - val_loss: 1.8516\n",
            "Epoch 10/15\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 519ms/step - accuracy: 0.3430 - loss: 1.7903 - val_accuracy: 0.3350 - val_loss: 1.7939\n",
            "Epoch 11/15\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 860ms/step - accuracy: 0.3585 - loss: 1.7528 - val_accuracy: 0.3200 - val_loss: 1.7576\n",
            "Epoch 12/15\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 488ms/step - accuracy: 0.3844 - loss: 1.6949 - val_accuracy: 0.3400 - val_loss: 1.7135\n",
            "Epoch 13/15\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 477ms/step - accuracy: 0.3870 - loss: 1.6767 - val_accuracy: 0.3350 - val_loss: 1.6967\n",
            "Epoch 14/15\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 477ms/step - accuracy: 0.4034 - loss: 1.6346 - val_accuracy: 0.3500 - val_loss: 1.6728\n",
            "Epoch 15/15\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 456ms/step - accuracy: 0.4068 - loss: 1.6137 - val_accuracy: 0.3400 - val_loss: 1.6486\n",
            "Validation accuracy: 0.3400000035762787\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== images_original → CNN =====\n",
        "\n",
        "IMG_DIR = \"/content/drive/MyDrive/Elevvo Internship/Task 6/gtzan/Data/images_original\"   # root with 10 genre subfolders\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    IMG_DIR, validation_split=0.2, subset=\"training\",\n",
        "    seed=42, image_size=(128,128), batch_size=32\n",
        ")\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    IMG_DIR, validation_split=0.2, subset=\"validation\",\n",
        "    seed=42, image_size=(128,128), batch_size=32\n",
        ")\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(128,128,3)),\n",
        "    tf.keras.layers.Rescaling(1./255),\n",
        "    tf.keras.layers.Conv2D(32, 3, activation=\"relu\"), tf.keras.layers.MaxPooling2D(),\n",
        "    tf.keras.layers.Conv2D(64, 3, activation=\"relu\"), tf.keras.layers.MaxPooling2D(),\n",
        "    tf.keras.layers.Conv2D(128, 3, activation=\"relu\"),\n",
        "    tf.keras.layers.GlobalAveragePooling2D(),\n",
        "    tf.keras.layers.Dense(128, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(len(train_ds.class_names), activation=\"softmax\"),\n",
        "])\n",
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "model.fit(train_ds, validation_data=val_ds, epochs=15)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTPiRLI3bqdp",
        "outputId": "92d2a96d-9a8d-42d4-f4ac-7a1476086872"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1001 files belonging to 10 classes.\n",
            "Using 801 files for training.\n",
            "Found 1001 files belonging to 10 classes.\n",
            "Using 200 files for validation.\n",
            "Epoch 1/15\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 2s/step - accuracy: 0.1176 - loss: 2.3070 - val_accuracy: 0.1550 - val_loss: 2.2985\n",
            "Epoch 2/15\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 1s/step - accuracy: 0.1870 - loss: 2.2708 - val_accuracy: 0.1700 - val_loss: 2.1966\n",
            "Epoch 3/15\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2s/step - accuracy: 0.1763 - loss: 2.1571 - val_accuracy: 0.1550 - val_loss: 2.0806\n",
            "Epoch 4/15\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 1s/step - accuracy: 0.1508 - loss: 2.0734 - val_accuracy: 0.1550 - val_loss: 2.0793\n",
            "Epoch 5/15\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 2s/step - accuracy: 0.2144 - loss: 2.0058 - val_accuracy: 0.2100 - val_loss: 2.0394\n",
            "Epoch 6/15\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 1s/step - accuracy: 0.2279 - loss: 1.9598 - val_accuracy: 0.1700 - val_loss: 2.0387\n",
            "Epoch 7/15\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 1s/step - accuracy: 0.2167 - loss: 1.9538 - val_accuracy: 0.2100 - val_loss: 1.9525\n",
            "Epoch 8/15\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 1s/step - accuracy: 0.2529 - loss: 1.8977 - val_accuracy: 0.1650 - val_loss: 2.0847\n",
            "Epoch 9/15\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1s/step - accuracy: 0.2451 - loss: 1.9448 - val_accuracy: 0.1900 - val_loss: 2.0289\n",
            "Epoch 10/15\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1s/step - accuracy: 0.2209 - loss: 1.9435 - val_accuracy: 0.2400 - val_loss: 1.8879\n",
            "Epoch 11/15\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 1s/step - accuracy: 0.2473 - loss: 1.8459 - val_accuracy: 0.2300 - val_loss: 1.9916\n",
            "Epoch 12/15\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 1s/step - accuracy: 0.2097 - loss: 2.0259 - val_accuracy: 0.2250 - val_loss: 1.9128\n",
            "Epoch 13/15\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 1s/step - accuracy: 0.2500 - loss: 1.8864 - val_accuracy: 0.2450 - val_loss: 1.8883\n",
            "Epoch 14/15\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1s/step - accuracy: 0.2598 - loss: 1.8216 - val_accuracy: 0.2400 - val_loss: 1.9627\n",
            "Epoch 15/15\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1s/step - accuracy: 0.3037 - loss: 1.8271 - val_accuracy: 0.2250 - val_loss: 1.8382\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7c16d5b5abd0>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== genre classification — MFCCs =====\n",
        "\n",
        "# 1) Collect WAV paths + labels from GTZAN\n",
        "AUDIO_ROOT = \"/content/drive/MyDrive/Elevvo Internship/Task 6/gtzan/Data/genres_original\"  # folder with 10 genre subfolders\n",
        "wav_paths = find_files(AUDIO_ROOT, ext=[\"wav\"], recurse=True)\n",
        "labels = [p.replace(\"\\\\\", \"/\").split(\"/\")[-2] for p in wav_paths]  # parent folder name\n",
        "\n",
        "# 2) Robust MFCC extractor (fast + skips bad files)\n",
        "def mfcc_mean_safe(path, n_mfcc=13, sr=22050, duration=10, hop_length=1024):\n",
        "    try:\n",
        "        y, sr = librosa.load(path, sr=sr, mono=True, duration=duration, res_type=\"kaiser_fast\")\n",
        "        if y is None or len(y) == 0:\n",
        "            return None\n",
        "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc, hop_length=hop_length)\n",
        "        feat = mfcc.mean(axis=1)  # 13-D (ndarray supports .mean without numpy import)\n",
        "        return [float(v) for v in feat]\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "# 3) Build dataset (skip unreadable files)\n",
        "X_raw = [mfcc_mean_safe(p) for p in wav_paths]\n",
        "X, y = [], []\n",
        "for xi, yi in zip(X_raw, labels):\n",
        "    if xi is not None:\n",
        "        X.append(xi); y.append(yi)\n",
        "\n",
        "# 4) Encode, split, scale, train\n",
        "le = LabelEncoder()\n",
        "y_enc = le.fit_transform(y)\n",
        "\n",
        "Xtr, Xte, ytr, yte = train_test_split(X, y_enc, test_size=0.2, stratify=y_enc, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "Xtr = scaler.fit_transform(Xtr)\n",
        "Xte = scaler.transform(Xte)\n",
        "\n",
        "clf = LogisticRegression(max_iter=1000, n_jobs=-1)   # strong + fast baseline\n",
        "clf.fit(Xtr, ytr)\n",
        "yhat = clf.predict(Xte)\n",
        "\n",
        "print(\"Accuracy:\", f\"{accuracy_score(yte, yhat):.4f}\")\n",
        "print(classification_report(yte, yhat, target_names=list(le.classes_)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GHIf7Jojuej",
        "outputId": "5e661d5b-e4fd-4e38-d76a-d747145820bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-98398754.py:17: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  y, sr = librosa.load(path, sr=sr, mono=True, duration=duration, res_type=\"kaiser_fast\")\n",
            "/usr/local/lib/python3.12/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.4400\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       blues       0.35      0.40      0.37        20\n",
            "   classical       0.88      0.75      0.81        20\n",
            "     country       0.32      0.35      0.33        20\n",
            "       disco       0.27      0.20      0.23        20\n",
            "      hiphop       0.25      0.30      0.27        20\n",
            "        jazz       0.26      0.30      0.28        20\n",
            "       metal       0.68      0.75      0.71        20\n",
            "         pop       0.64      0.80      0.71        20\n",
            "      reggae       0.42      0.40      0.41        20\n",
            "        rock       0.30      0.15      0.20        20\n",
            "\n",
            "    accuracy                           0.44       200\n",
            "   macro avg       0.44      0.44      0.43       200\n",
            "weighted avg       0.44      0.44      0.43       200\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== genre classification — Mel-spectrograms =====\n",
        "\n",
        "AUDIO_ROOT = \"/content/drive/MyDrive/Elevvo Internship/Task 6/gtzan/Data/genres_original\"\n",
        "wav_paths = find_files(AUDIO_ROOT, ext=[\"wav\"], recurse=True)\n",
        "labels = [p.replace(\"\\\\\", \"/\").split(\"/\")[-2] for p in wav_paths]\n",
        "\n",
        "# Label map (no sklearn needed)\n",
        "classes = sorted(set(labels))\n",
        "cls2id = {c:i for i,c in enumerate(classes)}\n",
        "y_all = tf.constant([cls2id[c] for c in labels], dtype=tf.int32)\n",
        "\n",
        "TARGET_MELS, TARGET_FRAMES = 128, 128\n",
        "\n",
        "def mel_db_tensor_safe(path, sr=22050, duration=10, n_mels=128, hop_length=512,\n",
        "                       target_mels=TARGET_MELS, target_frames=TARGET_FRAMES):\n",
        "    try:\n",
        "        y, sr = librosa.load(path, sr=sr, mono=True, duration=duration, res_type=\"kaiser_fast\")\n",
        "        if y is None or len(y) == 0:\n",
        "            return None\n",
        "        S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels, hop_length=hop_length)\n",
        "        S_db = librosa.power_to_db(S, ref=lambda x: S.max() if S.max() != 0 else 1.0)  # [mels,T]\n",
        "        t = tf.convert_to_tensor(S_db, dtype=tf.float32)[:, :, None]                   # [mels,T,1]\n",
        "        t = tf.image.resize(t, size=(target_mels, tf.shape(t)[1]))                     # fix mel bins\n",
        "        cur_T = tf.shape(t)[1]\n",
        "        t = tf.cond(cur_T < target_frames,\n",
        "                    lambda: tf.pad(t, [[0,0],[0, target_frames-cur_T],[0,0]]),\n",
        "                    lambda: t[:, :target_frames, :])\n",
        "        t = t[:, :target_frames, :]\n",
        "        # min-max normalize per sample\n",
        "        t_min, t_max = tf.reduce_min(t), tf.reduce_max(t)\n",
        "        t = tf.cond(t_max > t_min, lambda: (t - t_min) / (t_max - t_min), lambda: tf.zeros_like(t))\n",
        "        return t\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "# Build tensors (skip bad files)\n",
        "X_tensors, y_clean = [], []\n",
        "for p, lab in zip(wav_paths, y_all.numpy().tolist()):\n",
        "    t = mel_db_tensor_safe(p)\n",
        "    if t is not None:\n",
        "        X_tensors.append(t); y_clean.append(lab)\n",
        "\n",
        "X_all = tf.stack(X_tensors, axis=0)                           # [N,128,128,1]\n",
        "y_all = tf.constant(y_clean, dtype=tf.int32)\n",
        "\n",
        "# Train/val split (pure TF)\n",
        "N = tf.shape(X_all)[0]\n",
        "idx = tf.random.shuffle(tf.range(N), seed=42)\n",
        "val_n = tf.cast(tf.math.round(0.2 * tf.cast(N, tf.float32)), tf.int32)\n",
        "idx_val, idx_tr = idx[:val_n], idx[val_n:]\n",
        "Xtr, ytr = tf.gather(X_all, idx_tr), tf.gather(y_all, idx_tr)\n",
        "Xva, yva = tf.gather(X_all, idx_val), tf.gather(y_all, idx_val)\n",
        "\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((Xtr, ytr)).batch(16).prefetch(tf.data.AUTOTUNE)\n",
        "val_ds   = tf.data.Dataset.from_tensor_slices((Xva, yva)).batch(16).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Small CNN\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(TARGET_MELS, TARGET_FRAMES, 1)),\n",
        "    tf.keras.layers.Conv2D(32, 3, activation=\"relu\"), tf.keras.layers.MaxPooling2D(),\n",
        "    tf.keras.layers.Conv2D(64, 3, activation=\"relu\"), tf.keras.layers.MaxPooling2D(),\n",
        "    tf.keras.layers.Conv2D(128, 3, activation=\"relu\"),\n",
        "    tf.keras.layers.GlobalAveragePooling2D(),\n",
        "    tf.keras.layers.Dense(128, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(len(classes), activation=\"softmax\"),\n",
        "])\n",
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "history = model.fit(train_ds, validation_data=val_ds, epochs=15, verbose=1)\n",
        "print(\"Val accuracy:\", history.history[\"val_accuracy\"][-1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ger-GDtwj0Rx",
        "outputId": "bf895024-b331-4cfd-a581-c14365432853"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3343428929.py:20: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  y, sr = librosa.load(path, sr=sr, mono=True, duration=duration, res_type=\"kaiser_fast\")\n",
            "/usr/local/lib/python3.12/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 878ms/step - accuracy: 0.0999 - loss: 2.3059 - val_accuracy: 0.0850 - val_loss: 2.3119\n",
            "Epoch 2/15\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 452ms/step - accuracy: 0.1135 - loss: 2.2896 - val_accuracy: 0.1300 - val_loss: 2.2789\n",
            "Epoch 3/15\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 483ms/step - accuracy: 0.1671 - loss: 2.1985 - val_accuracy: 0.1550 - val_loss: 2.1561\n",
            "Epoch 4/15\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 722ms/step - accuracy: 0.2053 - loss: 2.1288 - val_accuracy: 0.1850 - val_loss: 2.1070\n",
            "Epoch 5/15\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 904ms/step - accuracy: 0.2391 - loss: 2.0782 - val_accuracy: 0.2300 - val_loss: 2.0533\n",
            "Epoch 6/15\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 526ms/step - accuracy: 0.2490 - loss: 2.0309 - val_accuracy: 0.2450 - val_loss: 2.0310\n",
            "Epoch 7/15\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 478ms/step - accuracy: 0.2505 - loss: 2.0064 - val_accuracy: 0.2400 - val_loss: 2.0243\n",
            "Epoch 8/15\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 458ms/step - accuracy: 0.2665 - loss: 1.9860 - val_accuracy: 0.2550 - val_loss: 2.0085\n",
            "Epoch 9/15\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 505ms/step - accuracy: 0.2947 - loss: 1.9503 - val_accuracy: 0.2650 - val_loss: 1.9819\n",
            "Epoch 10/15\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 429ms/step - accuracy: 0.3182 - loss: 1.9051 - val_accuracy: 0.2850 - val_loss: 1.8699\n",
            "Epoch 11/15\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 476ms/step - accuracy: 0.3140 - loss: 1.8351 - val_accuracy: 0.2850 - val_loss: 1.8149\n",
            "Epoch 12/15\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 541ms/step - accuracy: 0.3345 - loss: 1.7645 - val_accuracy: 0.4050 - val_loss: 1.7208\n",
            "Epoch 13/15\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 1s/step - accuracy: 0.3764 - loss: 1.6949 - val_accuracy: 0.3800 - val_loss: 1.7728\n",
            "Epoch 14/15\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 441ms/step - accuracy: 0.3498 - loss: 1.6884 - val_accuracy: 0.3800 - val_loss: 1.6495\n",
            "Epoch 15/15\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 475ms/step - accuracy: 0.3997 - loss: 1.6203 - val_accuracy: 0.3900 - val_loss: 1.6936\n",
            "Val accuracy: 0.38999998569488525\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== MFCC tabular pipeline =====\n",
        "\n",
        "# 1) Collect file paths + labels (parent folder name)\n",
        "AUDIO_ROOT = \"/content/drive/MyDrive/Elevvo Internship/Task 6/gtzan/Data/genres_original\"  # change if needed\n",
        "wav_paths = find_files(AUDIO_ROOT, ext=[\"wav\"], recurse=True)\n",
        "labels = [p.replace(\"\\\\\", \"/\").split(\"/\")[-2] for p in wav_paths]\n",
        "\n",
        "# 2) Robust/fast MFCC extractor (skips unreadable files)\n",
        "def mfcc_mean_safe(path, n_mfcc=13, sr=22050, duration=10, hop_length=1024):\n",
        "    try:\n",
        "        y, sr = librosa.load(path, sr=sr, mono=True, duration=duration, res_type=\"kaiser_fast\")\n",
        "        if y is None or len(y) == 0:\n",
        "            return None\n",
        "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc, hop_length=hop_length)\n",
        "        feat = mfcc.mean(axis=1)  # ndarray supports .mean without importing numpy\n",
        "        return [float(v) for v in feat]\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "# 3) Build tabular dataset\n",
        "X_raw = [mfcc_mean_safe(p) for p in wav_paths]\n",
        "X, y = [], []\n",
        "for xi, yi in zip(X_raw, labels):\n",
        "    if xi is not None:\n",
        "        X.append(xi); y.append(yi)\n",
        "\n",
        "# 4) Encode labels, split, scale\n",
        "le = LabelEncoder()\n",
        "y_enc = le.fit_transform(y)\n",
        "\n",
        "Xtr, Xte, ytr, yte = train_test_split(X, y_enc, test_size=0.2, stratify=y_enc, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "Xtr = scaler.fit_transform(Xtr)\n",
        "Xte = scaler.transform(Xte)\n",
        "\n",
        "# 5) Train a multi-class model (fast strong baseline)\n",
        "clf = LogisticRegression(max_iter=1000, n_jobs=-1, multi_class=\"auto\")\n",
        "clf.fit(Xtr, ytr)\n",
        "\n",
        "# 6) Evaluate\n",
        "yhat = clf.predict(Xte)\n",
        "print(\"Accuracy:\", f\"{accuracy_score(yte, yhat):.4f}\")\n",
        "print(classification_report(yte, yhat, target_names=list(le.classes_)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FubgG4MEofgO",
        "outputId": "a7716fc7-c471-4424-d4b9-a77f7f8b9d9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2537400203.py:17: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  y, sr = librosa.load(path, sr=sr, mono=True, duration=duration, res_type=\"kaiser_fast\")\n",
            "/usr/local/lib/python3.12/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.4400\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       blues       0.35      0.40      0.37        20\n",
            "   classical       0.88      0.75      0.81        20\n",
            "     country       0.32      0.35      0.33        20\n",
            "       disco       0.27      0.20      0.23        20\n",
            "      hiphop       0.25      0.30      0.27        20\n",
            "        jazz       0.26      0.30      0.28        20\n",
            "       metal       0.68      0.75      0.71        20\n",
            "         pop       0.64      0.80      0.71        20\n",
            "      reggae       0.42      0.40      0.41        20\n",
            "        rock       0.30      0.15      0.20        20\n",
            "\n",
            "    accuracy                           0.44       200\n",
            "   macro avg       0.44      0.44      0.43       200\n",
            "weighted avg       0.44      0.44      0.43       200\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Mel-spectrogram image pipeline =====\n",
        "\n",
        "# 1) Collect file paths + labels\n",
        "AUDIO_ROOT = \"/content/drive/MyDrive/Elevvo Internship/Task 6/gtzan/Data/genres_original\"\n",
        "wav_paths = find_files(AUDIO_ROOT, ext=[\"wav\"], recurse=True)\n",
        "labels = [p.replace(\"\\\\\", \"/\").split(\"/\")[-2] for p in wav_paths]\n",
        "\n",
        "# Build integer labels without sklearn\n",
        "classes = sorted(set(labels))\n",
        "cls2id = {c:i for i,c in enumerate(classes)}\n",
        "y_all = tf.constant([cls2id[c] for c in labels], dtype=tf.int32)\n",
        "\n",
        "# 2) Robust mel-spectrogram tensor (skips unreadable files if any)\n",
        "TARGET_MELS, TARGET_FRAMES = 128, 128\n",
        "def mel_db_tensor_safe(path, sr=22050, duration=10, n_mels=128, hop_length=512,\n",
        "                       target_mels=TARGET_MELS, target_frames=TARGET_FRAMES):\n",
        "    try:\n",
        "        y, sr = librosa.load(path, sr=sr, mono=True, duration=duration, res_type=\"kaiser_fast\")\n",
        "        if y is None or len(y) == 0:\n",
        "            return None\n",
        "        S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels, hop_length=hop_length)\n",
        "        S_db = librosa.power_to_db(S, ref=lambda x: S.max() if S.max() != 0 else 1.0)  # [mels,T]\n",
        "        t = tf.convert_to_tensor(S_db, dtype=tf.float32)[:, :, None]                   # [mels,T,1]\n",
        "        # Resize mel bins, then pad/crop time frames\n",
        "        t = tf.image.resize(t, size=(target_mels, tf.shape(t)[1]))\n",
        "        cur_T = tf.shape(t)[1]\n",
        "        t = tf.cond(cur_T < target_frames,\n",
        "                    lambda: tf.pad(t, [[0,0],[0, target_frames-cur_T],[0,0]]),\n",
        "                    lambda: t[:, :target_frames, :])\n",
        "        t = t[:, :target_frames, :]\n",
        "        # Min-max normalize per sample\n",
        "        t_min, t_max = tf.reduce_min(t), tf.reduce_max(t)\n",
        "        t = tf.cond(t_max > t_min, lambda: (t - t_min) / (t_max - t_min), lambda: tf.zeros_like(t))\n",
        "        return t\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "# 3) Build tensor dataset (skip failures)\n",
        "X_tensors, y_clean = [], []\n",
        "for p, lab_id in zip(wav_paths, y_all.numpy().tolist()):\n",
        "    t = mel_db_tensor_safe(p)\n",
        "    if t is not None:\n",
        "        X_tensors.append(t); y_clean.append(lab_id)\n",
        "\n",
        "X_all = tf.stack(X_tensors, axis=0)             # [N, 128, 128, 1]\n",
        "y_all = tf.constant(y_clean, dtype=tf.int32)    # [N]\n",
        "\n",
        "# 4) Train/validation split (pure TF, stratified-ish via shuffle)\n",
        "N = tf.shape(X_all)[0]\n",
        "idx = tf.random.shuffle(tf.range(N), seed=42)\n",
        "val_n = tf.cast(tf.math.round(0.2 * tf.cast(N, tf.float32)), tf.int32)\n",
        "idx_val, idx_tr = idx[:val_n], idx[val_n:]\n",
        "Xtr, ytr = tf.gather(X_all, idx_tr), tf.gather(y_all, idx_tr)\n",
        "Xva, yva = tf.gather(X_all, idx_val), tf.gather(y_all, idx_val)\n",
        "\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((Xtr, ytr)).batch(16).prefetch(tf.data.AUTOTUNE)\n",
        "val_ds   = tf.data.Dataset.from_tensor_slices((Xva, yva)).batch(16).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# 5) Small CNN\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(TARGET_MELS, TARGET_FRAMES, 1)),\n",
        "    tf.keras.layers.Conv2D(32, 3, activation=\"relu\"), tf.keras.layers.MaxPooling2D(),\n",
        "    tf.keras.layers.Conv2D(64, 3, activation=\"relu\"), tf.keras.layers.MaxPooling2D(),\n",
        "    tf.keras.layers.Conv2D(128, 3, activation=\"relu\"),\n",
        "    tf.keras.layers.GlobalAveragePooling2D(),\n",
        "    tf.keras.layers.Dense(128, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(len(classes), activation=\"softmax\"),\n",
        "])\n",
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "# 6) Train & evaluate\n",
        "history = model.fit(train_ds, validation_data=val_ds, epochs=15, verbose=1)\n",
        "print(\"Validation accuracy:\", history.history[\"val_accuracy\"][-1])\n",
        "\n",
        "# Optional: confusion matrix (pure TF)\n",
        "y_pred = tf.argmax(model.predict(val_ds, verbose=0), axis=1)\n",
        "cm = tf.math.confusion_matrix(\n",
        "    tf.concat([y for _, y in val_ds], axis=0),\n",
        "    y_pred,\n",
        "    num_classes=len(classes)\n",
        ")\n",
        "print(\"Confusion matrix (val):\")\n",
        "tf.print(cm)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMQOaNghon6N",
        "outputId": "230fae4d-a554-44d6-d252-796b52e6ab4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1193118227.py:21: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  y, sr = librosa.load(path, sr=sr, mono=True, duration=duration, res_type=\"kaiser_fast\")\n",
            "/usr/local/lib/python3.12/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 444ms/step - accuracy: 0.0829 - loss: 2.3057 - val_accuracy: 0.0800 - val_loss: 2.3034\n",
            "Epoch 2/15\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 483ms/step - accuracy: 0.1369 - loss: 2.2839 - val_accuracy: 0.2300 - val_loss: 2.1991\n",
            "Epoch 3/15\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 434ms/step - accuracy: 0.2371 - loss: 2.1315 - val_accuracy: 0.2550 - val_loss: 2.0813\n",
            "Epoch 4/15\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 465ms/step - accuracy: 0.2465 - loss: 2.0335 - val_accuracy: 0.2700 - val_loss: 2.0359\n",
            "Epoch 5/15\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 428ms/step - accuracy: 0.2290 - loss: 2.0001 - val_accuracy: 0.2350 - val_loss: 1.9812\n",
            "Epoch 6/15\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 475ms/step - accuracy: 0.2676 - loss: 1.9557 - val_accuracy: 0.3650 - val_loss: 1.9098\n",
            "Epoch 7/15\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 434ms/step - accuracy: 0.3006 - loss: 1.8530 - val_accuracy: 0.3400 - val_loss: 1.8758\n",
            "Epoch 8/15\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 469ms/step - accuracy: 0.3176 - loss: 1.7510 - val_accuracy: 0.3900 - val_loss: 1.7147\n",
            "Epoch 9/15\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 433ms/step - accuracy: 0.3709 - loss: 1.6870 - val_accuracy: 0.4100 - val_loss: 1.7525\n",
            "Epoch 10/15\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 479ms/step - accuracy: 0.3841 - loss: 1.6344 - val_accuracy: 0.4200 - val_loss: 1.7218\n",
            "Epoch 11/15\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 433ms/step - accuracy: 0.4069 - loss: 1.5938 - val_accuracy: 0.4200 - val_loss: 1.6751\n",
            "Epoch 12/15\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 461ms/step - accuracy: 0.4137 - loss: 1.5754 - val_accuracy: 0.4150 - val_loss: 1.6620\n",
            "Epoch 13/15\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 590ms/step - accuracy: 0.4292 - loss: 1.5464 - val_accuracy: 0.4050 - val_loss: 1.6125\n",
            "Epoch 14/15\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 465ms/step - accuracy: 0.4312 - loss: 1.5415 - val_accuracy: 0.3950 - val_loss: 1.5997\n",
            "Epoch 15/15\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 421ms/step - accuracy: 0.4427 - loss: 1.5231 - val_accuracy: 0.3800 - val_loss: 1.5840\n",
            "Validation accuracy: 0.3799999952316284\n",
            "Confusion matrix (val):\n",
            "[[2 0 5 ... 0 2 6]\n",
            " [0 16 2 ... 0 0 1]\n",
            " [0 2 9 ... 2 0 2]\n",
            " ...\n",
            " [2 0 1 ... 9 0 1]\n",
            " [0 0 0 ... 10 5 0]\n",
            " [0 0 7 ... 1 1 2]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Mel-spectrograms → CNN =====\n",
        "\n",
        "# -----------------------\n",
        "# 1) Collect WAV paths + labels\n",
        "# -----------------------\n",
        "AUDIO_ROOT = \"/content/drive/MyDrive/Elevvo Internship/Task 6/gtzan/Data/genres_original\"        # root folder with 10 genre subfolders\n",
        "wav_paths = find_files(AUDIO_ROOT, ext=[\"wav\"], recurse=True)\n",
        "labels = [p.replace(\"\\\\\", \"/\").split(\"/\")[-2] for p in wav_paths]   # parent folder name\n",
        "\n",
        "# Map class names <-> ids (no sklearn)\n",
        "classes = sorted(set(labels))\n",
        "cls2id = {c: i for i, c in enumerate(classes)}\n",
        "y_all = tf.constant([cls2id[c] for c in labels], dtype=tf.int32)\n",
        "\n",
        "# -----------------------\n",
        "# 2) WAV -> mel-spectrogram tensor (robust; skips unreadable files)\n",
        "# -----------------------\n",
        "TARGET_MELS, TARGET_FRAMES = 128, 128  # final “image” size HxW\n",
        "SR = 22050                              # fixed sample rate\n",
        "DUR = 10                                # seconds to load (speed knob)\n",
        "HOP = 512\n",
        "\n",
        "def mel_db_tensor_safe(path,\n",
        "                       sr=SR, duration=DUR, n_mels=128, hop_length=HOP,\n",
        "                       target_mels=TARGET_MELS, target_frames=TARGET_FRAMES):\n",
        "    \"\"\"Return [target_mels, target_frames, 1] tensor or None if file fails to decode.\"\"\"\n",
        "    try:\n",
        "        y, sr = librosa.load(path, sr=sr, mono=True, duration=duration, res_type=\"kaiser_fast\")\n",
        "        if y is None or len(y) == 0:\n",
        "            return None\n",
        "        S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels, hop_length=hop_length)\n",
        "        S_db = librosa.power_to_db(S, ref=lambda x: S.max() if S.max() != 0 else 1.0)  # [mels, T]\n",
        "\n",
        "        # to tensor [mels, T, 1]\n",
        "        t = tf.convert_to_tensor(S_db, dtype=tf.float32)\n",
        "        t = tf.expand_dims(t, -1)\n",
        "\n",
        "        # resize mel bins (H) to target, then pad/crop time (W) to target\n",
        "        t = tf.image.resize(t, size=(target_mels, tf.shape(t)[1]))\n",
        "        cur_T = tf.shape(t)[1]\n",
        "        t = tf.cond(cur_T < target_frames,\n",
        "                    lambda: tf.pad(t, [[0,0], [0, target_frames - cur_T], [0,0]]),\n",
        "                    lambda: t[:, :target_frames, :])\n",
        "        t = t[:, :target_frames, :]\n",
        "\n",
        "        # per-sample min-max normalization to [0,1]\n",
        "        t_min, t_max = tf.reduce_min(t), tf.reduce_max(t)\n",
        "        return tf.cond(t_max > t_min, lambda: (t - t_min) / (t_max - t_min),\n",
        "                       lambda: tf.zeros_like(t))\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "# Build tensors, skipping bad files\n",
        "X_tensors, y_clean = [], []\n",
        "for p, lab in zip(wav_paths, tf.unstack(y_all)):\n",
        "    t = mel_db_tensor_safe(p)\n",
        "    if t is not None:\n",
        "        X_tensors.append(t)\n",
        "        y_clean.append(lab.numpy().item())\n",
        "\n",
        "X_all = tf.stack(X_tensors, axis=0)                 # [N, 128, 128, 1]\n",
        "y_all = tf.constant(y_clean, dtype=tf.int32)        # [N]\n",
        "print(\"Kept:\", X_all.shape[0], \"samples\")\n",
        "\n",
        "# -----------------------\n",
        "# 3) Train/val split (pure TF)\n",
        "# -----------------------\n",
        "N = tf.shape(X_all)[0]\n",
        "idx = tf.random.shuffle(tf.range(N), seed=42)\n",
        "val_n = tf.cast(tf.math.round(0.2 * tf.cast(N, tf.float32)), tf.int32)\n",
        "idx_val, idx_tr = idx[:val_n], idx[val_n:]\n",
        "\n",
        "Xtr, ytr = tf.gather(X_all, idx_tr), tf.gather(y_all, idx_tr)\n",
        "Xva, yva = tf.gather(X_all, idx_val), tf.gather(y_all, idx_val)\n",
        "\n",
        "BATCH = 16\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((Xtr, ytr)).batch(BATCH).prefetch(tf.data.AUTOTUNE)\n",
        "val_ds   = tf.data.Dataset.from_tensor_slices((Xva, yva)).batch(BATCH).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# -----------------------\n",
        "# 4) CNN model\n",
        "# -----------------------\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(TARGET_MELS, TARGET_FRAMES, 1)),\n",
        "    tf.keras.layers.Conv2D(32, 3, activation=\"relu\"), tf.keras.layers.MaxPooling2D(),\n",
        "    tf.keras.layers.Conv2D(64, 3, activation=\"relu\"), tf.keras.layers.MaxPooling2D(),\n",
        "    tf.keras.layers.Conv2D(128, 3, activation=\"relu\"),\n",
        "    tf.keras.layers.GlobalAveragePooling2D(),\n",
        "    tf.keras.layers.Dense(128, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(len(classes), activation=\"softmax\"),\n",
        "])\n",
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "# -----------------------\n",
        "# 5) Train & evaluate\n",
        "# -----------------------\n",
        "EPOCHS = 15\n",
        "history = model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS, verbose=1)\n",
        "print(\"Val accuracy:\", history.history[\"val_accuracy\"][-1])\n",
        "\n",
        "# Confusion matrix (pure TF)\n",
        "y_val_true = tf.concat([y for _, y in val_ds], axis=0)\n",
        "y_val_pred = tf.argmax(model.predict(val_ds, verbose=0), axis=1)\n",
        "cm = tf.math.confusion_matrix(y_val_true, y_val_pred, num_classes=len(classes))\n",
        "print(\"Confusion matrix (validation):\")\n",
        "tf.print(cm)\n",
        "\n",
        "# Optional: per-class accuracy\n",
        "correct_per_class = tf.linalg.diag_part(cm)\n",
        "counts_per_class  = tf.reduce_sum(cm, axis=1)\n",
        "per_class_acc = tf.math.divide_no_nan(tf.cast(correct_per_class, tf.float32),\n",
        "                                      tf.cast(counts_per_class, tf.float32))\n",
        "for i, acc in enumerate(tf.unstack(per_class_acc)):\n",
        "    print(f\"{classes[i]:<10s}: {acc.numpy():.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "X-cYBlFWorNU",
        "outputId": "3d31af02-7d3c-4167-d8ea-a12fea0a4429"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-652007799.py:31: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  y, sr = librosa.load(path, sr=sr, mono=True, duration=duration, res_type=\"kaiser_fast\")\n",
            "/usr/local/lib/python3.12/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kept: 999 samples\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_5\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_5\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_15 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m320\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_10 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_16 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_11 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_17 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d_5      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m16,512\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,290\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d_5      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m110,474\u001b[0m (431.54 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">110,474</span> (431.54 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m110,474\u001b[0m (431.54 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">110,474</span> (431.54 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 458ms/step - accuracy: 0.1222 - loss: 2.3078 - val_accuracy: 0.1350 - val_loss: 2.3024\n",
            "Epoch 2/15\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 443ms/step - accuracy: 0.1491 - loss: 2.2934 - val_accuracy: 0.2400 - val_loss: 2.2556\n",
            "Epoch 3/15\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 529ms/step - accuracy: 0.1913 - loss: 2.2211 - val_accuracy: 0.1850 - val_loss: 2.1089\n",
            "Epoch 4/15\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 459ms/step - accuracy: 0.2023 - loss: 2.0606 - val_accuracy: 0.2000 - val_loss: 2.0441\n",
            "Epoch 5/15\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 452ms/step - accuracy: 0.2308 - loss: 2.0535 - val_accuracy: 0.2350 - val_loss: 1.9994\n",
            "Epoch 6/15\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 437ms/step - accuracy: 0.2450 - loss: 2.0183 - val_accuracy: 0.2350 - val_loss: 1.9753\n",
            "Epoch 7/15\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 486ms/step - accuracy: 0.2542 - loss: 1.9858 - val_accuracy: 0.2450 - val_loss: 1.9675\n",
            "Epoch 8/15\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 452ms/step - accuracy: 0.2789 - loss: 1.9552 - val_accuracy: 0.2800 - val_loss: 1.9070\n",
            "Epoch 9/15\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 452ms/step - accuracy: 0.2913 - loss: 1.8878 - val_accuracy: 0.3250 - val_loss: 1.8623\n",
            "Epoch 10/15\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 428ms/step - accuracy: 0.3345 - loss: 1.8003 - val_accuracy: 0.3050 - val_loss: 1.7628\n",
            "Epoch 11/15\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 459ms/step - accuracy: 0.3051 - loss: 1.8288 - val_accuracy: 0.3050 - val_loss: 1.7235\n",
            "Epoch 12/15\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 424ms/step - accuracy: 0.3522 - loss: 1.7433 - val_accuracy: 0.3250 - val_loss: 1.7158\n",
            "Epoch 13/15\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 475ms/step - accuracy: 0.3625 - loss: 1.7336 - val_accuracy: 0.3450 - val_loss: 1.6925\n",
            "Epoch 14/15\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 423ms/step - accuracy: 0.3638 - loss: 1.7122 - val_accuracy: 0.3250 - val_loss: 1.7188\n",
            "Epoch 15/15\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 438ms/step - accuracy: 0.3406 - loss: 1.7451 - val_accuracy: 0.3350 - val_loss: 1.6814\n",
            "Val accuracy: 0.33500000834465027\n",
            "Confusion matrix (validation):\n",
            "[[2 2 0 ... 0 0 2]\n",
            " [0 23 0 ... 0 0 1]\n",
            " [0 11 3 ... 0 0 1]\n",
            " ...\n",
            " [0 0 0 ... 4 1 1]\n",
            " [0 1 2 ... 5 7 2]\n",
            " [0 2 4 ... 2 0 5]]\n",
            "blues     : 0.143\n",
            "classical : 0.958\n",
            "country   : 0.111\n",
            "disco     : 0.333\n",
            "hiphop    : 0.000\n",
            "jazz      : 0.176\n",
            "metal     : 0.842\n",
            "pop       : 0.250\n",
            "reggae    : 0.318\n",
            "rock      : 0.200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Compare Tabular (MFCC) vs Image (Mel-spec) =====\n",
        "\n",
        "# -----------------------\n",
        "# 0) Config\n",
        "# -----------------------\n",
        "AUDIO_ROOT   = \"/content/drive/MyDrive/Elevvo Internship/Task 6/gtzan/Data/genres_original\"  # folder with 10 genre subfolders\n",
        "SR           = 22050              # fixed sample rate (faster & consistent)\n",
        "DUR          = 10                 # seconds to load (speed knob)\n",
        "N_MFCC       = 13                 # MFCC count (tabular features)\n",
        "HOP_MFCC     = 1024\n",
        "N_MELS       = 128\n",
        "HOP_MEL      = 512\n",
        "TARGET_MELS  = 128\n",
        "TARGET_FRAMES= 128\n",
        "BATCH        = 16\n",
        "EPOCHS       = 12\n",
        "SEED         = 42\n",
        "\n",
        "# -----------------------\n",
        "# 1) Collect paths + labels (no os)\n",
        "# -----------------------\n",
        "wav_paths = find_files(AUDIO_ROOT, ext=[\"wav\"], recurse=True)\n",
        "labels_raw = [p.replace(\"\\\\\", \"/\").split(\"/\")[-2] for p in wav_paths]\n",
        "\n",
        "# Build integer labels (no sklearn needed here, but we'll also keep string labels)\n",
        "classes = sorted(set(labels_raw))\n",
        "cls2id  = {c:i for i,c in enumerate(classes)}\n",
        "y_ids   = [cls2id[c] for c in labels_raw]\n",
        "\n",
        "# -----------------------\n",
        "# 2) One-pass feature builder: MFCC (tabular) + Mel-spec (image)\n",
        "#    - robust: skip unreadable files\n",
        "# -----------------------\n",
        "def extract_both_safe(path):\n",
        "    \"\"\"\n",
        "    Returns (mfcc_vec:list[float], mel_tensor:tf.Tensor[H,W,1]) or (None, None) on failure.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        y, sr = librosa.load(path, sr=SR, mono=True, duration=DUR, res_type=\"kaiser_fast\")\n",
        "        if y is None or len(y) == 0:\n",
        "            return None, None\n",
        "\n",
        "        # MFCC mean features (tabular)\n",
        "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=N_MFCC, hop_length=HOP_MFCC)\n",
        "        mfcc_vec = [float(v) for v in mfcc.mean(axis=1)]  # 13 numbers\n",
        "\n",
        "        # Mel-spectrogram image (tensor)\n",
        "        S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=N_MELS, hop_length=HOP_MEL)\n",
        "        S_db = librosa.power_to_db(S, ref=lambda x: S.max() if S.max() != 0 else 1.0)  # [mels,T]\n",
        "        t = tf.convert_to_tensor(S_db, dtype=tf.float32)[:, :, None]                   # [mels,T,1]\n",
        "        # resize mel bins, then pad/crop time to fixed width\n",
        "        t = tf.image.resize(t, size=(TARGET_MELS, tf.shape(t)[1]))\n",
        "        cur_T = tf.shape(t)[1]\n",
        "        t = tf.cond(cur_T < TARGET_FRAMES,\n",
        "                    lambda: tf.pad(t, [[0,0],[0, TARGET_FRAMES - cur_T],[0,0]]),\n",
        "                    lambda: t[:, :TARGET_FRAMES, :])\n",
        "        t = t[:, :TARGET_FRAMES, :]\n",
        "        # per-sample min-max normalize to [0,1]\n",
        "        t_min, t_max = tf.reduce_min(t), tf.reduce_max(t)\n",
        "        t = tf.cond(t_max > t_min, lambda: (t - t_min) / (t_max - t_min), lambda: tf.zeros_like(t))\n",
        "        return mfcc_vec, t\n",
        "    except Exception:\n",
        "        return None, None\n",
        "\n",
        "X_mfcc_all, X_mel_tensors, y_all_ids, y_all_labels = [], [], [], []\n",
        "skipped = 0\n",
        "for p, lab_id, lab_str in zip(wav_paths, y_ids, labels_raw):\n",
        "    f_tab, f_img = extract_both_safe(p)\n",
        "    if f_tab is None or f_img is None:\n",
        "        skipped += 1\n",
        "        continue\n",
        "    X_mfcc_all.append(f_tab)\n",
        "    X_mel_tensors.append(f_img)\n",
        "    y_all_ids.append(lab_id)\n",
        "    y_all_labels.append(lab_str)\n",
        "\n",
        "print(f\"Prepared samples: {len(X_mfcc_all)}  |  Skipped unreadable: {skipped}\")\n",
        "\n",
        "# Stack mel images\n",
        "X_mel_all = tf.stack(X_mel_tensors, axis=0)           # [N, 128, 128, 1]\n",
        "y_all = tf.constant(y_all_ids, dtype=tf.int32)\n",
        "\n",
        "# -----------------------\n",
        "# 3) One stratified split used by BOTH models\n",
        "# -----------------------\n",
        "idx_all = list(range(len(y_all_ids)))\n",
        "idx_tr, idx_va, y_tr_ids, y_va_ids = train_test_split(\n",
        "    idx_all, y_all_ids, test_size=0.2, stratify=y_all_ids, random_state=SEED\n",
        ")\n",
        "\n",
        "# Build splits for tabular\n",
        "Xtr_tab = [X_mfcc_all[i] for i in idx_tr]\n",
        "Xva_tab = [X_mfcc_all[i] for i in idx_va]\n",
        "ytr_tab = y_tr_ids\n",
        "yva_tab = y_va_ids\n",
        "\n",
        "# Build splits for images\n",
        "Xtr_img = tf.gather(X_mel_all, tf.constant(idx_tr, dtype=tf.int32))\n",
        "Xva_img = tf.gather(X_mel_all, tf.constant(idx_va, dtype=tf.int32))\n",
        "ytr_img = tf.constant(ytr_tab, dtype=tf.int32)\n",
        "yva_img = tf.constant(yva_tab, dtype=tf.int32)\n",
        "\n",
        "# -----------------------\n",
        "# 4) TABULAR model: MFCCs + Logistic Regression\n",
        "# -----------------------\n",
        "# scale features\n",
        "scaler = StandardScaler()\n",
        "Xtr_tab_s = scaler.fit_transform(Xtr_tab)\n",
        "Xva_tab_s = scaler.transform(Xva_tab)\n",
        "\n",
        "clf = LogisticRegression(max_iter=1000, n_jobs=-1, multi_class=\"auto\")\n",
        "clf.fit(Xtr_tab_s, ytr_tab)\n",
        "yhat_tab = clf.predict(Xva_tab_s)\n",
        "\n",
        "acc_tab = accuracy_score(yva_tab, yhat_tab)\n",
        "print(\"\\n=== Tabular (MFCC + Logistic Regression) ===\")\n",
        "print(\"Accuracy:\", f\"{acc_tab:.4f}\")\n",
        "print(classification_report(yva_tab, yhat_tab, target_names=classes))\n",
        "\n",
        "# Confusion matrix (tabular) using TensorFlow for symmetry\n",
        "cm_tab = tf.math.confusion_matrix(\n",
        "    tf.constant(yva_tab, dtype=tf.int32),\n",
        "    tf.constant(yhat_tab, dtype=tf.int32),\n",
        "    num_classes=len(classes)\n",
        ")\n",
        "print(\"Confusion matrix (tabular):\")\n",
        "tf.print(cm_tab)\n",
        "\n",
        "# -----------------------\n",
        "# 5) IMAGE model: Mel-spectrogram + small CNN\n",
        "# -----------------------\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((Xtr_img, ytr_img)).batch(BATCH).prefetch(tf.data.AUTOTUNE)\n",
        "val_ds   = tf.data.Dataset.from_tensor_slices((Xva_img, yva_img)).batch(BATCH).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(TARGET_MELS, TARGET_FRAMES, 1)),\n",
        "    tf.keras.layers.Conv2D(32, 3, activation=\"relu\"), tf.keras.layers.MaxPooling2D(),\n",
        "    tf.keras.layers.Conv2D(64, 3, activation=\"relu\"), tf.keras.layers.MaxPooling2D(),\n",
        "    tf.keras.layers.Conv2D(128, 3, activation=\"relu\"),\n",
        "    tf.keras.layers.GlobalAveragePooling2D(),\n",
        "    tf.keras.layers.Dense(128, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(len(classes), activation=\"softmax\"),\n",
        "])\n",
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "history = model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS, verbose=1)\n",
        "\n",
        "# Evaluate\n",
        "val_acc_img = history.history[\"val_accuracy\"][-1]\n",
        "print(\"\\n=== Image (Mel-spectrogram + CNN) ===\")\n",
        "print(\"Validation accuracy:\", f\"{val_acc_img:.4f}\")\n",
        "\n",
        "y_pred_img = tf.argmax(model.predict(val_ds, verbose=0), axis=1)\n",
        "cm_img = tf.math.confusion_matrix(\n",
        "    tf.concat([y for _, y in val_ds], axis=0),\n",
        "    y_pred_img,\n",
        "    num_classes=len(classes)\n",
        ")\n",
        "print(\"Confusion matrix (image):\")\n",
        "tf.print(cm_img)\n",
        "\n",
        "# -----------------------\n",
        "# 6) Side-by-side comparison\n",
        "# -----------------------\n",
        "print(\"\\n=== Side-by-side ===\")\n",
        "print(f\"Tabular (MFCC + LR)  Acc: {acc_tab:.4f}\")\n",
        "print(f\"Image   (Mel + CNN)  Acc: {val_acc_img:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jxb_4sT9o_MN",
        "outputId": "52491cca-6e0b-4c87-b264-8fc50b71b3eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1310889868.py:46: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  y, sr = librosa.load(path, sr=SR, mono=True, duration=DUR, res_type=\"kaiser_fast\")\n",
            "/usr/local/lib/python3.12/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prepared samples: 999  |  Skipped unreadable: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Tabular (MFCC + Logistic Regression) ===\n",
            "Accuracy: 0.4400\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       blues       0.35      0.40      0.37        20\n",
            "   classical       0.88      0.75      0.81        20\n",
            "     country       0.32      0.35      0.33        20\n",
            "       disco       0.27      0.20      0.23        20\n",
            "      hiphop       0.25      0.30      0.27        20\n",
            "        jazz       0.26      0.30      0.28        20\n",
            "       metal       0.68      0.75      0.71        20\n",
            "         pop       0.64      0.80      0.71        20\n",
            "      reggae       0.42      0.40      0.41        20\n",
            "        rock       0.30      0.15      0.20        20\n",
            "\n",
            "    accuracy                           0.44       200\n",
            "   macro avg       0.44      0.44      0.43       200\n",
            "weighted avg       0.44      0.44      0.43       200\n",
            "\n",
            "Confusion matrix (tabular):\n",
            "[[8 0 2 ... 0 2 2]\n",
            " [1 15 1 ... 0 0 1]\n",
            " [2 1 7 ... 0 1 1]\n",
            " ...\n",
            " [0 0 0 ... 16 2 0]\n",
            " [2 0 2 ... 1 8 0]\n",
            " [3 0 4 ... 1 0 3]]\n",
            "Epoch 1/12\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 477ms/step - accuracy: 0.0940 - loss: 2.3044 - val_accuracy: 0.1000 - val_loss: 2.3046\n",
            "Epoch 2/12\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 426ms/step - accuracy: 0.1043 - loss: 2.2985 - val_accuracy: 0.1700 - val_loss: 2.1789\n",
            "Epoch 3/12\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 473ms/step - accuracy: 0.2137 - loss: 2.1578 - val_accuracy: 0.2350 - val_loss: 2.1187\n",
            "Epoch 4/12\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 437ms/step - accuracy: 0.2488 - loss: 2.0816 - val_accuracy: 0.2200 - val_loss: 2.0695\n",
            "Epoch 5/12\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 473ms/step - accuracy: 0.2564 - loss: 2.0135 - val_accuracy: 0.2100 - val_loss: 2.0474\n",
            "Epoch 6/12\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 530ms/step - accuracy: 0.2619 - loss: 1.9739 - val_accuracy: 0.2350 - val_loss: 1.9963\n",
            "Epoch 7/12\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 467ms/step - accuracy: 0.3007 - loss: 1.9577 - val_accuracy: 0.2650 - val_loss: 1.9764\n",
            "Epoch 8/12\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 438ms/step - accuracy: 0.3141 - loss: 1.8830 - val_accuracy: 0.3200 - val_loss: 1.8748\n",
            "Epoch 9/12\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 462ms/step - accuracy: 0.3314 - loss: 1.7877 - val_accuracy: 0.2600 - val_loss: 1.9744\n",
            "Epoch 10/12\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 467ms/step - accuracy: 0.3299 - loss: 1.7226 - val_accuracy: 0.3050 - val_loss: 1.7792\n",
            "Epoch 11/12\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 511ms/step - accuracy: 0.3458 - loss: 1.6693 - val_accuracy: 0.2850 - val_loss: 1.9118\n",
            "Epoch 12/12\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 461ms/step - accuracy: 0.3504 - loss: 1.6496 - val_accuracy: 0.3200 - val_loss: 1.7644\n",
            "\n",
            "=== Image (Mel-spectrogram + CNN) ===\n",
            "Validation accuracy: 0.3200\n",
            "Confusion matrix (image):\n",
            "[[4 0 4 ... 0 3 3]\n",
            " [0 20 0 ... 0 0 0]\n",
            " [1 9 3 ... 0 0 4]\n",
            " ...\n",
            " [0 0 8 ... 0 8 1]\n",
            " [0 0 8 ... 0 7 3]\n",
            " [3 2 5 ... 0 0 5]]\n",
            "\n",
            "=== Side-by-side ===\n",
            "Tabular (MFCC + LR)  Acc: 0.4400\n",
            "Image   (Mel + CNN)  Acc: 0.3200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Compare CSV tabular vs PNG spectrograms =====\n",
        "# - Tabular: reads features_30_sec.csv (or features_3_sec.csv), trains Logistic Regression\n",
        "# - Image:   reads images_original/*/*.png, trains a small CNN\n",
        "\n",
        "# ------------- PATHS (edit these to your dataset locations) -------------\n",
        "CSV_PATH      = \"/content/drive/MyDrive/Elevvo Internship/Task 6/gtzan/Data/features_30_sec.csv\"     # or \"features_3_sec.csv\"\n",
        "IMAGES_ROOT   = \"/content/drive/MyDrive/Elevvo Internship/Task 6/gtzan/Data/images_original\"          # with 10 genre subfolders\n",
        "\n",
        "SEED          = 42\n",
        "TAB_TEST_SIZE = 0.2\n",
        "IMG_TEST_SIZE = 0.2\n",
        "\n",
        "# ============================\n",
        "# A) TABULAR: CSV -> scikit-learn\n",
        "# ============================\n",
        "# 1) Read header and detect label column\n",
        "with open(CSV_PATH, \"r\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    reader = csv.reader(f)\n",
        "    header = next(reader)\n",
        "\n",
        "label_candidates = {\"label\", \"genre\", \"class\", \"target\"}\n",
        "label_idx = None\n",
        "for i, col in enumerate(header):\n",
        "    if col.strip().lower() in label_candidates:\n",
        "        label_idx = i\n",
        "        break\n",
        "if label_idx is None:\n",
        "    raise ValueError(f\"Could not find label column in CSV header: {header}\")\n",
        "\n",
        "# 2) Choose feature columns (MFCCs + common spectral stats if present)\n",
        "def want_feature(colname: str) -> bool:\n",
        "    n = colname.strip().lower()\n",
        "    return (\n",
        "        n.startswith(\"mfcc\") or\n",
        "        n in {\n",
        "            \"chroma_stft_mean\", \"spectral_centroid_mean\",\n",
        "            \"spectral_bandwidth_mean\", \"rolloff_mean\",\n",
        "            \"zcr_mean\", \"rms_mean\"\n",
        "        }\n",
        "    )\n",
        "\n",
        "feat_indices = [i for i, c in enumerate(header) if i != label_idx and want_feature(c)]\n",
        "if not feat_indices:\n",
        "    raise ValueError(\"No feature columns detected. Inspect your CSV header and adjust selectors.\")\n",
        "\n",
        "# 3) Load rows\n",
        "X_tab, y_tab = [], []\n",
        "with open(CSV_PATH, \"r\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    reader = csv.reader(f)\n",
        "    next(reader)  # skip header\n",
        "    for row in reader:\n",
        "        try:\n",
        "            feats = [float(row[i]) for i in feat_indices]\n",
        "            label = row[label_idx]\n",
        "        except Exception:\n",
        "            # Skip malformed rows\n",
        "            continue\n",
        "        X_tab.append(feats)\n",
        "        y_tab.append(label)\n",
        "\n",
        "# 4) Encode, split, scale, train\n",
        "le_tab = LabelEncoder()\n",
        "y_tab_enc = le_tab.fit_transform(y_tab)\n",
        "\n",
        "Xtr_tab, Xte_tab, ytr_tab, yte_tab = train_test_split(\n",
        "    X_tab, y_tab_enc, test_size=TAB_TEST_SIZE, stratify=y_tab_enc, random_state=SEED\n",
        ")\n",
        "scaler = StandardScaler()\n",
        "Xtr_tab_s = scaler.fit_transform(Xtr_tab)\n",
        "Xte_tab_s = scaler.transform(Xte_tab)\n",
        "\n",
        "clf = LogisticRegression(max_iter=1000, n_jobs=-1, multi_class=\"auto\")\n",
        "clf.fit(Xtr_tab_s, ytr_tab)\n",
        "yhat_tab = clf.predict(Xte_tab_s)\n",
        "\n",
        "acc_tab = accuracy_score(yte_tab, yhat_tab)\n",
        "print(\"\\n=== TABULAR (CSV + Logistic Regression) ===\")\n",
        "print(\"Accuracy:\", f\"{acc_tab:.4f}\")\n",
        "print(classification_report(yte_tab, yhat_tab, target_names=list(le_tab.classes_)))\n",
        "\n",
        "# ==========================================\n",
        "# B) IMAGE: PNG spectrograms -> Keras CNN\n",
        "# ==========================================\n",
        "# 1) List PNG files and infer labels from parent folders (pure TF)\n",
        "#    Pattern: images_original/<genre>/*.png\n",
        "png_paths = tf.io.gfile.glob(IMAGES_ROOT + \"/*/*.png\")\n",
        "if not png_paths:\n",
        "    raise ValueError(f\"No PNGs found under {IMAGES_ROOT}. Check path or file extensions.\")\n",
        "\n",
        "# Normalize separators and extract folder name as label\n",
        "def parent_folder(path: str) -> str:\n",
        "    p = path.replace(\"\\\\\", \"/\")\n",
        "    parts = p.split(\"/\")\n",
        "    return parts[-2]  # parent directory\n",
        "\n",
        "labels_img_str = [parent_folder(p) for p in png_paths]\n",
        "classes_img = sorted(set(labels_img_str))\n",
        "cls2id_img = {c: i for i, c in enumerate(classes_img)}\n",
        "y_img_ids = [cls2id_img[c] for c in labels_img_str]\n",
        "\n",
        "# Optionally ensure the same 10 genres as CSV (usually identical in GTZAN)\n",
        "# If not identical, this still evaluates each branch independently.\n",
        "\n",
        "# 2) Train/val split for images\n",
        "idx_all = list(range(len(png_paths)))\n",
        "idx_tr, idx_va, y_tr_ids, y_va_ids = train_test_split(\n",
        "    idx_all, y_img_ids, test_size=IMG_TEST_SIZE, stratify=y_img_ids, random_state=SEED\n",
        ")\n",
        "\n",
        "paths_tr = [png_paths[i] for i in idx_tr]\n",
        "paths_va = [png_paths[i] for i in idx_va]\n",
        "ytr_img  = tf.constant(y_tr_ids, dtype=tf.int32)\n",
        "yva_img  = tf.constant(y_va_ids, dtype=tf.int32)\n",
        "\n",
        "IMG_SIZE = (128, 128)\n",
        "BATCH    = 32\n",
        "EPOCHS   = 12\n",
        "\n",
        "# 3) tf.data pipeline: read/decode PNG, resize, normalize to [0,1]\n",
        "def load_png(path, label):\n",
        "    img = tf.io.read_file(path)\n",
        "    img = tf.image.decode_png(img, channels=3)  # spectrograms usually RGB-like\n",
        "    img = tf.image.resize(img, IMG_SIZE)\n",
        "    img = tf.image.convert_image_dtype(img, tf.float32)  # [0,1]\n",
        "    return img, label\n",
        "\n",
        "ds_tr = tf.data.Dataset.from_tensor_slices((paths_tr, ytr_img)) \\\n",
        "        .shuffle(len(paths_tr), seed=SEED) \\\n",
        "        .map(load_png, num_parallel_calls=tf.data.AUTOTUNE) \\\n",
        "        .batch(BATCH) \\\n",
        "        .prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "ds_va = tf.data.Dataset.from_tensor_slices((paths_va, yva_img)) \\\n",
        "        .map(load_png, num_parallel_calls=tf.data.AUTOTUNE) \\\n",
        "        .batch(BATCH) \\\n",
        "        .prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# 4) Simple CNN for image classification\n",
        "num_classes_img = len(classes_img)\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=IMG_SIZE + (3,)),\n",
        "    tf.keras.layers.Conv2D(32, 3, activation=\"relu\"), tf.keras.layers.MaxPooling2D(),\n",
        "    tf.keras.layers.Conv2D(64, 3, activation=\"relu\"), tf.keras.layers.MaxPooling2D(),\n",
        "    tf.keras.layers.Conv2D(128, 3, activation=\"relu\"),\n",
        "    tf.keras.layers.GlobalAveragePooling2D(),\n",
        "    tf.keras.layers.Dense(128, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(num_classes_img, activation=\"softmax\"),\n",
        "])\n",
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "history = model.fit(ds_tr, validation_data=ds_va, epochs=EPOCHS, verbose=1)\n",
        "acc_img = history.history[\"val_accuracy\"][-1]\n",
        "\n",
        "# Confusion matrix (image branch)\n",
        "y_pred_img = tf.argmax(model.predict(ds_va, verbose=0), axis=1, output_type=tf.int32)\n",
        "y_true_img = tf.concat([y for _, y in ds_va], axis=0)\n",
        "cm_img = tf.math.confusion_matrix(y_true_img, y_pred_img, num_classes=num_classes_img)\n",
        "\n",
        "print(\"\\n=== IMAGE (PNG spectrograms + CNN) ===\")\n",
        "print(\"Validation accuracy:\", f\"{acc_img:.4f}\")\n",
        "print(\"Confusion matrix (image):\")\n",
        "tf.print(cm_img)\n",
        "\n",
        "# =======================\n",
        "# C) Side-by-side summary\n",
        "# =======================\n",
        "print(\"\\n=== Side-by-side ===\")\n",
        "print(f\"Tabular (CSV + LR)   Acc: {acc_tab:.4f}  (classes: {len(le_tab.classes_)})\")\n",
        "print(f\"Image   (PNG + CNN)  Acc: {acc_img:.4f}  (classes: {num_classes_img})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LdJZp5-LsH7A",
        "outputId": "a32b4d36-d1f0-4146-86a8-3e6e3039e271"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== TABULAR (CSV + Logistic Regression) ===\n",
            "Accuracy: 0.6700\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       blues       0.72      0.65      0.68        20\n",
            "   classical       0.90      0.95      0.93        20\n",
            "     country       0.55      0.60      0.57        20\n",
            "       disco       0.60      0.45      0.51        20\n",
            "      hiphop       0.50      0.60      0.55        20\n",
            "        jazz       0.71      0.85      0.77        20\n",
            "       metal       0.94      0.80      0.86        20\n",
            "         pop       0.83      0.95      0.88        20\n",
            "      reggae       0.52      0.55      0.54        20\n",
            "        rock       0.40      0.30      0.34        20\n",
            "\n",
            "    accuracy                           0.67       200\n",
            "   macro avg       0.67      0.67      0.66       200\n",
            "weighted avg       0.67      0.67      0.66       200\n",
            "\n",
            "Epoch 1/12\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 2s/step - accuracy: 0.1167 - loss: 8.9742 - val_accuracy: 0.1940 - val_loss: 2.2191\n",
            "Epoch 2/12\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 1s/step - accuracy: 0.2251 - loss: 2.1682 - val_accuracy: 0.2687 - val_loss: 2.0175\n",
            "Epoch 3/12\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 1s/step - accuracy: 0.2873 - loss: 2.0011 - val_accuracy: 0.3234 - val_loss: 1.8802\n",
            "Epoch 4/12\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 1s/step - accuracy: 0.2924 - loss: 1.8740 - val_accuracy: 0.3632 - val_loss: 1.8513\n",
            "Epoch 5/12\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 1s/step - accuracy: 0.3311 - loss: 1.8606 - val_accuracy: 0.3980 - val_loss: 1.7272\n",
            "Epoch 6/12\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 1s/step - accuracy: 0.3804 - loss: 1.6779 - val_accuracy: 0.3881 - val_loss: 1.6852\n",
            "Epoch 7/12\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 1s/step - accuracy: 0.3849 - loss: 1.7126 - val_accuracy: 0.4577 - val_loss: 1.6693\n",
            "Epoch 8/12\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 1s/step - accuracy: 0.4415 - loss: 1.5538 - val_accuracy: 0.4577 - val_loss: 1.4681\n",
            "Epoch 9/12\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1s/step - accuracy: 0.4811 - loss: 1.4841 - val_accuracy: 0.4975 - val_loss: 1.4784\n",
            "Epoch 10/12\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 1s/step - accuracy: 0.4913 - loss: 1.4805 - val_accuracy: 0.4975 - val_loss: 1.5763\n",
            "Epoch 11/12\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 1s/step - accuracy: 0.4498 - loss: 1.5241 - val_accuracy: 0.4129 - val_loss: 1.7371\n",
            "Epoch 12/12\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1s/step - accuracy: 0.5199 - loss: 1.3818 - val_accuracy: 0.4925 - val_loss: 1.4559\n",
            "\n",
            "=== IMAGE (PNG spectrograms + CNN) ===\n",
            "Validation accuracy: 0.4925\n",
            "Confusion matrix (image):\n",
            "[[5 1 3 ... 6 0 1]\n",
            " [1 16 3 ... 0 0 0]\n",
            " [1 0 13 ... 0 0 3]\n",
            " ...\n",
            " [1 1 1 ... 9 0 1]\n",
            " [0 0 1 ... 9 0 0]\n",
            " [0 0 1 ... 5 0 9]]\n",
            "\n",
            "=== Side-by-side ===\n",
            "Tabular (CSV + LR)   Acc: 0.6700  (classes: 10)\n",
            "Image   (PNG + CNN)  Acc: 0.4925  (classes: 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Transfer Learning on Mel-Spectrograms =====\n",
        "\n",
        "# -----------------------\n",
        "# CONFIG\n",
        "# -----------------------\n",
        "AUDIO_ROOT   = \"/content/drive/MyDrive/Elevvo Internship/Task 6/gtzan/Data/genres_original\"  # <-- set your real path\n",
        "SR           = 22050\n",
        "DUR          = 10\n",
        "N_MELS       = 128\n",
        "HOP_LENGTH   = 512\n",
        "IMG_SIZE     = (224, 224)        # MobileNet/EfficientNet default\n",
        "BATCH        = 16\n",
        "EPOCHS_HEAD  = 5\n",
        "EPOCHS_FT    = 10\n",
        "SEED         = 42\n",
        "\n",
        "# -----------------------\n",
        "# 1) Find WAVs + labels\n",
        "# -----------------------\n",
        "# Try wav/WAV or any audio if needed\n",
        "wav_paths = []\n",
        "for exts in ([\"wav\"], [\"WAV\"], None):  # None = all supported audio extensions\n",
        "    cand = find_files(AUDIO_ROOT, ext=exts, recurse=True)\n",
        "    if len(cand) > len(wav_paths):\n",
        "        wav_paths = cand\n",
        "\n",
        "print(f\"Found {len(wav_paths)} audio files under: {AUDIO_ROOT}\")\n",
        "if len(wav_paths) == 0:\n",
        "    raise ValueError(\"No audio files found. Double-check AUDIO_ROOT or your Drive mount.\")\n",
        "\n",
        "labels = [p.replace(\"\\\\\", \"/\").split(\"/\")[-2] for p in wav_paths]\n",
        "classes = sorted(set(labels))\n",
        "cls2id  = {c:i for i,c in enumerate(classes)}\n",
        "y_ids   = [cls2id[c] for c in labels]\n",
        "\n",
        "# -----------------------\n",
        "# 2) Robust per-class split with fallback\n",
        "# -----------------------\n",
        "idx_per_class = {c: [] for c in range(len(classes))}\n",
        "for i, cid in enumerate(y_ids):\n",
        "    idx_per_class[cid].append(i)\n",
        "\n",
        "train_idx, val_idx = [], []\n",
        "for cid, idxs in idx_per_class.items():\n",
        "    if len(idxs) == 0:\n",
        "        continue\n",
        "    idxs_tf = tf.random.shuffle(tf.constant(idxs, dtype=tf.int32), seed=SEED)\n",
        "    # use floor (not round) and keep at least one for train if possible\n",
        "    k = tf.cast(tf.math.floor(0.2 * tf.cast(tf.shape(idxs_tf)[0], tf.float32)), tf.int32)\n",
        "    k = tf.minimum(k, tf.shape(idxs_tf)[0] - 1)  # ensure ≥1 in train when class has ≥1 sample\n",
        "    val_idx.extend(idxs_tf[:k].numpy().tolist())\n",
        "    train_idx.extend(idxs_tf[k:].numpy().tolist())\n",
        "\n",
        "# Fallback to global 80/20 if per-class split broke\n",
        "if len(train_idx) == 0:\n",
        "    all_idx = tf.random.shuffle(tf.range(len(wav_paths)), seed=SEED).numpy().tolist()\n",
        "    cut = max(1, int(0.8 * len(all_idx)))\n",
        "    train_idx, val_idx = all_idx[:cut], all_idx[cut:]\n",
        "\n",
        "paths_tr = [wav_paths[i] for i in train_idx]\n",
        "paths_va = [wav_paths[i] for i in val_idx]\n",
        "labels_tr = [y_ids[i] for i in train_idx]\n",
        "labels_va = [y_ids[i] for i in val_idx]\n",
        "\n",
        "print(f\"Train files: {len(paths_tr)} | Val files: {len(paths_va)}\")\n",
        "if len(paths_tr) == 0:\n",
        "    raise ValueError(\"Empty training set after split — verify AUDIO_ROOT and class folders.\")\n",
        "\n",
        "# -----------------------\n",
        "# 3) WAV → Mel-spectrogram → 224x224x3\n",
        "# -----------------------\n",
        "def _wav_to_melspec_224x224x3(path_bytes):\n",
        "    path = path_bytes.numpy().decode(\"utf-8\")\n",
        "    try:\n",
        "        # Force librosa to use audioread (works with mp3 disguised as wav)\n",
        "        y, sr = librosa.load(path, sr=SR, mono=True, duration=DUR, res_type=\"kaiser_fast\", backend=\"audioread\")\n",
        "    except Exception as e:\n",
        "        # If unreadable, return a blank spectrogram\n",
        "        #print(f\"⚠️ Skipping unreadable file: {path} ({e})\")\n",
        "        return tf.zeros(IMG_SIZE + (3,), dtype=tf.float32)\n",
        "\n",
        "    if y is None or len(y) == 0:\n",
        "        return tf.zeros(IMG_SIZE + (3,), dtype=tf.float32)\n",
        "\n",
        "    # Mel spectrogram\n",
        "    S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=N_MELS, hop_length=HOP_LENGTH)\n",
        "    S_db = librosa.power_to_db(S, ref=lambda x: S.max() if S.max() != 0 else 1.0)\n",
        "\n",
        "    # Convert → tensor [H,W,1]\n",
        "    t = tf.convert_to_tensor(S_db, dtype=tf.float32)[:, :, None]\n",
        "\n",
        "    # Normalize per-sample\n",
        "    t_min, t_max = tf.reduce_min(t), tf.reduce_max(t)\n",
        "    t = tf.cond(t_max > t_min, lambda: (t - t_min) / (t_max - t_min), lambda: tf.zeros_like(t))\n",
        "\n",
        "    # Resize + tile to 3 channels\n",
        "    t = tf.image.resize(t, IMG_SIZE)\n",
        "    t = tf.tile(t, [1, 1, 3])\n",
        "    return t\n",
        "\n",
        "def tf_wav_to_melspec_224x224x3(path):\n",
        "    img = tf.py_function(_wav_to_melspec_224x224x3, [path], tf.float32)\n",
        "    img.set_shape(IMG_SIZE + (3,))  # static shape for batching/model\n",
        "    return img\n",
        "\n",
        "# -----------------------\n",
        "# 4) tf.data pipelines (from Python lists; known lengths)\n",
        "# -----------------------\n",
        "def make_ds(paths_list, labels_list, training=True):\n",
        "    ds = tf.data.Dataset.from_tensor_slices((paths_list, labels_list))\n",
        "    if training:\n",
        "        ds = ds.shuffle(buffer_size=max(1, len(paths_list)), seed=SEED)\n",
        "    ds = ds.map(lambda p, y: (tf_wav_to_melspec_224x224x3(p), y),\n",
        "                num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    if training:\n",
        "        aug = tf.keras.Sequential([\n",
        "            tf.keras.layers.RandomFlip(\"horizontal\"),  # time-axis flip\n",
        "            tf.keras.layers.RandomContrast(0.1),\n",
        "            tf.keras.layers.RandomZoom(0.1),\n",
        "        ])\n",
        "        ds = ds.map(lambda x, y: (aug(x, training=True), y),\n",
        "                    num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    ds = ds.batch(BATCH).prefetch(tf.data.AUTOTUNE)\n",
        "    return ds\n",
        "\n",
        "train_ds = make_ds(paths_tr, labels_tr, training=True)\n",
        "val_ds   = make_ds(paths_va, labels_va, training=False)\n",
        "\n",
        "steps_per_epoch  = max(1, math.ceil(len(paths_tr) / BATCH))\n",
        "validation_steps = max(1, math.ceil(len(paths_va) / BATCH))\n",
        "\n",
        "# -----------------------\n",
        "# 5) Transfer learning model (MobileNetV2)\n",
        "# -----------------------\n",
        "\n",
        "preproc = tf.keras.layers.Lambda(preprocess_input)\n",
        "base = MobileNetV2(input_shape=IMG_SIZE + (3,), include_top=False, weights=\"imagenet\")\n",
        "base.trainable = False\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=IMG_SIZE + (3,)),\n",
        "    preproc,\n",
        "    base,\n",
        "    tf.keras.layers.GlobalAveragePooling2D(),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "    tf.keras.layers.Dense(256, activation=\"relu\"),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "    tf.keras.layers.Dense(len(classes), activation=\"softmax\"),\n",
        "])\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# -----------------------\n",
        "# 6) Train classifier head\n",
        "# -----------------------\n",
        "hist_head = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=EPOCHS_HEAD,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    validation_steps=validation_steps,\n",
        "    verbose=1\n",
        ")\n",
        "print(\"Val acc after head training:\", hist_head.history[\"val_accuracy\"][-1])\n",
        "\n",
        "# -----------------------\n",
        "# 7) Fine-tune last blocks\n",
        "# -----------------------\n",
        "# Unfreeze from block_13 (≈ last third), or fallback to 70% boundary\n",
        "fine_tune_from = None\n",
        "for i, layer in enumerate(base.layers[::-1]):\n",
        "    if \"block_13\" in layer.name:\n",
        "        fine_tune_from = len(base.layers) - i\n",
        "        break\n",
        "if fine_tune_from is None:\n",
        "    fine_tune_from = int(0.7 * len(base.layers))\n",
        "\n",
        "for i, layer in enumerate(base.layers):\n",
        "    layer.trainable = (i >= fine_tune_from)\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "hist_ft = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=EPOCHS_FT,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    validation_steps=validation_steps,\n",
        "    verbose=1\n",
        ")\n",
        "print(\"Val acc after fine-tuning:\", hist_ft.history[\"val_accuracy\"][-1])\n",
        "\n",
        "# -----------------------\n",
        "# 8) Evaluation: confusion matrix & per-class accuracy\n",
        "# -----------------------\n",
        "y_true = tf.concat([y for _, y in val_ds], axis=0)\n",
        "y_prob = model.predict(val_ds, verbose=0, steps=validation_steps)\n",
        "y_pred = tf.argmax(y_prob, axis=1, output_type=tf.int32)\n",
        "\n",
        "cm = tf.math.confusion_matrix(y_true, y_pred, num_classes=len(classes))\n",
        "print(\"Confusion matrix:\")\n",
        "tf.print(cm)\n",
        "\n",
        "correct = tf.linalg.diag_part(cm)\n",
        "totals  = tf.reduce_sum(cm, axis=1)\n",
        "per_class_acc = tf.math.divide_no_nan(tf.cast(correct, tf.float32),\n",
        "                                      tf.cast(totals, tf.float32))\n",
        "print(\"\\nPer-class accuracy:\")\n",
        "for i, acc in enumerate(tf.unstack(per_class_acc)):\n",
        "    print(f\"{classes[i]:<10s}: {acc.numpy():.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "N3NnmGnd1VVo",
        "outputId": "fc80d9e9-242e-4968-8cc8-703029ea83f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1000 audio files under: /content/drive/MyDrive/Elevvo Internship/Task 6/gtzan/Data/genres_original\n",
            "Train files: 800 | Val files: 200\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_15\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_15\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ lambda_3 (\u001b[38;5;33mLambda\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ mobilenetv2_1.00_224            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m2,257,984\u001b[0m │\n",
              "│ (\u001b[38;5;33mFunctional\u001b[0m)                    │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d_11     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_22 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m327,936\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_23 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m2,570\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ lambda_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ mobilenetv2_1.00_224            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d_11     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">327,936</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,570</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,588,490\u001b[0m (9.87 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,588,490</span> (9.87 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m330,506\u001b[0m (1.26 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">330,506</span> (1.26 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 928ms/step - accuracy: 0.1127 - loss: 2.6617 - val_accuracy: 0.1000 - val_loss: 2.3775\n",
            "Epoch 2/5\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 860ms/step - accuracy: 0.0876 - loss: 2.4007 - val_accuracy: 0.1000 - val_loss: 2.3214\n",
            "Epoch 3/5\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 878ms/step - accuracy: 0.0985 - loss: 2.3423 - val_accuracy: 0.1000 - val_loss: 2.3044\n",
            "Epoch 4/5\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 930ms/step - accuracy: 0.0834 - loss: 2.3034 - val_accuracy: 0.1000 - val_loss: 2.3036\n",
            "Epoch 5/5\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 883ms/step - accuracy: 0.0982 - loss: 2.3151 - val_accuracy: 0.1000 - val_loss: 2.3058\n",
            "Val acc after head training: 0.10000000149011612\n",
            "Epoch 1/10\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 1s/step - accuracy: 0.1109 - loss: 2.3147 - val_accuracy: 0.1000 - val_loss: 2.3092\n",
            "Epoch 2/10\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 1s/step - accuracy: 0.0849 - loss: 2.3191 - val_accuracy: 0.1000 - val_loss: 2.3097\n",
            "Epoch 3/10\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 1s/step - accuracy: 0.0846 - loss: 2.3057 - val_accuracy: 0.1000 - val_loss: 2.3060\n",
            "Epoch 4/10\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 1s/step - accuracy: 0.1035 - loss: 2.3040 - val_accuracy: 0.1000 - val_loss: 2.3039\n",
            "Epoch 5/10\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 1s/step - accuracy: 0.0924 - loss: 2.3036 - val_accuracy: 0.1000 - val_loss: 2.3050\n",
            "Epoch 6/10\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 1s/step - accuracy: 0.0925 - loss: 2.3023 - val_accuracy: 0.1000 - val_loss: 2.3037\n",
            "Epoch 7/10\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 1s/step - accuracy: 0.0803 - loss: 2.3078 - val_accuracy: 0.1000 - val_loss: 2.3046\n",
            "Epoch 8/10\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 1s/step - accuracy: 0.1212 - loss: 2.3034 - val_accuracy: 0.1000 - val_loss: 2.3026\n",
            "Epoch 9/10\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 1s/step - accuracy: 0.0884 - loss: 2.3041 - val_accuracy: 0.1000 - val_loss: 2.3026\n",
            "Epoch 10/10\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 1s/step - accuracy: 0.1147 - loss: 2.3024 - val_accuracy: 0.1000 - val_loss: 2.3026\n",
            "Val acc after fine-tuning: 0.10000000149011612\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 21 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7c16bf20c9a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion matrix:\n",
            "[[20 0 0 ... 0 0 0]\n",
            " [20 0 0 ... 0 0 0]\n",
            " [20 0 0 ... 0 0 0]\n",
            " ...\n",
            " [20 0 0 ... 0 0 0]\n",
            " [20 0 0 ... 0 0 0]\n",
            " [20 0 0 ... 0 0 0]]\n",
            "\n",
            "Per-class accuracy:\n",
            "blues     : 1.000\n",
            "classical : 0.000\n",
            "country   : 0.000\n",
            "disco     : 0.000\n",
            "hiphop    : 0.000\n",
            "jazz      : 0.000\n",
            "metal     : 0.000\n",
            "pop       : 0.000\n",
            "reggae    : 0.000\n",
            "rock      : 0.000\n"
          ]
        }
      ]
    }
  ]
}