{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Level 3**\n",
        "**Task 8: Traffic Sign Recognition Description**"
      ],
      "metadata": {
        "id": "FI4nTIHS5y8n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Description:\n",
        "\n",
        "\n",
        "*   Dataset (Recommended): GTSRB (Kaggle).\n",
        "*   Classify traffic signs based on their image using deep learning.\n",
        "*   Preprocess images (resizing, normalization).\n",
        "*   Train a CNN model to recognize different traffic sign classes.\n",
        "*   Evaluate performance using accuracy and confusion matrix.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2gYwkRNb6AGn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tools & Libraries:\n",
        "\n",
        "\n",
        "*   Python\n",
        "*   Keras\n",
        "*   TensorFlow\n",
        "*   OpenCV\n",
        "\n",
        "\n",
        "Covered Topics:\n",
        "\n",
        "\n",
        "*   Computer vision (CNN)\n",
        "*   Multi-class classification\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ba1UMKVz6gZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bonus:\n",
        "\n",
        "\n",
        "*   Add data augmentation to improve performance.\n",
        "*   Compare custom CNN vs. pre-trained model (e.g., MobileNet).\n",
        "\n"
      ],
      "metadata": {
        "id": "FWFOF7oK6-et"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "piUGpDH44AD0"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import math\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVO-Ruxy_-9-",
        "outputId": "0b80f935-1c74-4e60-eb7e-431b59885352"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Preprocessing + CNN =====\n",
        "\n",
        "# -------------------------\n",
        "# Config\n",
        "# -------------------------\n",
        "TRAIN_DIR  = \"/content/drive/MyDrive/Elevvo Internship/Task 6/gtsrb\"\n",
        "IMG_SIZE   = 48\n",
        "BATCH_SIZE = 64\n",
        "VAL_SPLIT  = 0.2\n",
        "SEED       = 42\n",
        "AUTOTUNE   = tf.data.AUTOTUNE\n",
        "\n",
        "# If there are PPM images, keep this True to use OpenCV (PPM-friendly).\n",
        "USE_OPENCV = True\n",
        "\n",
        "# -------------------------\n",
        "# 1) Recursively list files (supports multiple depths)\n",
        "# -------------------------\n",
        "exts = [\"png\", \"ppm\", \"jpg\", \"jpeg\", \"PNG\", \"PPM\", \"JPG\", \"JPEG\"]\n",
        "\n",
        "def recursive_glob(root):\n",
        "    # Gather a few depth patterns explicitly (tf.io.gfile.glob has no **)\n",
        "    patterns = []\n",
        "    for ext in exts:\n",
        "        patterns += [\n",
        "            f\"{root}/*/*.{ext}\",\n",
        "            f\"{root}/*/*/*.{ext}\",\n",
        "            f\"{root}/*/*/*/*.{ext}\",\n",
        "            f\"{root}/*/*/*/*/*.{ext}\",\n",
        "        ]\n",
        "    files = []\n",
        "    for pat in patterns:\n",
        "        files.extend(tf.io.gfile.glob(pat))\n",
        "    return sorted(set(files))\n",
        "\n",
        "file_list = recursive_glob(TRAIN_DIR)\n",
        "print(\"Total files found (all depths):\", len(file_list))\n",
        "if len(file_list) == 0:\n",
        "    raise ValueError(\n",
        "        f\"No images found under {TRAIN_DIR}. \"\n",
        "        \"Check the path/mount and that the dataset is extracted.\"\n",
        "    )\n",
        "\n",
        "# -------------------------\n",
        "# 2) Extract label from the LAST numeric folder in the path\n",
        "# -------------------------\n",
        "def extract_numeric_label_from_path(path: str):\n",
        "    parts = path.replace(\"\\\\\", \"/\").split(\"/\")\n",
        "    for seg in reversed(parts[:-1]):  # skip filename\n",
        "        if re.fullmatch(r\"[0-9]+\", seg):\n",
        "            return int(seg)\n",
        "    return None\n",
        "\n",
        "paths, labels = [], []\n",
        "skipped_non_numeric = 0\n",
        "for p in file_list:\n",
        "    lab = extract_numeric_label_from_path(p)\n",
        "    if lab is None:\n",
        "        skipped_non_numeric += 1\n",
        "        continue\n",
        "    paths.append(p)\n",
        "    labels.append(lab)\n",
        "\n",
        "print(f\"Kept {len(paths)} files with numeric labels; skipped non-numeric parents: {skipped_non_numeric}\")\n",
        "if len(paths) == 0:\n",
        "    raise ValueError(\n",
        "        \"After scanning, no files had a numeric folder anywhere in their path. \"\n",
        "        \"You should have .../<digits>/file.ext somewhere.\"\n",
        "    )\n",
        "\n",
        "# Compact labels to 0..C-1\n",
        "unique_labels = sorted(set(labels))\n",
        "label_to_id = {lab: i for i, lab in enumerate(unique_labels)}\n",
        "labels_compact = [label_to_id[l] for l in labels]\n",
        "num_classes = len(unique_labels)\n",
        "print(f\"Detected {num_classes} classes.\")\n",
        "\n",
        "# -------------------------\n",
        "# 3) Deterministic 80/20 split (no sklearn)\n",
        "# -------------------------\n",
        "rng = tf.random.Generator.from_seed(SEED)\n",
        "perm = rng.uniform(shape=(len(paths),), minval=0, maxval=1, dtype=tf.float32).numpy().argsort()\n",
        "\n",
        "paths = [paths[i] for i in perm]\n",
        "labels_compact = [labels_compact[i] for i in perm]\n",
        "\n",
        "n = len(paths)\n",
        "n_val = max(1, int(n * VAL_SPLIT))\n",
        "val_paths  = paths[:n_val]\n",
        "val_labels = labels_compact[:n_val]\n",
        "train_paths  = paths[n_val:]\n",
        "train_labels = labels_compact[n_val:]\n",
        "\n",
        "print(f\"Train files: {len(train_paths)} | Val files: {len(val_paths)}\")\n",
        "if len(train_paths) == 0:\n",
        "    raise ValueError(\"Empty training set after split — reduce VAL_SPLIT or check dataset.\")\n",
        "\n",
        "# -------------------------\n",
        "# 4) Preprocessing (TF or OpenCV)\n",
        "# -------------------------\n",
        "def decode_resize_normalize_tf(path: tf.Tensor, label: tf.Tensor):\n",
        "    img_bytes = tf.io.read_file(path)\n",
        "    img = tf.io.decode_image(img_bytes, channels=3, expand_animations=False)  # (H,W,3) uint8\n",
        "    img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE), antialias=True)\n",
        "    img = tf.image.convert_image_dtype(img, tf.float32)  # [0,1]\n",
        "    return img, label\n",
        "\n",
        "# >>> FIXED: use .numpy().decode('utf-8') inside py_function\n",
        "def _decode_resize_normalize_cv2_py(path_tensor):\n",
        "    path = path_tensor.numpy().decode(\"utf-8\")\n",
        "    bgr = cv2.imread(path, cv2.IMREAD_COLOR)  # BGR, uint8\n",
        "    if bgr is None:\n",
        "        return (tf.zeros((IMG_SIZE, IMG_SIZE, 3), tf.float32)).numpy()\n",
        "    rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
        "    rgb = cv2.resize(rgb, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_AREA)\n",
        "    rgb = rgb.astype(\"float32\") / 255.0\n",
        "    return rgb\n",
        "\n",
        "def decode_resize_normalize_cv2(path: tf.Tensor, label: tf.Tensor):\n",
        "    img = tf.py_function(_decode_resize_normalize_cv2_py, [path], Tout=tf.float32)\n",
        "    img.set_shape((IMG_SIZE, IMG_SIZE, 3))  # static shape for batching\n",
        "    return img, label\n",
        "\n",
        "preproc = decode_resize_normalize_cv2 if USE_OPENCV else decode_resize_normalize_tf\n",
        "\n",
        "# -------------------------\n",
        "# 5) Build tf.data pipelines\n",
        "# -------------------------\n",
        "train_ds = (tf.data.Dataset.from_tensor_slices((train_paths, train_labels))\n",
        "            .shuffle(buffer_size=len(train_paths), seed=SEED, reshuffle_each_iteration=True)\n",
        "            .map(preproc, num_parallel_calls=AUTOTUNE))\n",
        "\n",
        "val_ds = (tf.data.Dataset.from_tensor_slices((val_paths, val_labels))\n",
        "          .map(preproc, num_parallel_calls=AUTOTUNE))\n",
        "\n",
        "# Optional augmentation (avoid horizontal flip for traffic signs)\n",
        "augment = keras.Sequential([\n",
        "    layers.RandomTranslation(0.06, 0.06, fill_mode=\"nearest\"),\n",
        "    layers.RandomContrast(0.1),\n",
        "    layers.RandomZoom(0.1),\n",
        "], name=\"augment\")\n",
        "\n",
        "def augment_if_needed(img, label):\n",
        "    img = augment(img, training=True)\n",
        "    return img, label\n",
        "\n",
        "train_ds = train_ds.map(augment_if_needed, num_parallel_calls=AUTOTUNE)\n",
        "train_ds = train_ds.cache().batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
        "val_ds   = val_ds.cache().batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
        "\n",
        "# -------------------------\n",
        "# 6) Small CNN model\n",
        "# -------------------------\n",
        "model = keras.Sequential([\n",
        "    layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3)),\n",
        "    layers.Conv2D(32, 3, activation=\"relu\"), layers.MaxPooling2D(),\n",
        "    layers.Conv2D(64, 3, activation=\"relu\"), layers.MaxPooling2D(),\n",
        "    layers.Conv2D(128, 3, activation=\"relu\"), layers.MaxPooling2D(),\n",
        "    layers.Dropout(0.25),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(256, activation=\"relu\"),\n",
        "    layers.Dropout(0.25),\n",
        "    layers.Dense(num_classes, activation=\"softmax\"),\n",
        "])\n",
        "\n",
        "model.compile(optimizer=\"adam\",\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "history = model.fit(train_ds, validation_data=val_ds, epochs=15)\n",
        "\n",
        "# -------------------------\n",
        "# 7) Evaluation\n",
        "# -------------------------\n",
        "val_loss, val_acc = model.evaluate(val_ds, verbose=0)\n",
        "print(f\"Validation accuracy: {val_acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6cftpHGEe-t",
        "outputId": "885499ec-e345-4bc9-fb32-49fee8c6838c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total files found (all depths): 19650\n",
            "Kept 6957 files with numeric labels; skipped non-numeric parents: 12693\n",
            "Detected 7 classes.\n",
            "Train files: 5566 | Val files: 1391\n",
            "Epoch 1/15\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m902s\u001b[0m 10s/step - accuracy: 0.3013 - loss: 1.6671 - val_accuracy: 0.6643 - val_loss: 0.8671\n",
            "Epoch 2/15\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 306ms/step - accuracy: 0.6858 - loss: 0.8038 - val_accuracy: 0.9482 - val_loss: 0.2177\n",
            "Epoch 3/15\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 290ms/step - accuracy: 0.9004 - loss: 0.3059 - val_accuracy: 0.9741 - val_loss: 0.1170\n",
            "Epoch 4/15\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 283ms/step - accuracy: 0.9524 - loss: 0.1617 - val_accuracy: 0.9885 - val_loss: 0.0581\n",
            "Epoch 5/15\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 303ms/step - accuracy: 0.9685 - loss: 0.1087 - val_accuracy: 0.9914 - val_loss: 0.0432\n",
            "Epoch 6/15\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 280ms/step - accuracy: 0.9774 - loss: 0.0785 - val_accuracy: 0.9928 - val_loss: 0.0304\n",
            "Epoch 7/15\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 299ms/step - accuracy: 0.9853 - loss: 0.0547 - val_accuracy: 0.9921 - val_loss: 0.0346\n",
            "Epoch 8/15\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 300ms/step - accuracy: 0.9854 - loss: 0.0591 - val_accuracy: 0.9942 - val_loss: 0.0227\n",
            "Epoch 9/15\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 279ms/step - accuracy: 0.9910 - loss: 0.0315 - val_accuracy: 0.9942 - val_loss: 0.0271\n",
            "Epoch 10/15\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 297ms/step - accuracy: 0.9952 - loss: 0.0217 - val_accuracy: 0.9928 - val_loss: 0.0228\n",
            "Epoch 11/15\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 287ms/step - accuracy: 0.9960 - loss: 0.0164 - val_accuracy: 0.9942 - val_loss: 0.0299\n",
            "Epoch 12/15\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 282ms/step - accuracy: 0.9956 - loss: 0.0172 - val_accuracy: 0.9942 - val_loss: 0.0248\n",
            "Epoch 13/15\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 303ms/step - accuracy: 0.9952 - loss: 0.0192 - val_accuracy: 0.9921 - val_loss: 0.0264\n",
            "Epoch 14/15\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 286ms/step - accuracy: 0.9953 - loss: 0.0180 - val_accuracy: 0.9935 - val_loss: 0.0192\n",
            "Epoch 15/15\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 271ms/step - accuracy: 0.9944 - loss: 0.0179 - val_accuracy: 0.9950 - val_loss: 0.0253\n",
            "Validation accuracy: 0.9950\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# Train a CNN\n",
        "# ================================\n",
        "\n",
        "# -------------------------\n",
        "# Config\n",
        "# -------------------------\n",
        "TRAIN_DIR  = \"/content/drive/MyDrive/Elevvo Internship/Task 6/gtsrb\"\n",
        "IMG_SIZE   = 48\n",
        "BATCH_SIZE = 64\n",
        "VAL_SPLIT  = 0.2\n",
        "EPOCHS     = 20\n",
        "SEED       = 42\n",
        "AUTOTUNE   = tf.data.AUTOTUNE\n",
        "USE_OPENCV = False  # True = OpenCV decode (ppm-friendly), False = pure TF\n",
        "\n",
        "# -------------------------\n",
        "# Robust file listing (no assertions if a pattern has no matches)\n",
        "# -------------------------\n",
        "exts = [\"png\", \"jpg\", \"jpeg\", \"ppm\", \"PNG\", \"JPG\", \"JPEG\", \"PPM\"]\n",
        "paths = []\n",
        "for ext in exts:\n",
        "    # two-level deep; add more /*/*/* if your data is deeper\n",
        "    paths += tf.io.gfile.glob(f\"{TRAIN_DIR}/*/*.{ext}\")\n",
        "    paths += tf.io.gfile.glob(f\"{TRAIN_DIR}/*/*/*.{ext}\")\n",
        "\n",
        "# Deduplicate & sort for deterministic split\n",
        "paths = sorted(set(paths))\n",
        "if not paths:\n",
        "    raise ValueError(f\"No image files found under {TRAIN_DIR}. \"\n",
        "                     \"Check the path and depth; add more /* if needed.\")\n",
        "\n",
        "# -------------------------\n",
        "# Label extraction: take the LAST numeric folder as class id\n",
        "# -------------------------\n",
        "def extract_label_from_path(p: str) -> int:\n",
        "    parts = p.replace(\"\\\\\", \"/\").split(\"/\")\n",
        "    # scan from the end (skip filename at [-1])\n",
        "    for seg in reversed(parts[:-1]):\n",
        "        if re.fullmatch(r\"[0-9]+\", seg):\n",
        "            return int(seg)\n",
        "    # if none found, return -1 to drop later\n",
        "    return -1\n",
        "\n",
        "labels = [extract_label_from_path(p) for p in paths]\n",
        "keep = [i for i, lab in enumerate(labels) if lab >= 0]\n",
        "paths = [paths[i] for i in keep]\n",
        "labels = [labels[i] for i in keep]\n",
        "\n",
        "if not paths:\n",
        "    raise ValueError(\"All files were filtered out because no numeric class folder was found \"\n",
        "                     \"(expected .../<digits>/filename.ext). Check your layout.\")\n",
        "\n",
        "# Remap labels to 0..C-1 (compact)\n",
        "unique_labs = sorted(set(labels))\n",
        "lab2id = {lab: i for i, lab in enumerate(unique_labs)}\n",
        "labels = [lab2id[lab] for lab in labels]\n",
        "num_classes = len(unique_labs)\n",
        "print(f\"Found {len(paths)} images across {num_classes} classes.\")\n",
        "\n",
        "# -------------------------\n",
        "# Build datasets (deterministic split)\n",
        "# -------------------------\n",
        "ds_all = tf.data.Dataset.from_tensor_slices((paths, labels))\n",
        "ds_all = ds_all.shuffle(buffer_size=len(paths), seed=SEED, reshuffle_each_iteration=False)\n",
        "\n",
        "n_total = len(paths)\n",
        "n_val = max(1, int(n_total * VAL_SPLIT))\n",
        "val_files = ds_all.take(n_val)\n",
        "trn_files = ds_all.skip(n_val)\n",
        "\n",
        "# Label tensor is second element from ds_all; we’ll map after preprocessing\n",
        "def path_to_label_from_pair(path, label):\n",
        "    return path, tf.cast(label, tf.int32)\n",
        "\n",
        "val_files = val_files.map(path_to_label_from_pair)\n",
        "trn_files = trn_files.map(path_to_label_from_pair)\n",
        "\n",
        "# -------------------------\n",
        "# Preprocessing\n",
        "# -------------------------\n",
        "def decode_resize_normalize_tf(path: tf.Tensor, label: tf.Tensor):\n",
        "    bytestr = tf.io.read_file(path)\n",
        "    img = tf.io.decode_image(bytestr, channels=3, expand_animations=False)\n",
        "    img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE), antialias=True)\n",
        "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "    return img, label\n",
        "\n",
        "def _preprocess_cv2_py(path_tensor):\n",
        "    path = path_tensor.numpy().decode(\"utf-8\")\n",
        "    bgr = cv2.imread(path, cv2.IMREAD_COLOR)\n",
        "    if bgr is None:\n",
        "        return (tf.zeros((IMG_SIZE, IMG_SIZE, 3), tf.float32)).numpy()\n",
        "    rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
        "    rgb = cv2.resize(rgb, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_AREA)\n",
        "    rgb = (rgb.astype(\"float32\") / 255.0)\n",
        "    return rgb\n",
        "\n",
        "def decode_resize_normalize_cv2(path: tf.Tensor, label: tf.Tensor):\n",
        "    img = tf.py_function(_preprocess_cv2_py, [path], Tout=tf.float32)\n",
        "    img.set_shape((IMG_SIZE, IMG_SIZE, 3))\n",
        "    return img, label\n",
        "\n",
        "preprocess = decode_resize_normalize_cv2 if USE_OPENCV else decode_resize_normalize_tf\n",
        "\n",
        "train_ds = trn_files.map(preprocess, num_parallel_calls=AUTOTUNE)\n",
        "val_ds   = val_files.map(preprocess, num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "# -------------------------\n",
        "# Augmentation (mild for traffic signs)\n",
        "# -------------------------\n",
        "augment = keras.Sequential([\n",
        "    layers.RandomTranslation(0.05, 0.05, fill_mode=\"nearest\"),\n",
        "    layers.RandomContrast(0.1),\n",
        "], name=\"augment\")\n",
        "\n",
        "def aug_fn(img, label):\n",
        "    return augment(img, training=True), label\n",
        "\n",
        "train_ds = (train_ds\n",
        "            .map(aug_fn, num_parallel_calls=AUTOTUNE)\n",
        "            .cache()\n",
        "            .shuffle(min(10000, n_total), seed=SEED, reshuffle_each_iteration=True)\n",
        "            .batch(BATCH_SIZE)\n",
        "            .prefetch(AUTOTUNE))\n",
        "\n",
        "val_ds = (val_ds\n",
        "          .cache()\n",
        "          .batch(BATCH_SIZE)\n",
        "          .prefetch(AUTOTUNE))\n",
        "\n",
        "# -------------------------\n",
        "# CNN model\n",
        "# -------------------------\n",
        "def build_model(num_classes):\n",
        "    return keras.Sequential([\n",
        "        layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3)),\n",
        "        layers.Conv2D(32, 3, padding=\"same\", activation=\"relu\"),\n",
        "        layers.Conv2D(32, 3, padding=\"same\", activation=\"relu\"),\n",
        "        layers.MaxPooling2D(), layers.Dropout(0.15),\n",
        "\n",
        "        layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\"),\n",
        "        layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\"),\n",
        "        layers.MaxPooling2D(), layers.Dropout(0.15),\n",
        "\n",
        "        layers.Conv2D(128, 3, padding=\"same\", activation=\"relu\"),\n",
        "        layers.Conv2D(128, 3, padding=\"same\", activation=\"relu\"),\n",
        "        layers.MaxPooling2D(), layers.Dropout(0.2),\n",
        "\n",
        "        layers.GlobalAveragePooling2D(),\n",
        "        layers.Dense(256, activation=\"relu\"),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(num_classes, activation=\"softmax\"),\n",
        "    ])\n",
        "\n",
        "model = build_model(num_classes)\n",
        "model.compile(optimizer=keras.optimizers.Adam(1e-3),\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ReduceLROnPlateau(monitor=\"val_accuracy\", factor=0.5, patience=3, min_lr=1e-5, verbose=1),\n",
        "    keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=6, restore_best_weights=True, verbose=1),\n",
        "]\n",
        "\n",
        "history = model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS, callbacks=callbacks, verbose=1)\n",
        "\n",
        "# -------------------------\n",
        "# Evaluation\n",
        "# -------------------------\n",
        "val_loss, val_acc = model.evaluate(val_ds, verbose=0)\n",
        "print(f\"\\nValidation accuracy: {val_acc:.4f}\")\n",
        "\n",
        "# Confusion matrix + per-class accuracy\n",
        "y_true, y_pred = [], []\n",
        "for xb, yb in val_ds:\n",
        "    logits = model.predict(xb, verbose=0)\n",
        "    y_true.extend(yb.numpy().tolist())\n",
        "    y_pred.extend(tf.argmax(logits, axis=1).numpy().tolist())\n",
        "\n",
        "cm = tf.math.confusion_matrix(y_true, y_pred, num_classes=num_classes)\n",
        "print(\"\\nConfusion matrix:\\n\", cm.numpy())\n",
        "\n",
        "diag = tf.linalg.diag_part(cm)\n",
        "per_class_acc = tf.math.divide_no_nan(tf.cast(diag, tf.float32),\n",
        "                                      tf.cast(tf.reduce_sum(cm, axis=1), tf.float32))\n",
        "print(\"\\nPer-class accuracy:\")\n",
        "for cls_id, acc in enumerate(per_class_acc.numpy().tolist()):\n",
        "    print(f\"Class {unique_labs[cls_id]:02d} -> idx {cls_id:02d}: {acc:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-FsQgj3LQkI",
        "outputId": "d92da57c-fbd2-4afa-9b37-1ae19f34e697"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6957 images across 7 classes.\n",
            "Epoch 1/20\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 1s/step - accuracy: 0.2437 - loss: 2.1704 - val_accuracy: 0.3602 - val_loss: 1.5172 - learning_rate: 0.0010\n",
            "Epoch 2/20\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 1s/step - accuracy: 0.3653 - loss: 1.4638 - val_accuracy: 0.5464 - val_loss: 0.9747 - learning_rate: 0.0010\n",
            "Epoch 3/20\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 1s/step - accuracy: 0.5821 - loss: 0.9686 - val_accuracy: 0.6520 - val_loss: 0.8375 - learning_rate: 0.0010\n",
            "Epoch 4/20\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 1s/step - accuracy: 0.7125 - loss: 0.6862 - val_accuracy: 0.8699 - val_loss: 0.3448 - learning_rate: 0.0010\n",
            "Epoch 5/20\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 1s/step - accuracy: 0.8814 - loss: 0.3063 - val_accuracy: 0.9827 - val_loss: 0.0612 - learning_rate: 0.0010\n",
            "Epoch 6/20\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 1s/step - accuracy: 0.9562 - loss: 0.1197 - val_accuracy: 0.9777 - val_loss: 0.1010 - learning_rate: 0.0010\n",
            "Epoch 7/20\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 1s/step - accuracy: 0.9641 - loss: 0.1132 - val_accuracy: 0.9978 - val_loss: 0.0117 - learning_rate: 0.0010\n",
            "Epoch 8/20\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 1s/step - accuracy: 0.9781 - loss: 0.0902 - val_accuracy: 0.9720 - val_loss: 0.1106 - learning_rate: 0.0010\n",
            "Epoch 9/20\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 1s/step - accuracy: 0.9593 - loss: 0.1461 - val_accuracy: 0.9986 - val_loss: 0.0085 - learning_rate: 0.0010\n",
            "Epoch 10/20\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 1s/step - accuracy: 0.9955 - loss: 0.0197 - val_accuracy: 0.9892 - val_loss: 0.0367 - learning_rate: 0.0010\n",
            "Epoch 11/20\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 1s/step - accuracy: 0.9907 - loss: 0.0385 - val_accuracy: 0.9763 - val_loss: 0.1079 - learning_rate: 0.0010\n",
            "Epoch 12/20\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9781 - loss: 0.1048\n",
            "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 1s/step - accuracy: 0.9781 - loss: 0.1046 - val_accuracy: 0.9950 - val_loss: 0.0234 - learning_rate: 0.0010\n",
            "Epoch 13/20\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 1s/step - accuracy: 0.9976 - loss: 0.0156 - val_accuracy: 0.9986 - val_loss: 0.0132 - learning_rate: 5.0000e-04\n",
            "Epoch 14/20\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 1s/step - accuracy: 0.9989 - loss: 0.0067 - val_accuracy: 0.9986 - val_loss: 0.0056 - learning_rate: 5.0000e-04\n",
            "Epoch 15/20\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9998 - loss: 0.0031\n",
            "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 1s/step - accuracy: 0.9998 - loss: 0.0031 - val_accuracy: 0.9986 - val_loss: 0.0038 - learning_rate: 5.0000e-04\n",
            "Epoch 15: early stopping\n",
            "Restoring model weights from the end of the best epoch: 9.\n",
            "\n",
            "Validation accuracy: 0.9986\n",
            "\n",
            "Confusion matrix:\n",
            " [[231   0   0   0   0   0   0]\n",
            " [  0 372   0   0   0   0   0]\n",
            " [  0   0  75   0   0   0   0]\n",
            " [  0   1   0 307   0   0   0]\n",
            " [  0   0   0   0 287   1   0]\n",
            " [  0   0   0   0   0  63   0]\n",
            " [  0   0   0   0   0   0  54]]\n",
            "\n",
            "Per-class accuracy:\n",
            "Class 04 -> idx 00: 1.000\n",
            "Class 05 -> idx 01: 1.000\n",
            "Class 06 -> idx 02: 1.000\n",
            "Class 08 -> idx 03: 0.997\n",
            "Class 09 -> idx 04: 0.997\n",
            "Class 40 -> idx 05: 1.000\n",
            "Class 42 -> idx 06: 1.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- Evaluate overall accuracy ----\n",
        "val_loss, val_acc = model.evaluate(val_ds, verbose=0)\n",
        "print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
        "print(f\"Validation Loss: {val_loss:.4f}\")\n",
        "\n",
        "# ---- Collect predictions and true labels ----\n",
        "y_true, y_pred = [], []\n",
        "for xb, yb in val_ds:\n",
        "    preds = model.predict(xb, verbose=0)\n",
        "    y_true.extend(yb.numpy().tolist())\n",
        "    y_pred.extend(tf.argmax(preds, axis=1).numpy().tolist())\n",
        "\n",
        "# ---- Confusion matrix (TensorFlow-only) ----\n",
        "cm = tf.math.confusion_matrix(y_true, y_pred, num_classes=num_classes)\n",
        "\n",
        "print(\"\\nConfusion Matrix (rows = true, cols = predicted):\\n\")\n",
        "print(cm.numpy())\n",
        "\n",
        "# ---- Plot confusion matrix as heatmap ----\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.imshow(cm, cmap=\"Blues\")\n",
        "plt.colorbar()\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.show()\n",
        "\n",
        "# ---- Per-class accuracy ----\n",
        "diag = tf.linalg.diag_part(cm)\n",
        "per_class_acc = diag / tf.reduce_sum(cm, axis=1)\n",
        "\n",
        "print(\"\\nPer-class accuracy:\")\n",
        "for cls, acc in enumerate(per_class_acc.numpy().tolist()):\n",
        "    print(f\"Class {cls:02d}: {acc:.3f}\")\n"
      ],
      "metadata": {
        "id": "z-HhRKmM95LK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "04652d10-2873-42a5-f8d2-0e4e04b74ff4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9986\n",
            "Validation Loss: 0.0085\n",
            "\n",
            "Confusion Matrix (rows = true, cols = predicted):\n",
            "\n",
            "[[231   0   0   0   0   0   0]\n",
            " [  0 372   0   0   0   0   0]\n",
            " [  0   0  75   0   0   0   0]\n",
            " [  0   1   0 307   0   0   0]\n",
            " [  0   0   0   0 287   1   0]\n",
            " [  0   0   0   0   0  63   0]\n",
            " [  0   0   0   0   0   0  54]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x800 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Per-class accuracy:\n",
            "Class 00: 1.000\n",
            "Class 01: 1.000\n",
            "Class 02: 1.000\n",
            "Class 03: 0.997\n",
            "Class 04: 0.997\n",
            "Class 05: 1.000\n",
            "Class 06: 1.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Keep augmentations mild: signs are orientation-sensitive\n",
        "augment = keras.Sequential([\n",
        "    layers.RandomTranslation(height_factor=0.06, width_factor=0.06, fill_mode=\"nearest\"),\n",
        "    layers.RandomZoom(height_factor=(-0.1, 0.1), width_factor=(-0.1, 0.1), fill_mode=\"nearest\"),\n",
        "    layers.RandomRotation(factor=0.05, fill_mode=\"nearest\"),  # ~±9°\n",
        "    layers.RandomContrast(0.1),\n",
        "], name=\"augment\")\n",
        "\n",
        "def aug_fn(img, label):\n",
        "    img = augment(img, training=True)\n",
        "    return img, label\n",
        "\n",
        "# apply to your train dataset (keep val_ds clean!)\n",
        "train_ds = (train_ds   # <-- use your preprocessed, UNBATCHED train dataset here\n",
        "            .map(aug_fn, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "            .cache()\n",
        "            .shuffle(10_000, seed=42, reshuffle_each_iteration=True)\n",
        "            .batch(BATCH_SIZE)\n",
        "            .prefetch(tf.data.AUTOTUNE))\n",
        "\n",
        "# val_ds should remain: val_ds = val_ds_unbatched.batch(BATCH_SIZE).prefetch(...)\n"
      ],
      "metadata": {
        "id": "zJgGFMTX_EoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Keep augmentations mild: signs are orientation-sensitive\n",
        "augment = keras.Sequential([\n",
        "    layers.RandomTranslation(height_factor=0.06, width_factor=0.06, fill_mode=\"nearest\"),\n",
        "    layers.RandomZoom(height_factor=(-0.1, 0.1), width_factor=(-0.1, 0.1), fill_mode=\"nearest\"),\n",
        "    layers.RandomRotation(factor=0.05, fill_mode=\"nearest\"),  # ~±9°\n",
        "    layers.RandomContrast(0.1),\n",
        "], name=\"augment\")\n",
        "\n",
        "def aug_fn(img, label):\n",
        "    img = augment(img, training=True)\n",
        "    return img, label\n",
        "\n",
        "# apply to your train dataset (keep val_ds clean!)\n",
        "train_ds = (train_ds   # <-- use your preprocessed, UNBATCHED train dataset here\n",
        "            .map(aug_fn, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "            .cache()\n",
        "            .shuffle(10_000, seed=42, reshuffle_each_iteration=True)\n",
        "            .batch(BATCH_SIZE)\n",
        "            .prefetch(tf.data.AUTOTUNE))\n",
        "\n",
        "# val_ds should remain: val_ds = val_ds_unbatched.batch(BATCH_SIZE).prefetch(...)\n"
      ],
      "metadata": {
        "id": "7cP0M8OWT0aW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 43  # GTSRB\n",
        "\n",
        "def one_hot(y):\n",
        "    return tf.one_hot(y, depth=num_classes)\n",
        "\n",
        "def sample_beta(alpha=0.2, shape=()):\n",
        "    # Beta(alpha, alpha) via Gamma sampling\n",
        "    gamma1 = tf.random.gamma(shape=shape, alpha=[alpha])\n",
        "    gamma2 = tf.random.gamma(shape=shape, alpha=[alpha])\n",
        "    lam = gamma1 / (gamma1 + gamma2)\n",
        "    return tf.cast(lam[0], tf.float32)\n",
        "\n",
        "def mixup_batch(images, labels, alpha=0.2):\n",
        "    lam = sample_beta(alpha, shape=(images.shape[0],))\n",
        "    lam_img = tf.reshape(lam, (-1, 1, 1, 1))\n",
        "    lam_lbl = tf.reshape(lam, (-1, 1))\n",
        "\n",
        "    # Shuffle within the batch\n",
        "    idx = tf.random.shuffle(tf.range(tf.shape(images)[0]))\n",
        "    images2 = tf.gather(images, idx)\n",
        "    labels2 = tf.gather(labels, idx)\n",
        "\n",
        "    mixed_images = images * lam_img + images2 * (1.0 - lam_img)\n",
        "    mixed_labels = one_hot(labels) * lam_lbl + one_hot(labels2) * (1.0 - lam_lbl)\n",
        "    return mixed_images, mixed_labels\n",
        "\n",
        "def mixup_map(images, labels):\n",
        "    return tf.py_function(\n",
        "        func=lambda x, y: mixup_batch(x, y, alpha=0.2),\n",
        "        inp=[images, labels],\n",
        "        Tout=[tf.float32, tf.float32]\n",
        "    )\n",
        "\n",
        "# Build an augmented + MixUp pipeline\n",
        "train_ds = (train_ds\n",
        "            .map(aug_fn, num_parallel_calls=tf.data.AUTOTUNE)      # A) geometric/photometric aug\n",
        "            .shuffle(10_000, seed=42, reshuffle_each_iteration=True)\n",
        "            .batch(BATCH_SIZE)\n",
        "            .map(mixup_map, num_parallel_calls=tf.data.AUTOTUNE)   # B) MixUp after batching\n",
        "            .prefetch(tf.data.AUTOTUNE))\n",
        "\n",
        "# If you enable MixUp, compile with categorical loss:\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(1e-3),\n",
        "    loss=\"categorical_crossentropy\",       # <- because labels are mixed one-hots\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n"
      ],
      "metadata": {
        "id": "noSDI6Yh_FhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# GTSRB: Compare Custom CNN vs MobileNetV2\n",
        "# ================================\n",
        "\n",
        "# -------------------------\n",
        "# Config\n",
        "# -------------------------\n",
        "TRAIN_DIR  = \"/content/drive/MyDrive/Elevvo Internship/Task 6/gtsrb\"  # adjust as needed\n",
        "IMG_SIZE   = 64          # 48 or 64 work well\n",
        "BATCH_SIZE = 64\n",
        "VAL_SPLIT  = 0.2\n",
        "EPOCHS_SCRATCH = 18\n",
        "EPOCHS_TL_WARM = 6       # frozen warm-up for MobileNet\n",
        "EPOCHS_TL_FT   = 10      # fine-tune epochs\n",
        "SEED       = 42\n",
        "AUTOTUNE   = tf.data.AUTOTUNE\n",
        "USE_OPENCV = False       # True = decode/resize with OpenCV; False = pure TF\n",
        "\n",
        "# -------------------------\n",
        "# List files (TF-only; no os/glob)\n",
        "# Folder layout: train/<class_id>/*.(png|ppm|jpg|jpeg)\n",
        "# -------------------------\n",
        "patterns = [\n",
        "    f\"{TRAIN_DIR}/*/*.png\", f\"{TRAIN_DIR}/*/*.ppm\",\n",
        "    f\"{TRAIN_DIR}/*/*.jpg\", f\"{TRAIN_DIR}/*/*.jpeg\"\n",
        "]\n",
        "files = tf.data.Dataset.from_tensor_slices(patterns)\n",
        "files = files.flat_map(tf.data.Dataset.list_files).shuffle(200_000, seed=SEED, reshuffle_each_iteration=False)\n",
        "\n",
        "# Label from folder name\n",
        "def path_to_label(path: tf.Tensor) -> tf.Tensor:\n",
        "    parts = tf.strings.split(path, \"/\")\n",
        "    return tf.strings.to_number(parts[-2], out_type=tf.int32)  # e.g., \"00014\" -> 14\n",
        "\n",
        "# A) Pure-TF decode/resize/normalize to [0,1]\n",
        "def decode_tf(path: tf.Tensor):\n",
        "    label = path_to_label(path)\n",
        "    img = tf.io.decode_image(tf.io.read_file(path), channels=3, expand_animations=False)  # uint8\n",
        "    img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE), antialias=True)\n",
        "    img = tf.image.convert_image_dtype(img, tf.float32)  # [0,1]\n",
        "    return img, label\n",
        "\n",
        "# B) OpenCV (optional)\n",
        "def _decode_cv2_py(path_bytes):\n",
        "    path = path_bytes.decode(\"utf-8\")\n",
        "    bgr = cv2.imread(path, cv2.IMREAD_COLOR)\n",
        "    if bgr is None:\n",
        "        return (tf.zeros((IMG_SIZE, IMG_SIZE, 3), tf.float32)).numpy()\n",
        "    rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
        "    rgb = cv2.resize(rgb, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_AREA)\n",
        "    rgb = (rgb.astype(\"float32\") / 255.0)\n",
        "    return rgb\n",
        "\n",
        "def decode_cv2(path: tf.Tensor):\n",
        "    label = path_to_label(path)\n",
        "    img = tf.py_function(_decode_cv2_py, [path], Tout=tf.float32)\n",
        "    img.set_shape((IMG_SIZE, IMG_SIZE, 3))\n",
        "    return img, label\n",
        "\n",
        "decode = decode_cv2 if USE_OPENCV else decode_tf\n",
        "\n",
        "# Materialize file list to split (cardinality can be unknown)\n",
        "file_list = list(files.as_numpy_iterator())\n",
        "N = len(file_list)\n",
        "n_val = int(N * VAL_SPLIT)\n",
        "val_files = tf.data.Dataset.from_tensor_slices(file_list[:n_val])\n",
        "trn_files = tf.data.Dataset.from_tensor_slices(file_list[n_val:])\n",
        "\n",
        "# Base, unbatched datasets with images in [0,1]\n",
        "train_base = trn_files.map(decode, num_parallel_calls=AUTOTUNE)\n",
        "val_base   = val_files.map(decode, num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "# Augment (kept mild & realistic)\n",
        "augment = keras.Sequential([\n",
        "    layers.RandomTranslation(0.06, 0.06, fill_mode=\"nearest\"),\n",
        "    layers.RandomZoom((-0.1, 0.1), (-0.1, 0.1), fill_mode=\"nearest\"),\n",
        "    layers.RandomRotation(0.06, fill_mode=\"nearest\"),\n",
        "    layers.RandomContrast(0.1),\n",
        "], name=\"augment\")\n",
        "\n",
        "def augment_fn(img, label):\n",
        "    return augment(img, training=True), label\n",
        "\n",
        "# ----- Pipeline for Custom CNN (expects [0,1]) -----\n",
        "train_custom = (train_base\n",
        "                .map(augment_fn, num_parallel_calls=AUTOTUNE)\n",
        "                .cache()\n",
        "                .shuffle(10_000, seed=SEED, reshuffle_each_iteration=True)\n",
        "                .batch(BATCH_SIZE)\n",
        "                .prefetch(AUTOTUNE))\n",
        "val_custom = (val_base.cache().batch(BATCH_SIZE).prefetch(AUTOTUNE))\n",
        "\n",
        "# ----- Pipeline for MobileNetV2 (expects preprocess_input) -----\n",
        "def to_mobilenet(img, label):\n",
        "    # our img is float32 in [0,1] → scale to [0,255] then preprocess_input (→ [-1,1])\n",
        "    img = preprocess_input(img * 255.0)\n",
        "    return img, label\n",
        "\n",
        "train_mnet = (train_base\n",
        "              .map(augment_fn, num_parallel_calls=AUTOTUNE)\n",
        "              .map(to_mobilenet, num_parallel_calls=AUTOTUNE)\n",
        "              .cache()\n",
        "              .shuffle(10_000, seed=SEED, reshuffle_each_iteration=True)\n",
        "              .batch(BATCH_SIZE)\n",
        "              .prefetch(AUTOTUNE))\n",
        "val_mnet = (val_base\n",
        "            .map(to_mobilenet, num_parallel_calls=AUTOTUNE)\n",
        "            .cache().batch(BATCH_SIZE).prefetch(AUTOTUNE))\n",
        "\n",
        "# Infer number of classes\n",
        "for _, yb in val_custom.take(1):\n",
        "    num_classes = int(tf.reduce_max(yb).numpy()) + 1\n",
        "\n",
        "# -------------------------\n",
        "# Models\n",
        "# -------------------------\n",
        "def build_custom_cnn(num_classes: int):\n",
        "    return keras.Sequential([\n",
        "        layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3)),\n",
        "        layers.Conv2D(32, 3, padding=\"same\", activation=\"relu\"),\n",
        "        layers.Conv2D(32, 3, padding=\"same\", activation=\"relu\"),\n",
        "        layers.MaxPooling2D(), layers.Dropout(0.15),\n",
        "\n",
        "        layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\"),\n",
        "        layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\"),\n",
        "        layers.MaxPooling2D(), layers.Dropout(0.15),\n",
        "\n",
        "        layers.Conv2D(128, 3, padding=\"same\", activation=\"relu\"),\n",
        "        layers.Conv2D(128, 3, padding=\"same\", activation=\"relu\"),\n",
        "        layers.MaxPooling2D(), layers.Dropout(0.20),\n",
        "\n",
        "        layers.GlobalAveragePooling2D(),\n",
        "        layers.Dense(256, activation=\"relu\"),\n",
        "        layers.Dropout(0.30),\n",
        "        layers.Dense(num_classes, activation=\"softmax\"),\n",
        "    ])\n",
        "\n",
        "def build_mobilenet_v2_head(num_classes: int):\n",
        "    base = MobileNetV2(input_shape=(IMG_SIZE, IMG_SIZE, 3), include_top=False, weights=\"imagenet\")\n",
        "    base.trainable = False  # warm-up\n",
        "    inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
        "    x = base(inputs, training=False)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    return model, base\n",
        "\n",
        "# -------------------------\n",
        "# Training helpers\n",
        "# -------------------------\n",
        "def train_and_eval(model, train_ds, val_ds, epochs, lr=1e-3, tag=\"Model\"):\n",
        "    model.compile(optimizer=keras.optimizers.Adam(lr),\n",
        "                  loss=\"sparse_categorical_crossentropy\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "    cbs = [\n",
        "        keras.callbacks.ReduceLROnPlateau(monitor=\"val_accuracy\", factor=0.5, patience=3, min_lr=1e-5, verbose=1),\n",
        "        keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=6, restore_best_weights=True, verbose=1),\n",
        "    ]\n",
        "    hist = model.fit(train_ds, validation_data=val_ds, epochs=epochs, callbacks=cbs, verbose=1)\n",
        "    val_loss, val_acc = model.evaluate(val_ds, verbose=0)\n",
        "    print(f\"\\n[{tag}] Validation accuracy: {val_acc:.4f}\")\n",
        "    return val_acc\n",
        "\n",
        "def confusion_matrix_tf(model, val_ds, num_classes: int, tag=\"Model\"):\n",
        "    y_true, y_pred = [], []\n",
        "    for xb, yb in val_ds:\n",
        "        logits = model.predict(xb, verbose=0)\n",
        "        y_true.extend(yb.numpy().tolist())\n",
        "        y_pred.extend(tf.argmax(logits, axis=1).numpy().tolist())\n",
        "    cm = tf.math.confusion_matrix(y_true, y_pred, num_classes=num_classes)\n",
        "    print(f\"\\n[{tag}] Confusion matrix (rows=true, cols=pred):\\n\", cm.numpy())\n",
        "    diag = tf.linalg.diag_part(cm)\n",
        "    per_cls = diag / tf.reduce_sum(cm, axis=1)\n",
        "    print(f\"\\n[{tag}] Per-class accuracy:\")\n",
        "    for cls, acc in enumerate(per_cls.numpy().tolist()):\n",
        "        print(f\"  class {cls:02d}: {acc:.3f}\")\n",
        "    return cm\n",
        "\n",
        "# -------------------------\n",
        "# 1) Train Custom CNN\n",
        "# -------------------------\n",
        "custom = build_custom_cnn(num_classes)\n",
        "acc_custom = train_and_eval(custom, train_custom, val_custom, epochs=EPOCHS_SCRATCH, tag=\"Custom CNN\")\n",
        "cm_custom = confusion_matrix_tf(custom, val_custom, num_classes, tag=\"Custom CNN\")\n",
        "\n",
        "# -------------------------\n",
        "# 2) Train MobileNetV2 (transfer learning)\n",
        "#    (a) warm-up with backbone frozen, (b) fine-tune upper layers\n",
        "# -------------------------\n",
        "mnet, base = build_mobilenet_v2_head(num_classes)\n",
        "acc_warm = train_and_eval(mnet, train_mnet, val_mnet, epochs=EPOCHS_TL_WARM, lr=1e-3, tag=\"MobileNetV2 (frozen)\")\n",
        "\n",
        "# Unfreeze last third of layers for fine-tuning\n",
        "for i, layer in enumerate(base.layers):\n",
        "    layer.trainable = (i >= int(len(base.layers) * 0.66))\n",
        "acc_finetuned = train_and_eval(mnet, train_mnet, val_mnet, epochs=EPOCHS_TL_FT, lr=1e-4, tag=\"MobileNetV2 (fine-tuned)\")\n",
        "cm_mnet = confusion_matrix_tf(mnet, val_mnet, num_classes, tag=\"MobileNetV2 (fine-tuned)\")\n",
        "\n",
        "# -------------------------\n",
        "# Summary\n",
        "# -------------------------\n",
        "print(\"\\n================ Summary ================\")\n",
        "print(f\"Custom CNN accuracy           : {acc_custom:.4f}\")\n",
        "print(f\"MobileNetV2 (frozen) accuracy : {acc_warm:.4f}\")\n",
        "print(f\"MobileNetV2 (fine-tuned) acc  : {acc_finetuned:.4f}\")\n",
        "print(\"=========================================\")\n"
      ],
      "metadata": {
        "id": "JQYAcrYW_nMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# GTSRB preprocessing (resize + normalize)\n",
        "# ================================\n",
        "\n",
        "# -------------------------\n",
        "# Config\n",
        "# -------------------------\n",
        "TRAIN_DIR = \"/content/drive/MyDrive/Elevvo Internship/Task 6/gtsrb\"\n",
        "TEST_DIR  = \"/content/drive/MyDrive/Elevvo Internship/Task 6/gtsrb\"\n",
        "IMG_SIZE  = 48         # 32/48/64 are common for GTSRB; 48 is a good balance\n",
        "BATCH     = 64\n",
        "VAL_SPLIT = 0.2\n",
        "SEED      = 42\n",
        "AUTOTUNE  = tf.data.AUTOTUNE\n",
        "\n",
        "# -------------------------\n",
        "# File listing (TF-only; no os/glob)\n",
        "# Train assumed as Train/<class_id>/*.ext\n",
        "# -------------------------\n",
        "train_patterns = [\n",
        "    f\"{TRAIN_DIR}/*/*.png\", f\"{TRAIN_DIR}/*/*.ppm\",\n",
        "    f\"{TRAIN_DIR}/*/*.jpg\", f\"{TRAIN_DIR}/*/*.jpeg\"\n",
        "]\n",
        "train_files = (tf.data.Dataset.from_tensor_slices(train_patterns)\n",
        "               .flat_map(tf.data.Dataset.list_files)\n",
        "               .shuffle(200_000, seed=SEED, reshuffle_each_iteration=False))\n",
        "\n",
        "# Extract integer label from the parent folder name\n",
        "def path_to_label(path: tf.Tensor) -> tf.Tensor:\n",
        "    parts = tf.strings.split(path, \"/\")\n",
        "    return tf.strings.to_number(parts[-2], out_type=tf.int32)  # e.g., \".../00014/img.ppm\" -> 14\n",
        "\n",
        "# -------------------------\n",
        "# OpenCV-based decode → resize → normalize to [0,1]\n",
        "# (kept to your requirement to use cv2)\n",
        "# -------------------------\n",
        "def _read_cv2(path_bytes):\n",
        "    path = path_bytes.decode(\"utf-8\")\n",
        "    bgr = cv2.imread(path, cv2.IMREAD_COLOR)       # BGR uint8\n",
        "    if bgr is None:\n",
        "        # fallback to black image if read fails\n",
        "        return (tf.zeros((IMG_SIZE, IMG_SIZE, 3), tf.float32)).numpy()\n",
        "    rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
        "    rgb = cv2.resize(rgb, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_AREA)\n",
        "    rgb = rgb.astype(\"float32\") / 255.0            # normalize to [0,1]\n",
        "    return rgb\n",
        "\n",
        "def preprocess_with_cv2(path: tf.Tensor):\n",
        "    label = path_to_label(path)\n",
        "    img = tf.py_function(_read_cv2, [path], Tout=tf.float32)\n",
        "    img.set_shape((IMG_SIZE, IMG_SIZE, 3))\n",
        "    return img, label\n",
        "\n",
        "# -------------------------\n",
        "# Split into train/val without pandas/sklearn\n",
        "# -------------------------\n",
        "# Materialize list to determine size (cardinality can be unknown)\n",
        "file_list = list(train_files.as_numpy_iterator())\n",
        "N = len(file_list)\n",
        "n_val = int(N * VAL_SPLIT)\n",
        "\n",
        "val_files   = tf.data.Dataset.from_tensor_slices(file_list[:n_val])\n",
        "train_files = tf.data.Dataset.from_tensor_slices(file_list[n_val:])\n",
        "\n",
        "train_ds_unbatched = val_files.map(preprocess_with_cv2, num_parallel_calls=AUTOTUNE)\n",
        "val_ds_unbatched   = train_files.map(preprocess_with_cv2, num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "# (swap names to keep the conventional split: last part as validation)\n",
        "train_ds_unbatched, val_ds_unbatched = val_ds_unbatched, train_ds_unbatched\n",
        "\n",
        "# Optional mild augmentation (good for generalization; keep realistic)\n",
        "augment = keras.Sequential([\n",
        "    layers.RandomTranslation(0.06, 0.06, fill_mode=\"nearest\"),\n",
        "    layers.RandomZoom((-0.1, 0.1), (-0.1, 0.1), fill_mode=\"nearest\"),\n",
        "    layers.RandomRotation(0.06, fill_mode=\"nearest\"),\n",
        "    layers.RandomContrast(0.1),\n",
        "], name=\"augment\")\n",
        "\n",
        "def aug_fn(img, label):\n",
        "    return augment(img, training=True), label\n",
        "\n",
        "# Final datasets\n",
        "train_ds = (train_ds_unbatched\n",
        "            .map(aug_fn, num_parallel_calls=AUTOTUNE)\n",
        "            .cache()\n",
        "            .shuffle(10_000, seed=SEED, reshuffle_each_iteration=True)\n",
        "            .batch(BATCH)\n",
        "            .prefetch(AUTOTUNE))\n",
        "\n",
        "val_ds = (val_ds_unbatched\n",
        "          .cache()\n",
        "          .batch(BATCH)\n",
        "          .prefetch(AUTOTUNE))\n",
        "\n",
        "# -------------------------\n",
        "# (Optional) Build a tiny CNN to verify the pipeline\n",
        "# -------------------------\n",
        "for _, yb in val_ds.take(1):\n",
        "    num_classes = int(tf.reduce_max(yb).numpy()) + 1  # infer from labels\n",
        "\n",
        "model = keras.Sequential([\n",
        "    layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3)),\n",
        "    layers.Conv2D(32, 3, activation=\"relu\"), layers.MaxPooling2D(),\n",
        "    layers.Conv2D(64, 3, activation=\"relu\"), layers.MaxPooling2D(),\n",
        "    layers.Conv2D(128, 3, activation=\"relu\"), layers.MaxPooling2D(),\n",
        "    layers.Dropout(0.25),\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dense(256, activation=\"relu\"),\n",
        "    layers.Dropout(0.25),\n",
        "    layers.Dense(num_classes, activation=\"softmax\"),\n",
        "])\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "history = model.fit(train_ds, validation_data=val_ds, epochs=15)\n",
        "\n",
        "val_loss, val_acc = model.evaluate(val_ds, verbose=0)\n",
        "print(f\"Validation accuracy: {val_acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "UPWQ8sMu3Fx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GTSRBPreprocessor:\n",
        "    def __init__(self, dataset_path, target_size=(32, 32)):\n",
        "        \"\"\"\n",
        "        Initialize GTSRB preprocessor\n",
        "\n",
        "        Args:\n",
        "            dataset_path (str): Path to the GTSRB dataset folder\n",
        "            target_size (tuple): Target image size (height, width)\n",
        "        \"\"\"\n",
        "        self.dataset_path = dataset_path\n",
        "        self.target_size = target_size\n",
        "\n",
        "    def load_csv_file(self, filename):\n",
        "        \"\"\"\n",
        "        Load CSV file manually using TensorFlow file operations\n",
        "\n",
        "        Args:\n",
        "            filename (str): Name of the CSV file\n",
        "\n",
        "        Returns:\n",
        "            tuple: (data, headers)\n",
        "        \"\"\"\n",
        "        filepath = self.dataset_path + '/' + filename\n",
        "\n",
        "        try:\n",
        "            # Read file using TensorFlow\n",
        "            file_content = tf.io.read_file(filepath)\n",
        "            lines = tf.strings.split(file_content, '\\n')\n",
        "\n",
        "            # Convert to python list for processing\n",
        "            lines_list = [line.numpy().decode('utf-8') for line in lines if line.numpy().decode('utf-8').strip()]\n",
        "\n",
        "            if not lines_list:\n",
        "                print(f\"Error: Empty file {filename}\")\n",
        "                return None, None\n",
        "\n",
        "            # Get headers from first line\n",
        "            headers = [h.strip() for h in lines_list[0].split(',')]\n",
        "\n",
        "            # Process data lines\n",
        "            data = []\n",
        "            for line in lines_list[1:]:\n",
        "                if line.strip():  # Skip empty lines\n",
        "                    values = [v.strip() for v in line.split(',')]\n",
        "                    row_dict = {}\n",
        "                    for i, header in enumerate(headers):\n",
        "                        if i < len(values):\n",
        "                            # Try to convert to int if it looks like a number\n",
        "                            value = values[i]\n",
        "                            if value.isdigit() or (value.startswith('-') and value[1:].isdigit()):\n",
        "                                row_dict[header] = int(value)\n",
        "                            else:\n",
        "                                row_dict[header] = value\n",
        "                    data.append(row_dict)\n",
        "\n",
        "            print(f\"Loaded {filename}: {len(data)} samples\")\n",
        "            return data, headers\n",
        "\n",
        "        except tf.errors.NotFoundError:\n",
        "            print(f\"Error: Could not find file {filename}\")\n",
        "            print(\"Please convert Excel files to CSV format\")\n",
        "            return None, None\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {filename}: {e}\")\n",
        "            return None, None\n",
        "\n",
        "    def preprocess_image(self, image_path, normalize=True):\n",
        "        \"\"\"\n",
        "        Preprocess a single image using OpenCV and TensorFlow\n",
        "\n",
        "        Args:\n",
        "            image_path (str): Path to the image\n",
        "            normalize (bool): Whether to normalize pixel values\n",
        "\n",
        "        Returns:\n",
        "            tensorflow tensor: Preprocessed image\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Read image using OpenCV\n",
        "            image = cv2.imread(image_path)\n",
        "            if image is None:\n",
        "                print(f\"Warning: Could not load image {image_path}\")\n",
        "                return None\n",
        "\n",
        "            # Convert BGR to RGB (OpenCV loads as BGR by default)\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            # Resize image using OpenCV\n",
        "            image = cv2.resize(image, self.target_size, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "            # Convert to TensorFlow tensor\n",
        "            image_tensor = tf.constant(image)\n",
        "\n",
        "            # Normalize pixel values to [0, 1] range\n",
        "            if normalize:\n",
        "                image_tensor = tf.cast(image_tensor, tf.float32) / 255.0\n",
        "\n",
        "            return image_tensor\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing image {image_path}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def load_and_preprocess_images(self, image_paths, labels):\n",
        "        \"\"\"\n",
        "        Load and preprocess all images using TensorFlow operations\n",
        "\n",
        "        Args:\n",
        "            image_paths (list): List of image paths\n",
        "            labels (list): List of corresponding labels\n",
        "\n",
        "        Returns:\n",
        "            tuple: (images, labels) as TensorFlow tensors\n",
        "        \"\"\"\n",
        "        processed_images = []\n",
        "        valid_labels = []\n",
        "        failed_count = 0\n",
        "\n",
        "        print(f\"Processing {len(image_paths)} images...\")\n",
        "\n",
        "        for i in range(len(image_paths)):\n",
        "            image_path = image_paths[i]\n",
        "            label = labels[i]\n",
        "\n",
        "            # Preprocess image\n",
        "            processed_image = self.preprocess_image(image_path)\n",
        "\n",
        "            if processed_image is not None:\n",
        "                processed_images.append(processed_image)\n",
        "                valid_labels.append(label)\n",
        "            else:\n",
        "                failed_count += 1\n",
        "\n",
        "            # Progress indicator\n",
        "            if (i + 1) % 1000 == 0:\n",
        "                print(f\"Processed {i + 1}/{len(image_paths)} images...\")\n",
        "\n",
        "        if failed_count > 0:\n",
        "            print(f\"Warning: Failed to process {failed_count} images\")\n",
        "\n",
        "        # Stack into tensors\n",
        "        if processed_images:\n",
        "            images_tensor = tf.stack(processed_images)\n",
        "            labels_tensor = tf.constant(valid_labels, dtype=tf.int32)\n",
        "            return images_tensor, labels_tensor\n",
        "        else:\n",
        "            return None, None\n",
        "\n",
        "    def create_validation_split(self, images, labels, validation_split=0.2, seed=42):\n",
        "        \"\"\"\n",
        "        Create train-validation split using TensorFlow operations\n",
        "\n",
        "        Args:\n",
        "            images (tf.Tensor): Tensor of images\n",
        "            labels (tf.Tensor): Tensor of labels\n",
        "            validation_split (float): Fraction of data to use for validation\n",
        "            seed (int): Random seed\n",
        "\n",
        "        Returns:\n",
        "            tuple: (train_images, val_images, train_labels, val_labels)\n",
        "        \"\"\"\n",
        "        # Set seed for reproducibility\n",
        "        tf.random.set_seed(seed)\n",
        "\n",
        "        # Get dataset size\n",
        "        dataset_size = tf.shape(images)[0]\n",
        "\n",
        "        # Create random indices\n",
        "        indices = tf.range(dataset_size)\n",
        "        indices = tf.random.shuffle(indices, seed=seed)\n",
        "\n",
        "        # Calculate split point\n",
        "        val_size = tf.cast(tf.cast(dataset_size, tf.float32) * validation_split, tf.int32)\n",
        "        train_size = dataset_size - val_size\n",
        "\n",
        "        # Split indices\n",
        "        train_indices = indices[:train_size]\n",
        "        val_indices = indices[train_size:]\n",
        "\n",
        "        # Gather data based on indices\n",
        "        train_images = tf.gather(images, train_indices)\n",
        "        val_images = tf.gather(images, val_indices)\n",
        "        train_labels = tf.gather(labels, train_indices)\n",
        "        val_labels = tf.gather(labels, val_indices)\n",
        "\n",
        "        return train_images, val_images, train_labels, val_labels\n",
        "\n",
        "    def create_tensorflow_dataset(self, images, labels, batch_size=32, shuffle=True, seed=42):\n",
        "        \"\"\"\n",
        "        Create TensorFlow dataset from images and labels tensors\n",
        "\n",
        "        Args:\n",
        "            images (tf.Tensor): Tensor of preprocessed images\n",
        "            labels (tf.Tensor): Tensor of labels\n",
        "            batch_size (int): Batch size for the dataset\n",
        "            shuffle (bool): Whether to shuffle the dataset\n",
        "            seed (int): Random seed\n",
        "\n",
        "        Returns:\n",
        "            tf.data.Dataset: TensorFlow dataset\n",
        "        \"\"\"\n",
        "        dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
        "\n",
        "        if shuffle:\n",
        "            buffer_size = tf.shape(images)[0]\n",
        "            dataset = dataset.shuffle(buffer_size=buffer_size, seed=seed)\n",
        "\n",
        "        dataset = dataset.batch(batch_size)\n",
        "        dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "        return dataset\n",
        "\n",
        "    def apply_data_augmentation(self):\n",
        "        \"\"\"\n",
        "        Create data augmentation pipeline using Keras layers\n",
        "\n",
        "        Returns:\n",
        "            keras.Sequential: Data augmentation model\n",
        "        \"\"\"\n",
        "        data_augmentation = keras.Sequential([\n",
        "            keras.layers.RandomFlip(\"horizontal\"),\n",
        "            keras.layers.RandomRotation(0.1),\n",
        "            keras.layers.RandomZoom(0.1),\n",
        "            keras.layers.RandomContrast(0.1),\n",
        "            keras.layers.RandomBrightness(0.1)\n",
        "        ])\n",
        "\n",
        "        return data_augmentation\n",
        "\n",
        "    def get_unique_classes(self, labels_tensor):\n",
        "        \"\"\"\n",
        "        Get unique classes from labels tensor using TensorFlow operations\n",
        "\n",
        "        Args:\n",
        "            labels_tensor (tf.Tensor): Tensor of labels\n",
        "\n",
        "        Returns:\n",
        "            int: Number of unique classes\n",
        "        \"\"\"\n",
        "        unique_labels = tf.unique(labels_tensor)[0]\n",
        "        return tf.shape(unique_labels)[0]\n",
        "\n",
        "    def preprocess_dataset_from_csv(self, train_csv='Train.csv', test_csv='Test.csv',\n",
        "                                  validation_split=0.2, seed=42):\n",
        "        \"\"\"\n",
        "        Preprocess dataset from CSV files\n",
        "\n",
        "        Args:\n",
        "            train_csv (str): Training CSV filename\n",
        "            test_csv (str): Test CSV filename\n",
        "            validation_split (float): Fraction of training data to use for validation\n",
        "            seed (int): Random seed for reproducibility\n",
        "\n",
        "        Returns:\n",
        "            dict: Dictionary containing preprocessed datasets and metadata\n",
        "        \"\"\"\n",
        "        # Set random seed for reproducibility\n",
        "        tf.random.set_seed(seed)\n",
        "\n",
        "        # Load CSV files\n",
        "        train_data, train_headers = self.load_csv_file(train_csv)\n",
        "        test_data, test_headers = self.load_csv_file(test_csv)\n",
        "\n",
        "        if train_data is None or test_data is None:\n",
        "            return None\n",
        "\n",
        "        # Extract image paths and labels from CSV data\n",
        "        # Assuming columns are 'Path' and 'ClassId' - adjust if different\n",
        "        train_paths = []\n",
        "        train_labels = []\n",
        "        for row in train_data:\n",
        "            path = row.get('Path', '')\n",
        "            if not path.startswith('/'):\n",
        "                path = self.dataset_path + '/' + path\n",
        "            train_paths.append(path)\n",
        "            train_labels.append(row.get('ClassId', 0))\n",
        "\n",
        "        test_paths = []\n",
        "        test_labels = []\n",
        "        for row in test_data:\n",
        "            path = row.get('Path', '')\n",
        "            if not path.startswith('/'):\n",
        "                path = self.dataset_path + '/' + path\n",
        "            test_paths.append(path)\n",
        "            test_labels.append(row.get('ClassId', 0))\n",
        "\n",
        "        return self._preprocess_from_paths(train_paths, train_labels, test_paths, test_labels, validation_split, seed)\n",
        "\n",
        "    def _preprocess_from_paths(self, train_paths, train_labels, test_paths, test_labels, validation_split, seed):\n",
        "        \"\"\"\n",
        "        Internal method to preprocess from image paths and labels\n",
        "        \"\"\"\n",
        "        # Load and preprocess training images\n",
        "        print(\"\\nPreprocessing training images...\")\n",
        "        train_images, train_labels_tensor = self.load_and_preprocess_images(train_paths, train_labels)\n",
        "\n",
        "        if train_images is None:\n",
        "            print(\"Error: Failed to load training images\")\n",
        "            return None\n",
        "\n",
        "        # Load and preprocess test images\n",
        "        print(\"\\nPreprocessing test images...\")\n",
        "        test_images, test_labels_tensor = self.load_and_preprocess_images(test_paths, test_labels)\n",
        "\n",
        "        if test_images is None:\n",
        "            print(\"Error: Failed to load test images\")\n",
        "            return None\n",
        "\n",
        "        # Create validation split if requested\n",
        "        if validation_split > 0:\n",
        "            print(f\"\\nCreating validation split ({validation_split*100}%)...\")\n",
        "            train_imgs, val_imgs, train_lbls, val_lbls = self.create_validation_split(\n",
        "                train_images, train_labels_tensor, validation_split, seed\n",
        "            )\n",
        "        else:\n",
        "            train_imgs, val_imgs = train_images, None\n",
        "            train_lbls, val_lbls = train_labels_tensor, None\n",
        "\n",
        "        # Calculate statistics using TensorFlow operations\n",
        "        num_classes = self.get_unique_classes(train_labels_tensor).numpy()\n",
        "        image_shape = train_imgs.shape[1:].as_list()\n",
        "\n",
        "        print(f\"\\nDataset Summary:\")\n",
        "        print(f\"Training samples: {train_imgs.shape[0]}\")\n",
        "        if val_imgs is not None:\n",
        "            print(f\"Validation samples: {val_imgs.shape[0]}\")\n",
        "        print(f\"Test samples: {test_images.shape[0]}\")\n",
        "        print(f\"Number of classes: {num_classes}\")\n",
        "        print(f\"Image shape: {image_shape}\")\n",
        "\n",
        "        # Create results dictionary\n",
        "        results = {\n",
        "            'train_images': train_imgs,\n",
        "            'train_labels': train_lbls,\n",
        "            'test_images': test_images,\n",
        "            'test_labels': test_labels_tensor,\n",
        "            'num_classes': num_classes,\n",
        "            'image_shape': image_shape\n",
        "        }\n",
        "\n",
        "        if val_imgs is not None:\n",
        "            results['val_images'] = val_imgs\n",
        "            results['val_labels'] = val_lbls\n",
        "\n",
        "        return results\n",
        "\n",
        "# Example usage\n",
        "def main():\n",
        "    # Initialize preprocessor\n",
        "    dataset_path = \"/content/drive/MyDrive/Elevvo Internship/Task 6/gtsrb\"  # Update this path\n",
        "    preprocessor = GTSRBPreprocessor(dataset_path, target_size=(32, 32))\n",
        "\n",
        "    # Preprocess dataset from CSV files (converted from Excel)\n",
        "    results = preprocessor.preprocess_dataset_from_csv('Train.csv', 'Test.csv', validation_split=0.2)\n",
        "\n",
        "    if results:\n",
        "        print(\"\\nPreprocessing completed successfully!\")\n",
        "\n",
        "        # Create TensorFlow datasets\n",
        "        train_dataset = preprocessor.create_tensorflow_dataset(\n",
        "            results['train_images'],\n",
        "            results['train_labels'],\n",
        "            batch_size=32,\n",
        "            shuffle=True\n",
        "        )\n",
        "\n",
        "        if 'val_images' in results:\n",
        "            val_dataset = preprocessor.create_tensorflow_dataset(\n",
        "                results['val_images'],\n",
        "                results['val_labels'],\n",
        "                batch_size=32,\n",
        "                shuffle=False\n",
        "            )\n",
        "            print(f\"Validation dataset: {val_dataset}\")\n",
        "\n",
        "        test_dataset = preprocessor.create_tensorflow_dataset(\n",
        "            results['test_images'],\n",
        "            results['test_labels'],\n",
        "            batch_size=32,\n",
        "            shuffle=False\n",
        "        )\n",
        "\n",
        "        print(f\"Train dataset: {train_dataset}\")\n",
        "        print(f\"Test dataset: {test_dataset}\")\n",
        "\n",
        "        # Create data augmentation pipeline\n",
        "        augmentation = preprocessor.apply_data_augmentation()\n",
        "        print(\"Data augmentation pipeline created\")\n",
        "\n",
        "        # Example of how to use the preprocessed data with Keras\n",
        "        print(f\"\\nModel specifications:\")\n",
        "        print(f\"Input shape: {results['image_shape']}\")\n",
        "        print(f\"Number of classes: {results['num_classes']}\")\n",
        "\n",
        "        # Example simple model\n",
        "        model = keras.Sequential([\n",
        "            keras.layers.Input(shape=results['image_shape']),\n",
        "            augmentation,  # Apply data augmentation\n",
        "            keras.layers.Conv2D(32, 3, activation='relu'),\n",
        "            keras.layers.MaxPooling2D(),\n",
        "            keras.layers.Conv2D(64, 3, activation='relu'),\n",
        "            keras.layers.MaxPooling2D(),\n",
        "            keras.layers.Flatten(),\n",
        "            keras.layers.Dense(64, activation='relu'),\n",
        "            keras.layers.Dense(results['num_classes'], activation='softmax')\n",
        "        ])\n",
        "\n",
        "        print(f\"\\nExample model created:\")\n",
        "        model.summary()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "091ntFxG5xLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# GTSRB (CSV-driven) preprocessing using only\n",
        "# - Reads Train.csv/Test.csv\n",
        "# - Decodes images via OpenCV\n",
        "# - Resizes to IMG_SIZE and normalizes to [0,1]\n",
        "# - Builds train/val tf.data pipelines\n",
        "# ================================\n",
        "\n",
        "# -------------------------\n",
        "# Config\n",
        "# -------------------------\n",
        "\n",
        "ROOT       = \"/content/drive/MyDrive/Elevvo Internship/Task 6/gtsrb\"\n",
        "TRAIN_CSV  = f\"{ROOT}/Train.csv\"\n",
        "TEST_CSV   = f\"{ROOT}/Test.csv\"    # may or may not have labels; we handle both\n",
        "IMG_SIZE   = 48                    # 32/48/64 are common\n",
        "BATCH      = 64\n",
        "VAL_SPLIT  = 0.2\n",
        "SEED       = 42\n",
        "AUTOTUNE   = tf.data.AUTOTUNE\n",
        "\n",
        "# -------------------------\n",
        "# Read CSV header to find columns using TF only\n",
        "# -------------------------\n",
        "with tf.io.gfile.GFile(TRAIN_CSV, \"r\") as f:\n",
        "    header_line = f.readline().strip()\n",
        "\n",
        "# Handle common separators (',' or ';')\n",
        "delim = \",\" if header_line.count(\",\") >= header_line.count(\";\") else \";\"\n",
        "cols  = header_line.split(delim)\n",
        "\n",
        "def find_col(name_options):\n",
        "    for name in name_options:\n",
        "        if name in cols:\n",
        "            return cols.index(name)\n",
        "    raise ValueError(f\"None of {name_options} found in CSV header: {cols}\")\n",
        "\n",
        "IDX_PATH  = find_col([\"Path\", \"Filename\"])  # relative path to image\n",
        "IDX_LABEL = find_col([\"ClassId\"])           # integer class id\n",
        "\n",
        "# Build per-column defaults for tf.io.decode_csv\n",
        "# We'll parse everything as string and cast only what we need\n",
        "record_defaults = [tf.constant(\"\", dtype=tf.string) for _ in cols]\n",
        "\n",
        "# -------------------------\n",
        "# CSV → (path, label) tensors\n",
        "# -------------------------\n",
        "def parse_csv_line(line):\n",
        "    # Skip empty lines\n",
        "    line = tf.strings.strip(line)\n",
        "    fields = tf.io.decode_csv(line, record_defaults=record_defaults, field_delim=delim)\n",
        "    rel_path = fields[IDX_PATH]                 # e.g., \"Train/00014/00014_00001.ppm\"\n",
        "    path = tf.where(\n",
        "        tf.strings.regex_full_match(rel_path, r\"^/.*\"),\n",
        "        rel_path,\n",
        "        tf.strings.join([ROOT, \"/\", rel_path])  # join ROOT if path is relative\n",
        "    )\n",
        "    # Some CSVs store only filenames; if so, you may need to prefix a subfolder here.\n",
        "    label = tf.strings.to_number(fields[IDX_LABEL], out_type=tf.int32)\n",
        "    return path, label\n",
        "\n",
        "def load_csv_dataset(csv_path, has_header=True):\n",
        "    ds = tf.data.TextLineDataset(csv_path)\n",
        "    if has_header:\n",
        "        ds = ds.skip(1)\n",
        "    # Filter out potential empty lines\n",
        "    ds = ds.filter(lambda l: tf.strings.length(tf.strings.strip(l)) > 0)\n",
        "    return ds.map(lambda l: parse_csv_line(l), num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "train_pairs = load_csv_dataset(TRAIN_CSV, has_header=True)\n",
        "\n",
        "# -------------------------\n",
        "# OpenCV decode → resize → normalize [0,1]\n",
        "# -------------------------\n",
        "def _cv2_read_resize_norm(path_bytes):\n",
        "    path = path_bytes.decode(\"utf-8\")\n",
        "    img  = cv2.imread(path, cv2.IMREAD_COLOR)   # BGR uint8\n",
        "    if img is None:\n",
        "        # Fallback: black image to avoid crashing the pipeline\n",
        "        return (tf.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=tf.float32)).numpy()\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    img = cv2.resize(img, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_AREA)\n",
        "    img = img.astype(\"float32\") / 255.0         # [0,1]\n",
        "    return img\n",
        "\n",
        "def preprocess_with_cv2(path, label):\n",
        "    img = tf.py_function(_cv2_read_resize_norm, [path], Tout=tf.float32)\n",
        "    img.set_shape((IMG_SIZE, IMG_SIZE, 3))\n",
        "    return img, label\n",
        "\n",
        "# Convert (path,label) → (image,label)\n",
        "train_unbatched = train_pairs.map(preprocess_with_cv2, num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "# -------------------------\n",
        "# Train/Val split (TF-only)\n",
        "# -------------------------\n",
        "# Materialize paths once to split deterministically\n",
        "paths_cache = list(train_pairs.map(lambda p, _: p).as_numpy_iterator())\n",
        "N = len(paths_cache)\n",
        "if N == 0:\n",
        "    raise FileNotFoundError(\n",
        "        \"No training samples found. Check ROOT and the 'Path'/'Filename' column values.\\n\"\n",
        "        f\"Example ROOT: {ROOT}\\n\"\n",
        "        f\"Example first 5 paths:\\n{paths_cache[:5]}\"\n",
        "    )\n",
        "n_val = int(N * VAL_SPLIT)\n",
        "\n",
        "# Rebuild datasets in the same order, then split\n",
        "train_pairs = tf.data.Dataset.from_tensor_slices(paths_cache).map(\n",
        "    lambda p: (p, 0)  # dummy label to keep signature; we'll re-lookup real labels\n",
        ")\n",
        "# Re-join labels by re-parsing lines in order for correctness\n",
        "# (Alternatively, just shuffle + split directly on train_unbatched if order doesn't matter.)\n",
        "\n",
        "# Simpler: shuffle then split directly on images (recommended)\n",
        "ds_all = train_unbatched.shuffle(N, seed=SEED, reshuffle_each_iteration=False)\n",
        "val_ds_unbatched = ds_all.take(n_val)\n",
        "train_ds_unbatched = ds_all.skip(n_val)\n",
        "\n",
        "# Optional mild augmentation\n",
        "augment = keras.Sequential([\n",
        "    layers.RandomTranslation(0.06, 0.06, fill_mode=\"nearest\"),\n",
        "    layers.RandomZoom((-0.1, 0.1), (-0.1, 0.1), fill_mode=\"nearest\"),\n",
        "    layers.RandomRotation(0.06, fill_mode=\"nearest\"),\n",
        "    layers.RandomContrast(0.1),\n",
        "], name=\"augment\")\n",
        "\n",
        "def aug_fn(img, label):\n",
        "    return augment(img, training=True), label\n",
        "\n",
        "# Final batched datasets\n",
        "train_ds = (train_ds_unbatched\n",
        "            .map(aug_fn, num_parallel_calls=AUTOTUNE)\n",
        "            .cache()\n",
        "            .shuffle(10_000, seed=SEED, reshuffle_each_iteration=True)\n",
        "            .batch(BATCH)\n",
        "            .prefetch(AUTOTUNE))\n",
        "\n",
        "val_ds = (val_ds_unbatched\n",
        "          .cache()\n",
        "          .batch(BATCH)\n",
        "          .prefetch(AUTOTUNE))\n",
        "\n",
        "# -------------------------\n",
        "# (Optional) Build a tiny CNN to sanity-check the pipeline\n",
        "# -------------------------\n",
        "for _, yb in val_ds.take(1):\n",
        "    num_classes = int(tf.reduce_max(yb).numpy()) + 1  # infer classes from val batch\n",
        "\n",
        "model = keras.Sequential([\n",
        "    layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3)),\n",
        "    layers.Conv2D(32, 3, activation=\"relu\"), layers.MaxPooling2D(),\n",
        "    layers.Conv2D(64, 3, activation=\"relu\"), layers.MaxPooling2D(),\n",
        "    layers.Conv2D(128, 3, activation=\"relu\"), layers.MaxPooling2D(),\n",
        "    layers.Dropout(0.25),\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dense(256, activation=\"relu\"),\n",
        "    layers.Dropout(0.25),\n",
        "    layers.Dense(num_classes, activation=\"softmax\"),\n",
        "])\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "history = model.fit(train_ds, validation_data=val_ds, epochs=10, verbose=1)\n",
        "\n",
        "val_loss, val_acc = model.evaluate(val_ds, verbose=0)\n",
        "print(f\"Validation accuracy: {val_acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "Epn4HrfJ7pJs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}