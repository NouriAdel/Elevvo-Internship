{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Level 2**\n",
        "**Task 5: Movie Recommendation System Description**"
      ],
      "metadata": {
        "id": "FI4nTIHS5y8n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Description:\n",
        "\n",
        "\n",
        "*   Dataset (Recommended): MovieLens 100K Dataset (Kaggle).\n",
        "*   Build a system that recommends movies based on user similarity.\n",
        "*   Use a user-item matrix to compute similarity scores.\n",
        "*   Recommend top-rated unseen movies for a given user.\n",
        "*   Evaluate performance using precision at K.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2gYwkRNb6AGn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tools & Libraries:\n",
        "\n",
        "\n",
        "*   Python\n",
        "*   Pandas\n",
        "*   Numpy\n",
        "*   Scikit-Learn\n",
        "\n",
        "\n",
        "Covered Topics:\n",
        "\n",
        "\n",
        "*   Recommendation systems\n",
        "*   Similarity-based modeling\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ba1UMKVz6gZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bonus:\n",
        "\n",
        "\n",
        "*   Implement item-based collaborative filtering.\n",
        "*   Try matrix factorization (SVD).\n",
        "\n"
      ],
      "metadata": {
        "id": "FWFOF7oK6-et"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "piUGpDH44AD0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import TruncatedSVD"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =====  user–item matrix and similarity  =====\n",
        "\n",
        "# 1) Load ratings (MovieLens 100K: u.data has user_id, item_id, rating, timestamp)\n",
        "ratings = pd.read_csv(\n",
        "    \"u.data\", sep=\"\\t\", header=None,\n",
        "    names=[\"user_id\", \"item_id\", \"rating\", \"timestamp\"]\n",
        ").drop(columns=[\"timestamp\"])\n",
        "\n",
        "# 2) Build the user–item matrix (rows = users, cols = items)\n",
        "UI = ratings.pivot_table(index=\"user_id\", columns=\"item_id\", values=\"rating\")\n",
        "\n",
        "# ----- Mean-center by user to reduce user bias -----\n",
        "UI_centered = UI.sub(UI.mean(axis=1), axis=0)\n",
        "\n",
        "# 3) Replace NaNs with zeros (cosine similarity needs numbers)\n",
        "UI_filled = UI.fillna(0.0).astype(np.float32)\n",
        "UIc_filled = UI_centered.fillna(0.0).astype(np.float32)\n",
        "\n",
        "# 4) USER–USER cosine similarity\n",
        "user_sim = pd.DataFrame(\n",
        "    cosine_similarity(UIc_filled.values),\n",
        "    index=UI.index, columns=UI.index\n",
        ")\n",
        "\n",
        "# 5) ITEM–ITEM cosine similarity\n",
        "item_sim = pd.DataFrame(\n",
        "    cosine_similarity(UI_filled.T.values),\n",
        "    index=UI.columns, columns=UI.columns\n",
        ")\n",
        "\n",
        "# ---------- Helpers to inspect/top-N ----------\n",
        "def top_similar_users(user_id, k=10, sim_matrix=user_sim):\n",
        "    sims = sim_matrix.loc[user_id].drop(user_id)  # exclude self\n",
        "    return sims.sort_values(ascending=False).head(k)\n",
        "\n",
        "def top_similar_items(item_id, k=10, sim_matrix=item_sim):\n",
        "    sims = sim_matrix.loc[item_id].drop(item_id)\n",
        "    return sims.sort_values(ascending=False).head(k)\n",
        "\n",
        "# Examples:\n",
        "print(\"\\nTop similar users to user 10:\")\n",
        "print(top_similar_users(10, k=5))\n",
        "\n",
        "print(\"\\nTop similar items to item 50:\")\n",
        "print(top_similar_items(50, k=5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPVEhMHlYnSZ",
        "outputId": "9a03ade4-94e6-42e3-eae8-c709ab9edd97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top similar users to user 10:\n",
            "user_id\n",
            "321    0.209248\n",
            "313    0.206029\n",
            "710    0.201272\n",
            "293    0.190827\n",
            "322    0.178730\n",
            "Name: 10, dtype: float32\n",
            "\n",
            "Top similar items to item 50:\n",
            "item_id\n",
            "181    0.884476\n",
            "174    0.764885\n",
            "172    0.749819\n",
            "1      0.734572\n",
            "127    0.697332\n",
            "Name: 50, dtype: float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== User-based CF Recommender =====\n",
        "\n",
        "# 1) Load ratings (u.data: user_id, item_id, rating, timestamp)\n",
        "ratings = pd.read_csv(\n",
        "    \"u.data\", sep=\"\\t\", header=None,\n",
        "    names=[\"user_id\", \"movie_id\", \"rating\", \"timestamp\"]\n",
        ")[[\"user_id\", \"movie_id\", \"rating\"]]\n",
        "\n",
        "# Load movie titles (u.item)\n",
        "# u.item fields: movie_id|title|release_date|... (pipe-separated)\n",
        "movies = pd.read_csv(\n",
        "    \"u.item\", sep=\"|\", header=None, encoding=\"latin-1\",\n",
        "    usecols=[0, 1], names=[\"movie_id\", \"title\"]\n",
        ")\n",
        "\n",
        "# 2) Build user–item rating matrix\n",
        "UI = ratings.pivot_table(index=\"user_id\", columns=\"movie_id\", values=\"rating\")\n",
        "\n",
        "# 3) Mean-center by user (reduces user bias)\n",
        "user_means = UI.mean(axis=1)\n",
        "UI_centered = UI.sub(user_means, axis=0)\n",
        "\n",
        "# 4) Compute USER–USER cosine similarity on centered data (fill NaNs with 0)\n",
        "UIc_filled = UI_centered.fillna(0.0).astype(np.float32)\n",
        "user_ids = UIc_filled.index.to_numpy()\n",
        "user_sim = cosine_similarity(UIc_filled.values)  # shape: n_users x n_users\n",
        "user_sim = pd.DataFrame(user_sim, index=user_ids, columns=user_ids)\n",
        "\n",
        "# --- Helper: find k nearest neighbors with at least min_overlap co-rated items ---\n",
        "def _nearest_neighbors(target_uid, k=30, min_overlap=5):\n",
        "    # Overlap: count of movies both target and candidate rated\n",
        "    target_rated = UI.loc[target_uid].notna()\n",
        "    overlap = UI.notna().dot(target_rated.astype(int))  # fast overlap counts per user\n",
        "    sims = user_sim.loc[target_uid].drop(index=target_uid)  # exclude self\n",
        "    # filter by minimum overlap and positive similarity\n",
        "    mask = (overlap.drop(index=target_uid) >= min_overlap) & (sims > 0)\n",
        "    sims = sims[mask]\n",
        "    # top-k neighbors\n",
        "    return sims.sort_values(ascending=False).head(k)\n",
        "\n",
        "# --- Core prediction: weighted sum of neighbor deviations + user mean ---\n",
        "def predict_user_scores(target_uid, k=30, min_overlap=5, shrink=10.0):\n",
        "    \"\"\"\n",
        "    Returns a pandas Series of predicted ratings for all movies for target_uid.\n",
        "    Uses mean-centered neighbor ratings with cosine similarities and a shrinkage term.\n",
        "    \"\"\"\n",
        "    if target_uid not in UI.index:\n",
        "        raise ValueError(f\"user_id {target_uid} not found.\")\n",
        "\n",
        "    neighbors = _nearest_neighbors(target_uid, k=k, min_overlap=min_overlap)\n",
        "    if neighbors.empty:\n",
        "        # Fallback: everyone’s global mean for unseen movies\n",
        "        global_mean = ratings[\"rating\"].mean()\n",
        "        return pd.Series(global_mean, index=UI.columns)\n",
        "\n",
        "    # Build neighbor-centered matrix restricted to neighbors\n",
        "    N = neighbors.index\n",
        "    sims = neighbors.values  # similarity weights\n",
        "    # deviations from each neighbor's mean\n",
        "    neigh_means = user_means.loc[N]\n",
        "    neigh_centered = UI.loc[N].sub(neigh_means, axis=0)\n",
        "\n",
        "    # Weighted sum of neighbor deviations\n",
        "    # numerator[j] = sum_i sim_i * dev_{i,j}; denominator[j] = sum_i |sim_i|\n",
        "    dev = neigh_centered  # can contain NaN\n",
        "    sim_vec = pd.Series(sims, index=N)\n",
        "\n",
        "    # Use matrix multiplication with NaN-safe handling\n",
        "    dev_filled = dev.fillna(0.0).to_numpy(dtype=np.float32)                 # n_neighbors x n_items\n",
        "    weights = sim_vec.to_numpy(dtype=np.float32)[:, None]                   # n_neighbors x 1\n",
        "    num = (weights * dev_filled).sum(axis=0)                                # n_items\n",
        "\n",
        "    # Denominator: sum of |sim| over neighbors who rated the item\n",
        "    rated_mask = (~dev.isna()).to_numpy(dtype=np.float32)                   # 1 if neighbor rated\n",
        "    den = (np.abs(weights) * rated_mask).sum(axis=0) + shrink               # add shrinkage\n",
        "\n",
        "    # Predicted deviations\n",
        "    pred_dev = num / den\n",
        "\n",
        "    # Add back target user's mean\n",
        "    base = float(user_means.loc[target_uid])\n",
        "    preds = pd.Series(base + pred_dev, index=UI.columns)\n",
        "\n",
        "    return preds\n",
        "\n",
        "# --- Recommend top-N unseen movies for a user ---\n",
        "def recommend_for_user(target_uid, top_n=10, k=30, min_overlap=5, shrink=10.0):\n",
        "    preds = predict_user_scores(target_uid, k=k, min_overlap=min_overlap, shrink=shrink)\n",
        "\n",
        "    # Exclude already-rated movies\n",
        "    already_rated = UI.loc[target_uid].dropna().index\n",
        "    preds = preds.drop(index=already_rated, errors=\"ignore\")\n",
        "\n",
        "    # Clip to rating range (MovieLens 100K is 1..5)\n",
        "    preds = preds.clip(lower=1.0, upper=5.0)\n",
        "\n",
        "    top = preds.sort_values(ascending=False).head(top_n).reset_index()\n",
        "    top.columns = [\"movie_id\", \"pred_rating\"]\n",
        "    if \"title\" in movies.columns:\n",
        "        top = top.merge(movies, on=\"movie_id\", how=\"left\")[[\"movie_id\", \"title\", \"pred_rating\"]]\n",
        "    return top\n",
        "\n",
        "# ===== Example =====\n",
        "# Choose any existing user_id from UI.index, e.g., 100\n",
        "target_user = int(UI.index[0])  # or specify e.g. 100\n",
        "recs = recommend_for_user(target_user, top_n=10, k=40, min_overlap=5, shrink=15.0)\n",
        "\n",
        "print(f\"\\nTop recommendations for user {target_user}:\")\n",
        "print(recs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ys_t8r4QYqxk",
        "outputId": "e00cc2ec-552f-485e-f4bc-3c3309ecdb2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top recommendations for user 1:\n",
            "   movie_id                                              title  pred_rating\n",
            "0       318                            Schindler's List (1993)     3.837789\n",
            "1       483                                  Casablanca (1942)     3.827525\n",
            "2       651                                       Glory (1989)     3.821201\n",
            "3       474  Dr. Strangelove or: How I Learned to Stop Worr...     3.791263\n",
            "4       408                              Close Shave, A (1995)     3.790018\n",
            "5       357             One Flew Over the Cuckoo's Nest (1975)     3.782414\n",
            "6       603                                 Rear Window (1954)     3.774031\n",
            "7       302                           L.A. Confidential (1997)     3.762339\n",
            "8       484                         Maltese Falcon, The (1941)     3.762025\n",
            "9       435          Butch Cassidy and the Sundance Kid (1969)     3.759227\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1) Load ratings and movie titles ---\n",
        "ratings = pd.read_csv(\n",
        "    \"u.data\", sep=\"\\t\", header=None,\n",
        "    names=[\"user_id\", \"movie_id\", \"rating\", \"timestamp\"]\n",
        ")[[\"user_id\", \"movie_id\", \"rating\"]]\n",
        "\n",
        "movies = pd.read_csv(\n",
        "    \"u.item\", sep=\"|\", header=None, encoding=\"latin-1\",\n",
        "    usecols=[0, 1], names=[\"movie_id\", \"title\"]\n",
        ")\n",
        "\n",
        "# --- 2) Build user–item matrix + mean-centering ---\n",
        "UI = ratings.pivot_table(index=\"user_id\", columns=\"movie_id\", values=\"rating\")\n",
        "user_means = UI.mean(axis=1)\n",
        "UI_centered = UI.sub(user_means, axis=0)\n",
        "\n",
        "# Precompute cosine similarity between users on centered matrix\n",
        "UIc_filled = UI_centered.fillna(0.0).astype(np.float32)\n",
        "user_sim = pd.DataFrame(\n",
        "    cosine_similarity(UIc_filled.values),\n",
        "    index=UI.index, columns=UI.index\n",
        ")\n",
        "\n",
        "# --- 3) Helpers: neighbors, predictions, recommendations ---\n",
        "def _nearest_neighbors(target_uid, k=30, min_overlap=5):\n",
        "    \"\"\"Return top-k neighbors with positive sim and at least min_overlap co-rated items.\"\"\"\n",
        "    if target_uid not in UI.index:\n",
        "        raise ValueError(f\"user_id {target_uid} not found.\")\n",
        "    target_rated = UI.loc[target_uid].notna()\n",
        "    overlap = UI.notna().dot(target_rated.astype(int))              # users × items → counts\n",
        "    sims = user_sim.loc[target_uid].drop(index=target_uid)          # exclude self\n",
        "    mask = (overlap.drop(index=target_uid) >= min_overlap) & (sims > 0)\n",
        "    return sims[mask].sort_values(ascending=False).head(k)\n",
        "\n",
        "def _predict_all_items_for_user(target_uid, k=30, min_overlap=5, shrink=10.0):\n",
        "    \"\"\"Predict ratings for all items via weighted neighbor deviations + user mean.\"\"\"\n",
        "    nbrs = _nearest_neighbors(target_uid, k=k, min_overlap=min_overlap)\n",
        "    if nbrs.empty:\n",
        "        # fallback: global mean for all items\n",
        "        return pd.Series(ratings[\"rating\"].mean(), index=UI.columns)\n",
        "\n",
        "    N = nbrs.index\n",
        "    w = nbrs.values.astype(np.float32)[:, None]                     # n_neighbors × 1\n",
        "    neigh_means = user_means.loc[N]\n",
        "    dev = UI.loc[N].sub(neigh_means, axis=0)                        # neighbor-centered\n",
        "\n",
        "    dev_filled = dev.fillna(0.0).to_numpy(dtype=np.float32)         # n_neighbors × n_items\n",
        "    num = (w * dev_filled).sum(axis=0)                              # weighted deviations\n",
        "    rated_mask = (~dev.isna()).to_numpy(dtype=np.float32)\n",
        "    den = (np.abs(w) * rated_mask).sum(axis=0) + shrink             # shrink to stabilize\n",
        "    pred_dev = num / den\n",
        "\n",
        "    base = float(user_means.loc[target_uid])\n",
        "    preds = pd.Series(base + pred_dev, index=UI.columns)\n",
        "    return preds.clip(lower=1.0, upper=5.0)\n",
        "\n",
        "def recommend_top_unseen(target_uid, top_n=10, k=30, min_overlap=5, shrink=10.0):\n",
        "    \"\"\"Return top-N highest predicted ratings for movies the user hasn't rated yet.\"\"\"\n",
        "    preds = _predict_all_items_for_user(target_uid, k=k, min_overlap=min_overlap, shrink=shrink)\n",
        "    unseen = UI.loc[target_uid][UI.loc[target_uid].isna()].index    # movies not rated by user\n",
        "    top = preds.loc[unseen].sort_values(ascending=False).head(top_n).reset_index()\n",
        "    top.columns = [\"movie_id\", \"pred_rating\"]\n",
        "    # attach titles if available\n",
        "    return top.merge(movies, on=\"movie_id\", how=\"left\")[[\"movie_id\", \"title\", \"pred_rating\"]]\n",
        "\n",
        "# --- Example ---\n",
        "user_id = int(UI.index[0])    # or set a specific user id, e.g., 100\n",
        "recs = recommend_top_unseen(user_id, top_n=10, k=40, min_overlap=5, shrink=15.0)\n",
        "print(f\"Top recommendations for user {user_id}:\")\n",
        "print(recs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jiSATO5sYtCq",
        "outputId": "85fa3ea0-a31a-4a04-bd46-c8fc4e3c44aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top recommendations for user 1:\n",
            "   movie_id                                              title  pred_rating\n",
            "0       318                            Schindler's List (1993)     3.837789\n",
            "1       483                                  Casablanca (1942)     3.827525\n",
            "2       651                                       Glory (1989)     3.821201\n",
            "3       474  Dr. Strangelove or: How I Learned to Stop Worr...     3.791263\n",
            "4       408                              Close Shave, A (1995)     3.790018\n",
            "5       357             One Flew Over the Cuckoo's Nest (1975)     3.782414\n",
            "6       603                                 Rear Window (1954)     3.774031\n",
            "7       302                           L.A. Confidential (1997)     3.762339\n",
            "8       484                         Maltese Falcon, The (1941)     3.762025\n",
            "9       435          Butch Cassidy and the Sundance Kid (1969)     3.759227\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Precision@K for User-based CF =====\n",
        "\n",
        "# ----------------------------\n",
        "# 1) Load ratings (+ titles)\n",
        "# ----------------------------\n",
        "ratings = pd.read_csv(\n",
        "    \"u.data\", sep=\"\\t\", header=None,\n",
        "    names=[\"user_id\", \"movie_id\", \"rating\", \"timestamp\"]\n",
        ")[[\"user_id\", \"movie_id\", \"rating\"]]\n",
        "\n",
        "movies = pd.read_csv(\n",
        "    \"u.item\", sep=\"|\", header=None, encoding=\"latin-1\",\n",
        "    usecols=[0, 1], names=[\"movie_id\", \"title\"]\n",
        ")\n",
        "\n",
        "# -----------------------------------------\n",
        "# 2) Build a per-user train/test holdout\n",
        "# -----------------------------------------\n",
        "min_interactions = 10     # users must have at least this many ratings to evaluate\n",
        "n_holdout = 5             # hold out 5 positives per user\n",
        "rng = np.random.default_rng(42)\n",
        "\n",
        "# keep only users with enough interactions\n",
        "counts = ratings.groupby(\"user_id\")[\"movie_id\"].count()\n",
        "eligible_users = counts[counts >= (min_interactions + n_holdout)].index\n",
        "\n",
        "# sample holdout positives per user\n",
        "def sample_holdout(g):\n",
        "    # g is the group (one user's ratings)\n",
        "    test_idx = rng.choice(g.index.values, size=n_holdout, replace=False)\n",
        "    g = g.copy()\n",
        "    g[\"is_test\"] = False\n",
        "    g.loc[test_idx, \"is_test\"] = True\n",
        "    return g\n",
        "\n",
        "split = (\n",
        "    ratings[ratings[\"user_id\"].isin(eligible_users)]\n",
        "    .groupby(\"user_id\", group_keys=False)\n",
        "    .apply(sample_holdout)\n",
        ")\n",
        "\n",
        "train = split[~split[\"is_test\"]].drop(columns=[\"is_test\"])\n",
        "test  = split[ split[\"is_test\"]].drop(columns=[\"is_test\"])\n",
        "\n",
        "# map test positives per user as a set of movie_ids\n",
        "test_pos = test.groupby(\"user_id\")[\"movie_id\"].apply(set)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 3) Build user–item matrix & similarity from TRAIN only\n",
        "# ---------------------------------------------------------\n",
        "UI = train.pivot_table(index=\"user_id\", columns=\"movie_id\", values=\"rating\")\n",
        "user_means = UI.mean(axis=1)\n",
        "UI_centered = UI.sub(user_means, axis=0)\n",
        "\n",
        "UIc_filled = UI_centered.fillna(0.0).astype(np.float32)\n",
        "user_sim = pd.DataFrame(\n",
        "    cosine_similarity(UIc_filled.values),\n",
        "    index=UI.index, columns=UI.index\n",
        ")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 4) Recommender (same logic, but using TRAIN matrices)\n",
        "# ---------------------------------------------------------\n",
        "def _nearest_neighbors(target_uid, k=30, min_overlap=5):\n",
        "    if target_uid not in UI.index:\n",
        "        return pd.Series(dtype=float)  # user not in train (shouldn't happen with our filter)\n",
        "    target_rated = UI.loc[target_uid].notna()\n",
        "    overlap = UI.notna().dot(target_rated.astype(int))  # users × items → overlap counts\n",
        "    sims = user_sim.loc[target_uid].drop(index=target_uid)\n",
        "    mask = (overlap.drop(index=target_uid) >= min_overlap) & (sims > 0)\n",
        "    return sims[mask].sort_values(ascending=False).head(k)\n",
        "\n",
        "def _predict_all_items_for_user(target_uid, k=30, min_overlap=5, shrink=10.0):\n",
        "    nbrs = _nearest_neighbors(target_uid, k=k, min_overlap=min_overlap)\n",
        "    if nbrs.empty:\n",
        "        # fallback: global mean for all items in train\n",
        "        return pd.Series(train[\"rating\"].mean(), index=UI.columns)\n",
        "\n",
        "    N = nbrs.index\n",
        "    w = nbrs.values.astype(np.float32)[:, None]                # n_neighbors × 1\n",
        "    neigh_means = user_means.loc[N]\n",
        "    dev = UI.loc[N].sub(neigh_means, axis=0)                   # neighbor-centered\n",
        "\n",
        "    dev_filled = dev.fillna(0.0).to_numpy(dtype=np.float32)    # n_neighbors × n_items\n",
        "    num = (w * dev_filled).sum(axis=0)\n",
        "    rated_mask = (~dev.isna()).to_numpy(dtype=np.float32)\n",
        "    den = (np.abs(w) * rated_mask).sum(axis=0) + shrink\n",
        "    pred_dev = num / den\n",
        "\n",
        "    base = float(user_means.loc[target_uid])\n",
        "    preds = pd.Series(base + pred_dev, index=UI.columns)\n",
        "    return preds.clip(lower=1.0, upper=5.0)\n",
        "\n",
        "def recommend_top_unseen_train(target_uid, top_n=10, k=30, min_overlap=5, shrink=10.0):\n",
        "    preds = _predict_all_items_for_user(target_uid, k=k, min_overlap=min_overlap, shrink=shrink)\n",
        "    # exclude items already rated in TRAIN\n",
        "    already = UI.loc[target_uid].dropna().index if target_uid in UI.index else pd.Index([])\n",
        "    candidates = preds.drop(index=already, errors=\"ignore\")\n",
        "    top = candidates.sort_values(ascending=False).head(top_n).reset_index()\n",
        "    top.columns = [\"movie_id\", \"pred_rating\"]\n",
        "    return top\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 5) Precision@K evaluation\n",
        "# ---------------------------------------------------------\n",
        "def precision_at_k_for_user(uid, K=10, k_nbrs=30, min_overlap=5, shrink=10.0):\n",
        "    \"\"\"Precision@K: (# held-out positives in top-K) / K.\"\"\"\n",
        "    if uid not in test_pos:   # user not in eval set\n",
        "        return np.nan\n",
        "    top = recommend_top_unseen_train(uid, top_n=K, k=k_nbrs, min_overlap=min_overlap, shrink=shrink)\n",
        "    hit_set = test_pos[uid]\n",
        "    hits = top[\"movie_id\"].isin(hit_set).sum()\n",
        "    return hits / float(K)\n",
        "\n",
        "def evaluate_precision_at_k(K_list=(5, 10, 20), k_nbrs=40, min_overlap=5, shrink=15.0):\n",
        "    users = sorted(test_pos.index.intersection(UI.index))\n",
        "    results = {}\n",
        "    for K in K_list:\n",
        "        vals = [precision_at_k_for_user(u, K=K, k_nbrs=k_nbrs, min_overlap=min_overlap, shrink=shrink)\n",
        "                for u in users]\n",
        "        vals = pd.Series(vals, index=users).dropna()\n",
        "        results[K] = {\n",
        "            \"mean_precision\": float(vals.mean()),\n",
        "            \"median_precision\": float(vals.median()),\n",
        "            \"users_evaluated\": int(vals.shape[0])\n",
        "        }\n",
        "    return pd.DataFrame(results).T\n",
        "\n",
        "# -------------------------\n",
        "# 6) Run the evaluation\n",
        "# -------------------------\n",
        "report = evaluate_precision_at_k(K_list=(5, 10, 20), k_nbrs=40, min_overlap=5, shrink=15.0)\n",
        "print(\"\\nPrecision@K report (user-based CF):\")\n",
        "print(report)\n",
        "\n",
        "# Optional: preview some recs for a sample user and see which held-out items hit\n",
        "sample_user = report.index.name  # ignore; just pick one user explicitly:\n",
        "sample_user = test_pos.index[0]\n",
        "preview = recommend_top_unseen_train(sample_user, top_n=10, k=40, min_overlap=5, shrink=15.0)\n",
        "preview = preview.merge(movies, on=\"movie_id\", how=\"left\")\n",
        "preview[\"is_holdout_positive\"] = preview[\"movie_id\"].isin(test_pos[sample_user])\n",
        "print(f\"\\nTop-10 recommendations for user {sample_user} (✓ = in holdout):\")\n",
        "print(preview[[\"movie_id\", \"title\", \"pred_rating\", \"is_holdout_positive\"]])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1qf0ETgYvx7",
        "outputId": "f0f93ad3-11c1-4ab3-e76b-d05c705c0504"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1575917725.py:42: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  .apply(sample_holdout)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Precision@K report (user-based CF):\n",
            "    mean_precision  median_precision  users_evaluated\n",
            "5         0.091198              0.00            943.0\n",
            "10        0.068611              0.10            943.0\n",
            "20        0.047402              0.05            943.0\n",
            "\n",
            "Top-10 recommendations for user 1 (✓ = in holdout):\n",
            "   movie_id                                              title  pred_rating  \\\n",
            "0       173                         Princess Bride, The (1987)     3.867249   \n",
            "1       318                            Schindler's List (1993)     3.822474   \n",
            "2       651                                       Glory (1989)     3.817047   \n",
            "3       483                                  Casablanca (1942)     3.810419   \n",
            "4       357             One Flew Over the Cuckoo's Nest (1975)     3.772603   \n",
            "5       408                              Close Shave, A (1995)     3.767138   \n",
            "6       474  Dr. Strangelove or: How I Learned to Stop Worr...     3.766849   \n",
            "7       603                                 Rear Window (1954)     3.750643   \n",
            "8       302                           L.A. Confidential (1997)     3.746663   \n",
            "9       276                           Leaving Las Vegas (1995)     3.746602   \n",
            "\n",
            "   is_holdout_positive  \n",
            "0                 True  \n",
            "1                False  \n",
            "2                False  \n",
            "3                False  \n",
            "4                False  \n",
            "5                False  \n",
            "6                False  \n",
            "7                False  \n",
            "8                False  \n",
            "9                False  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Item-based CF=====\n",
        "\n",
        "# 1) Load ratings (+ titles)\n",
        "ratings = pd.read_csv(\n",
        "    \"u.data\", sep=\"\\t\", header=None,\n",
        "    names=[\"user_id\", \"movie_id\", \"rating\", \"timestamp\"]\n",
        ")[[\"user_id\", \"movie_id\", \"rating\"]]\n",
        "\n",
        "movies = pd.read_csv(\n",
        "    \"u.item\", sep=\"|\", header=None, encoding=\"latin-1\",\n",
        "    usecols=[0, 1], names=[\"movie_id\", \"title\"]\n",
        ")\n",
        "\n",
        "# 2) Build user–item matrix\n",
        "UI = ratings.pivot_table(index=\"user_id\", columns=\"movie_id\", values=\"rating\")\n",
        "\n",
        "# 3) Adjusted-cosine: center by *user* mean (to remove user bias)\n",
        "user_means = UI.mean(axis=1)\n",
        "UI_centered = UI.sub(user_means, axis=0)\n",
        "\n",
        "# 4) Compute ITEM–ITEM cosine similarity on centered data\n",
        "#    (fill NaNs with 0, since missing entries mean \"no rating\")\n",
        "IIc = UI_centered.fillna(0.0).astype(np.float32)\n",
        "item_sim = pd.DataFrame(\n",
        "    cosine_similarity(IIc.T.values),   # similarity among columns (items)\n",
        "    index=UI.columns, columns=UI.columns\n",
        ")\n",
        "\n",
        "# ---------- Core prediction (item-based) ----------\n",
        "def predict_user_scores_item_based(target_uid, k=50, shrink=10.0, min_sim=0.0):\n",
        "    \"\"\"\n",
        "    Predict ratings for all items for a given user using item-based CF:\n",
        "    pred(j) = mean_user + sum_i( sim(j,i) * dev_user(i) ) / sum_i( |sim(j,i)| )\n",
        "    where dev_user(i) = rating_user(i) - mean_user.\n",
        "    - k: use top-k similar *neighbor items* per candidate item\n",
        "    - shrink: stability term when few neighbors exist\n",
        "    - min_sim: ignore neighbors with similarity below this\n",
        "    Returns: pandas Series (index = movie_id) of predicted ratings (clipped to 1..5)\n",
        "    \"\"\"\n",
        "    if target_uid not in UI.index:\n",
        "        raise ValueError(f\"user_id {target_uid} not found.\")\n",
        "\n",
        "    # user’s ratings & deviations\n",
        "    user_row = UI.loc[target_uid]\n",
        "    rated_items = user_row.dropna()\n",
        "    if rated_items.empty:\n",
        "        # cold start fallback: global item mean\n",
        "        global_mean = ratings[\"rating\"].mean()\n",
        "        return pd.Series(global_mean, index=UI.columns)\n",
        "\n",
        "    mu_u = float(user_means.loc[target_uid])\n",
        "    dev_u = rated_items - mu_u  # Series indexed by item_id the user rated\n",
        "\n",
        "    # For every candidate item j (including rated ones; we'll drop them later):\n",
        "    preds = {}\n",
        "    # Pre-extract for speed\n",
        "    sim_sub = item_sim.loc[:, rated_items.index]  # similarities from *all items* to user's rated items\n",
        "\n",
        "    for j in item_sim.index:\n",
        "        # similarities between j and items user rated\n",
        "        s = sim_sub.loc[j]\n",
        "        # filter by min_sim and drop self if present\n",
        "        s = s[s.index != j]\n",
        "        s = s[s.abs() >= min_sim]\n",
        "        if s.empty:\n",
        "            preds[j] = mu_u  # fallback to user's mean\n",
        "            continue\n",
        "\n",
        "        # take top-k by absolute similarity (strongest signals)\n",
        "        s = s.reindex(s.abs().sort_values(ascending=False).head(k).index)\n",
        "        # align with dev_u (should already match indices)\n",
        "        dev = dev_u.reindex(s.index)\n",
        "\n",
        "        # weighted deviation\n",
        "        num = (s.values * dev.values).sum()\n",
        "        den = np.abs(s.values).sum() + shrink\n",
        "        preds[j] = mu_u + num / den\n",
        "\n",
        "    # Clip to the rating scale (MovieLens 100K: 1..5)\n",
        "    preds = pd.Series(preds, index=item_sim.index).clip(lower=1.0, upper=5.0)\n",
        "    return preds\n",
        "\n",
        "def recommend_item_based(target_uid, top_n=10, k=50, shrink=10.0, min_sim=0.0):\n",
        "    \"\"\"\n",
        "    Recommend top-N *unseen* items for the user using item-based CF predictions.\n",
        "    \"\"\"\n",
        "    preds = predict_user_scores_item_based(target_uid, k=k, shrink=shrink, min_sim=min_sim)\n",
        "    # Exclude already-rated movies\n",
        "    seen = UI.loc[target_uid].dropna().index\n",
        "    recs = preds.drop(index=seen, errors=\"ignore\").sort_values(ascending=False).head(top_n).reset_index()\n",
        "    recs.columns = [\"movie_id\", \"pred_rating\"]\n",
        "    return recs.merge(movies, on=\"movie_id\", how=\"left\")[[\"movie_id\", \"title\", \"pred_rating\"]]\n",
        "\n",
        "# ===== Example =====\n",
        "example_user = int(UI.index[0])  # pick an existing user_id\n",
        "recs = recommend_item_based(example_user, top_n=10, k=75, shrink=15.0, min_sim=0.05)\n",
        "print(f\"\\nTop item-based recommendations for user {example_user}:\")\n",
        "print(recs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zig_a0ChYyah",
        "outputId": "b386335a-0956-4da8-d367-ccc763ae12db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top item-based recommendations for user 1:\n",
            "   movie_id                                              title  pred_rating\n",
            "0       483                                  Casablanca (1942)     4.043228\n",
            "1       603                                 Rear Window (1954)     4.035880\n",
            "2       479                                     Vertigo (1958)     4.026748\n",
            "3       357             One Flew Over the Cuckoo's Nest (1975)     4.022097\n",
            "4       408                              Close Shave, A (1995)     4.014354\n",
            "5       651                                       Glory (1989)     4.001076\n",
            "6       511                          Lawrence of Arabia (1962)     3.998182\n",
            "7       435          Butch Cassidy and the Sundance Kid (1969)     3.997117\n",
            "8       302                           L.A. Confidential (1997)     3.997114\n",
            "9       474  Dr. Strangelove or: How I Learned to Stop Worr...     3.995288\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Matrix Factorization (FunkSVD with biases, SGD) =====\n",
        "\n",
        "# 1) Load ratings (+ titles)\n",
        "ratings = pd.read_csv(\n",
        "    \"u.data\", sep=\"\\t\", header=None,\n",
        "    names=[\"user_id\", \"movie_id\", \"rating\", \"timestamp\"]\n",
        ")[[\"user_id\", \"movie_id\", \"rating\"]]\n",
        "\n",
        "movies = pd.read_csv(\n",
        "    \"u.item\", sep=\"|\", header=None, encoding=\"latin-1\",\n",
        "    usecols=[0, 1], names=[\"movie_id\", \"title\"]\n",
        ")\n",
        "\n",
        "# Map raw ids → contiguous indices (0..n_users-1 / 0..n_items-1)\n",
        "user_ids = ratings[\"user_id\"].unique()\n",
        "item_ids = ratings[\"movie_id\"].unique()\n",
        "uid_map = {u:i for i,u in enumerate(sorted(user_ids))}\n",
        "iid_map = {m:i for i,m in enumerate(sorted(item_ids))}\n",
        "rid_map = {i:u for u,i in uid_map.items()}\n",
        "mid_map = {i:m for m,i in iid_map.items()}\n",
        "\n",
        "ratings[\"u_idx\"] = ratings[\"user_id\"].map(uid_map)\n",
        "ratings[\"i_idx\"] = ratings[\"movie_id\"].map(iid_map)\n",
        "\n",
        "# 2) Train/test split on observed ratings (no leakage)\n",
        "train_df, test_df = train_test_split(\n",
        "    ratings, test_size=0.1, random_state=42, stratify=ratings[\"user_id\"]\n",
        ")\n",
        "\n",
        "n_users = len(uid_map)\n",
        "n_items = len(iid_map)\n",
        "print(f\"Users={n_users}, Items={n_items}, Train={len(train_df)}, Test={len(test_df)}\")\n",
        "\n",
        "# 3) FunkSVD with biases (SGD)\n",
        "class FunkSVD:\n",
        "    def __init__(self, n_factors=50, n_epochs=20, lr=0.01, reg=0.02, seed=42):\n",
        "        self.k = n_factors\n",
        "        self.n_epochs = n_epochs\n",
        "        self.lr = lr\n",
        "        self.reg = reg\n",
        "        self.rng = np.random.default_rng(seed)\n",
        "\n",
        "    def fit(self, df, n_users, n_items):\n",
        "        self.n_users = n_users\n",
        "        self.n_items = n_items\n",
        "\n",
        "        self.mu = df[\"rating\"].mean()\n",
        "        self.bu = np.zeros(n_users, dtype=np.float32)\n",
        "        self.bi = np.zeros(n_items, dtype=np.float32)\n",
        "        self.P = 0.1 * self.rng.standard_normal((n_users, self.k), dtype=np.float32)\n",
        "        self.Q = 0.1 * self.rng.standard_normal((n_items, self.k), dtype=np.float32)\n",
        "\n",
        "        u = df[\"u_idx\"].to_numpy(np.int32)\n",
        "        i = df[\"i_idx\"].to_numpy(np.int32)\n",
        "        r = df[\"rating\"].to_numpy(np.float32)\n",
        "\n",
        "        for epoch in range(self.n_epochs):\n",
        "            # shuffle samples\n",
        "            idx = self.rng.permutation(len(df))\n",
        "            u_sh, i_sh, r_sh = u[idx], i[idx], r[idx]\n",
        "\n",
        "            for uu, ii, rr in zip(u_sh, i_sh, r_sh):\n",
        "                pred = self.mu + self.bu[uu] + self.bi[ii] + np.dot(self.P[uu], self.Q[ii])\n",
        "                err = rr - pred\n",
        "\n",
        "                # cache factors\n",
        "                Pu = self.P[uu]\n",
        "                Qi = self.Q[ii]\n",
        "\n",
        "                # updates\n",
        "                self.bu[uu] += self.lr * (err - self.reg * self.bu[uu])\n",
        "                self.bi[ii] += self.lr * (err - self.reg * self.bi[ii])\n",
        "                self.P[uu]  += self.lr * (err * Qi - self.reg * Pu)\n",
        "                self.Q[ii]  += self.lr * (err * Pu - self.reg * Qi)\n",
        "\n",
        "            # quick RMSE on train each epoch\n",
        "            if (epoch + 1) % 5 == 0 or epoch == 0:\n",
        "                rmse_tr = self.rmse(df)\n",
        "                print(f\"Epoch {epoch+1:02d}/{self.n_epochs} | Train RMSE: {rmse_tr:.4f}\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict_pair(self, u_idx, i_idx):\n",
        "        # clip to 1..5 for MovieLens\n",
        "        x = (self.mu + self.bu[u_idx] + self.bi[i_idx] +\n",
        "             np.dot(self.P[u_idx], self.Q[i_idx]))\n",
        "        return float(np.clip(x, 1.0, 5.0))\n",
        "\n",
        "    def rmse(self, df):\n",
        "        u = df[\"u_idx\"].to_numpy(np.int32)\n",
        "        i = df[\"i_idx\"].to_numpy(np.int32)\n",
        "        r = df[\"rating\"].to_numpy(np.float32)\n",
        "        preds = self.mu + self.bu[u] + self.bi[i] + np.sum(self.P[u] * self.Q[i], axis=1)\n",
        "        preds = np.clip(preds, 1.0, 5.0)\n",
        "        return float(np.sqrt(np.mean((r - preds) ** 2)))\n",
        "\n",
        "    def full_user_scores(self, u_idx):\n",
        "        # vectorized scores for one user over all items\n",
        "        scores = self.mu + self.bu[u_idx] + self.bi + self.P[u_idx] @ self.Q.T\n",
        "        return np.clip(scores, 1.0, 5.0)\n",
        "\n",
        "# 4) Train the model\n",
        "svd = FunkSVD(n_factors=50, n_epochs=20, lr=0.01, reg=0.05, seed=42).fit(\n",
        "    train_df, n_users=n_users, n_items=n_items\n",
        ")\n",
        "\n",
        "# 5) Evaluate RMSE on test set\n",
        "rmse_te = svd.rmse(test_df)\n",
        "print(f\"\\nTest RMSE: {rmse_te:.4f}\")\n",
        "\n",
        "# 6) Recommend top-N unseen movies for a given user\n",
        "def recommend_for_user_svd(raw_user_id, top_n=10):\n",
        "    if raw_user_id not in uid_map:\n",
        "        raise ValueError(\"Unknown user_id\")\n",
        "\n",
        "    u_idx = uid_map[raw_user_id]\n",
        "\n",
        "    # Find items the user has rated (in all data)\n",
        "    seen_items = ratings.loc[ratings[\"user_id\"] == raw_user_id, \"i_idx\"].unique()\n",
        "    seen_mask = np.zeros(n_items, dtype=bool)\n",
        "    seen_mask[seen_items] = True\n",
        "\n",
        "    # Predict scores for all items then remove seen\n",
        "    scores = svd.full_user_scores(u_idx)\n",
        "    scores[seen_mask] = -np.inf\n",
        "\n",
        "    top_iidx = np.argpartition(-scores, top_n)[:top_n]\n",
        "    top_iidx = top_iidx[np.argsort(-scores[top_iidx])]\n",
        "    top_movie_ids = [mid_map[i] for i in top_iidx]\n",
        "    top_scores = scores[top_iidx]\n",
        "\n",
        "    out = pd.DataFrame({\"movie_id\": top_movie_ids, \"pred_rating\": top_scores})\n",
        "    out = out.merge(movies, on=\"movie_id\", how=\"left\")[[\"movie_id\", \"title\", \"pred_rating\"]]\n",
        "    return out\n",
        "\n",
        "# ===== Example =====\n",
        "example_user = int(ratings[\"user_id\"].iloc[0])\n",
        "recs = recommend_for_user_svd(example_user, top_n=10)\n",
        "print(f\"\\nTop SVD-based recommendations for user {example_user}:\")\n",
        "print(recs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BcgBDk7mY1D5",
        "outputId": "b3b4b40d-4df4-47e2-b3cc-be48dd257ff7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Users=943, Items=1682, Train=90000, Test=10000\n",
            "Epoch 01/20 | Train RMSE: 0.9570\n",
            "Epoch 05/20 | Train RMSE: 0.8922\n",
            "Epoch 10/20 | Train RMSE: 0.8265\n",
            "Epoch 15/20 | Train RMSE: 0.7419\n",
            "Epoch 20/20 | Train RMSE: 0.6666\n",
            "\n",
            "Test RMSE: 0.9095\n",
            "\n",
            "Top SVD-based recommendations for user 196:\n",
            "   movie_id                                    title  pred_rating\n",
            "0        64         Shawshank Redemption, The (1994)     4.755151\n",
            "1       318                  Schindler's List (1993)     4.668643\n",
            "2       603                       Rear Window (1954)     4.573271\n",
            "3      1449                   Pather Panchali (1955)     4.516722\n",
            "4       197                     Graduate, The (1967)     4.495232\n",
            "5       313                           Titanic (1997)     4.489260\n",
            "6       963  Some Folks Call It a Sling Blade (1993)     4.453530\n",
            "7        69                      Forrest Gump (1994)     4.444258\n",
            "8       480                North by Northwest (1959)     4.423979\n",
            "9       180                    Apocalypse Now (1979)     4.412159\n"
          ]
        }
      ]
    }
  ]
}